+=============================================================+
|                       CERTEUS — HEART                        |
+=============================================================+
CERTEUS PACK — Context for AI assistants (Claude/GPT/Gemini).
PL: Pliki repo są oddzielone '===== FILE: … ====='.
EN: Files are delimited by '===== FILE: … ====='.
Guidance: read sequentially; do not assume missing files exist; respect file boundaries.



===== FILE: schemas/publication_contract_v1.json =====
```text
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "PublicationContractV1",
    "type": "object",
    "x-meta": {
        "banner": "+=============================================================+\\n|                       CERTEUS — HEART                        |\\n+=============================================================+\\n| FILE: schemas/publication_contract_v1.json                  |\\n| ROLE:                                                       |\\n|  PL: Schemat publicznej odpowiedzi publikacyjnej.           |\\n|  EN: Schema of public publication response.                 |\\n+=============================================================+",
        "description_pl": "Kontrakt zwracany przez endpointy publikacyjne (status + PCO/plan + eta_hint).",
        "description_en": "Contract returned by publication endpoints (status + PCO/plan + eta_hint)."
    },
    "required": ["status", "headers"],
    "properties": {
        "status": {
            "type": "string",
            "enum": ["PUBLISH", "CONDITIONAL", "PENDING", "ABSTAIN"]
        },
        "eta_hint": {
            "type": "string"
        },
        "proof_task_id": {
            "type": "string"
        },
        "pco": {
            "type": "object"
        },
        "pco.plan": {
            "type": "object"
        },
        "headers": {
            "type": "object",
            "required": ["X-Norm-Pack-ID", "X-Jurisdiction"],
            "properties": {
                "X-Norm-Pack-ID": {
                    "type": "string"
                },
                "X-Jurisdiction": {
                    "type": "string"
                }
            },
            "additionalProperties": false
        }
    },
    "additionalProperties": false
}

```


===== FILE: scripts/apply_headers.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/apply_headers.py                |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: scripts/apply_headers.py                              |
# | ROLE: Auto-add CERTEUS header block and module docstrings   |
# |       to Python files that are missing them.                |
# | PLIK: scripts/apply_headers.py                              |
# | ROLA: Automatyczne dodanie nagłówków i docstringów modułów  |
# |       do plików Python bez tych elementów.                  |
# +-------------------------------------------------------------+
"""
PL: Skrypt skanuje repozytorium, znajduje pliki .py i dodaje:
    (1) standardowy nagłówek CERTEUS (ASCII box) oraz
    (2) module docstring PL/EN — jeśli brakuje.
    Działa idempotentnie (nie dubluje nagłówków).
EN: The script scans the repo for .py files and adds:
    (1) a standard CERTEUS header block (ASCII box), and
    (2) a PL/EN module docstring — if missing.
    Idempotent (won't duplicate headers).
"""

from __future__ import annotations

import ast
import os
from collections.abc import Iterable
from pathlib import Path

# === KONFIG: które katalogi skanować ========================== #
SCAN_DIRS = [
    "cje",
    "clients",
    "core",
    "kernel",
    "plugins",
    "scripts",
    "services",
    "tests",
]

# Pliki, które zwykle pomijamy (jeśli chcesz – usuń z listy)
SKIP_FILES = {
    "plugins/__init__.py",  # często pusty
}


# === Generator nagłówka ======================================= #
def build_header(rel_path: str) -> str:
    """
    Zwraca standardowy nagłówek CERTEUS z dynamiczną ścieżką pliku.
    """
    box = (
        "# +-------------------------------------------------------------+\n"
        "# |                          CERTEUS                            |\n"
        "# +-------------------------------------------------------------+\n"
        f"# | FILE: {rel_path:<52}|\n"
        "# | ROLE: Project module.                                       |\n"
        f"# | PLIK: {rel_path:<52}|\n"
        "# | ROLA: Moduł projektu.                                       |\n"
        "# +-------------------------------------------------------------+\n"
    )
    return box


# === Domyślny module docstring ================================ #
DEFAULT_DOCSTRING = (
    '"""\n'
    "PL: Moduł CERTEUS – uzupełnij opis funkcjonalny.\n"
    "EN: CERTEUS module – please complete the functional description.\n"
    '"""\n'
)


# === Wykrywanie istniejącego nagłówka/docstringa ============== #
def has_header(text: str) -> bool:
    # szukamy znaku rozpoznawczego w pierwszych ~10 liniach
    head = "\n".join(text.splitlines()[:10])
    return "CERTEUS" in head and "FILE:" in head


def has_module_docstring(text: str) -> bool:
    try:
        tree = ast.parse(text)
        return ast.get_docstring(tree) is not None
    except Exception:
        # Jeśli nie możemy sparsować (np. niekompletna składnia), uznaj brak
        return False


# === Wstawianie nagłówka/docstringa =========================== #
def ensure_header_and_docstring(path: Path, project_root: Path) -> bool:
    """
    Modyfikuje plik, jeśli brakuje nagłówka lub docstringa.
    Zwraca True, jeśli plik został zmodyfikowany.
    """
    text = path.read_text(encoding="utf-8", errors="ignore")
    changed = False

    # zachowaj shebang/encoding (jeśli są w 1. linii)
    lines = text.splitlines(keepends=True)
    shebang = ""
    if lines and lines[0].startswith("#!"):
        shebang = lines[0]
        body = "".join(lines[1:])
    else:
        body = text

    # 1) Nagłówek
    if not has_header(body):
        rel = str(path.relative_to(project_root)).replace("\\", "/")
        header = build_header(rel)
        body = header + ("\n" if not body.startswith("\n") else "") + body
        changed = True

    # 2) Module docstring
    if not has_module_docstring(body):
        # Docstring musi być pierwszym statementem modułu → wstawiamy na samą górę body
        body = DEFAULT_DOCSTRING + ("\n" if not body.startswith("\n") else "") + body
        changed = True

    if changed:
        new_text = (shebang + body) if shebang else body
        # Na Windows trzymajmy CRLF (Git i tak poradzi sobie z autocrlf)
        new_text = new_text.replace("\r\n", "\n").replace("\n", os.linesep)
        path.write_text(new_text, encoding="utf-8")
    return changed


# === Skan repo i przetwarzanie ================================ #
def iter_python_files(root: Path, dirs: Iterable[str]) -> Iterable[Path]:
    for d in dirs:
        p = root / d
        if not p.exists():
            continue
        for py in p.rglob("*.py"):
            rel = str(py.relative_to(root)).replace("\\", "/")
            if rel in SKIP_FILES:
                continue
            yield py


def main() -> None:
    root = Path(__file__).resolve().parents[1]  # repo root
    changed_total = 0
    for py in iter_python_files(root, SCAN_DIRS):
        if ensure_header_and_docstring(py, root):
            changed_total += 1
            print(f"[UPDATED] {py.relative_to(root)}")
    print(f"\nDone. Files updated: {changed_total}")


if __name__ == "__main__":
    main()

```


===== FILE: scripts/build_flags_from_mapping.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/build_flags_from_mapping.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |              CERTEUS - Build Flags From Mapping             |
# +-------------------------------------------------------------+
# | PLIK: build_flags_from_mapping.py                           |
# | ROLA: Buduje zestaw flag (JSON) na podstawie reguł i mapy.  |
# +-------------------------------------------------------------+
#
# PL: Skrypt iteracyjnie uruchamia evaluator LEXLOG i na podstawie
#     brakujących przesłanek/wykluczników tworzy plik z flagami.
# EN: Iteratively runs the LEXLOG evaluator and, using missing
#     premises/excludes, produces a flags JSON for downstream tests.
"""
PL: Buduje plik flags JSON na podstawie mapowania przesłanek→flagi silnika.
    Uzupełnia brakujące przesłanki przez mapę P_* → ZNAMIE_* (MVP).
EN: Builds flags JSON from mapping of premises→engine flags. Fills missing
    premises via P_* → ZNAMIE_* mapping (MVP).
"""

from __future__ import annotations

import argparse
import json
import re
from collections.abc import Iterable as AbcIterable
from collections.abc import Mapping, MutableMapping, Sequence
from inspect import Signature, signature
from pathlib import Path
from typing import (
    Any,
    cast,
)

from services.lexlog_parser.evaluator import evaluate_rule
from services.lexlog_parser.mapping import load_mapping
from services.lexlog_parser.parser import parse_lexlog


def _to_set(xs: object | None) -> set[str]:
    """
    PL: Zamiana dowolnego wejścia na set[str] z jawnie typowanym elementem.
    EN: Convert arbitrary input to set[str] with explicit element typing.
    """
    out: set[str] = set()
    if xs is None:
        return out
    if isinstance(xs, str):
        out.add(xs)
        return out
    if isinstance(xs, AbcIterable) and not isinstance(xs, bytes | bytearray):
        for elem in cast(AbcIterable[object], xs):
            s: str = str(elem)
            out.add(s)
        return out
    out.add(str(xs))
    return out


def _detect_rule_id_from_text(text: str, prefer_token: str = "286") -> str | None:
    """
    PL: Wyszukaj identyfikatory po 'RULE' i preferuj ten zawierający token (np. '286').
    EN: Extract identifiers after 'RULE' and prefer one containing a token (e.g. '286').
    """
    ids = re.findall(r"^\s*RULE\s+([A-Za-z0-9_.\-]+)", text, flags=re.MULTILINE)
    if not ids:
        return None
    for rid in ids:
        if prefer_token and prefer_token.lower() in rid.lower():
            return rid
    return ids[0]


def _call_evaluate(ast: Any, rule_id: str | None, flags: Mapping[str, bool], ctx: Any) -> Any:
    """
    PL: Wywołaj evaluate_rule zgodnie z aktualną sygnaturą (różne wersje).
    EN: Call evaluate_rule according to the current signature (versions may differ).
    """
    sig: Signature = signature(evaluate_rule)
    params: Sequence[str] = list(sig.parameters.keys())

    kwargs: dict[str, Any] = {}
    if "ast" in params:
        kwargs["ast"] = ast
    if "rule_id" in params:
        if not rule_id:
            raise ValueError("evaluate_rule requires 'rule_id' but none was detected")
        kwargs["rule_id"] = rule_id
    if "flags" in params:
        kwargs["flags"] = flags
    if "ctx" in params:
        kwargs["ctx"] = ctx

    try:
        return evaluate_rule(**kwargs)  # type: ignore[misc]
    except TypeError:
        # Fallbacks for very old signatures
        try:
            if "rule_id" not in params:
                return evaluate_rule(ast, flags, ctx)  # type: ignore[misc]
        except TypeError:
            return evaluate_rule(ast, flags)  # type: ignore[misc]


def _step(
    ast: Any, rule_id: str | None, flags: MutableMapping[str, bool], ctx: Any
) -> tuple[bool, set[str], set[str], Any]:
    """
    PL: Jeden krok ewaluacji -> (ok, missing_premises, failing_excludes, raw_result).
    EN: One evaluation step -> (ok, missing_premises, failing_excludes, raw_result).
    """
    res = _call_evaluate(ast, rule_id, flags, ctx)
    ok = bool(getattr(res, "satisfied", False))
    missing = _to_set(getattr(res, "missing_premises", []) or [])
    failing = _to_set(getattr(res, "failing_excludes", []) or [])
    return ok, missing, failing, res


def main() -> None:
    parser = argparse.ArgumentParser(
        prog="build_flags_from_mapping",
        description="Build flags JSON by iteratively satisfying a rule.",
    )
    parser.add_argument(
        "--rules",
        default="packs/jurisdictions/PL/rules/kk.lex",
        help="Path to .lex file",
    )
    parser.add_argument(
        "--map",
        default="packs/jurisdictions/PL/rules/kk.mapping.json",
        help="Path to mapping.json",
    )
    parser.add_argument(
        "--out",
        default="packs/jurisdictions/PL/flags/lexenith_results_latest.json",
        help="Output JSON file",
    )
    parser.add_argument("--rule-id", default=None, help="Explicit rule_id (overrides auto-detection)")
    parser.add_argument(
        "--prefer-token",
        default="286",
        help="Token preferred when auto-selecting RULE id",
    )
    parser.add_argument("--max-iter", type=int, default=10, help="Max refinement iterations")
    args = parser.parse_args()

    rules_path = Path(args.rules)
    map_path = Path(args.map)
    out_path = Path(args.out)

    rules_text = rules_path.read_text(encoding="utf-8")
    ast = parse_lexlog(rules_text)
    ctx = load_mapping(map_path)

    rule_id = args.rule_id or _detect_rule_id_from_text(rules_text, prefer_token=args.prefer_token)
    if rule_id:
        print(f"[INFO] Using rule_id: {rule_id}")
    else:
        print("[WARN] Could not detect rule_id from .lex; trying evaluator without rule_id")

    flags: dict[str, bool] = {}

    for _ in range(args.max_iter):
        ok, miss, fail, _ = _step(ast, rule_id, flags, ctx)
        if ok:
            break

        if miss:
            for p in miss:
                engine_flag = ctx.premise_to_flag.get(p)  # P_* -> ZNAMIE_* jeśli dostępne
                key = str(engine_flag or p)
                flags[key] = True
        if fail:
            for k in fail:
                flags[str(k)] = False

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(json.dumps({"flags": flags}, ensure_ascii=True, indent=2), encoding="utf-8")
    print("[OK] wrote", out_path)
    print("[OK] True:", sorted([k for k, v in flags.items() if v]))
    print("[OK] False:", sorted([k for k, v in flags.items() if not v]))


if __name__ == "__main__":
    main()

```


===== FILE: scripts/check_drat.sh =====
```text
#!/usr/bin/env bash
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE / PLIK: scripts/check_drat.sh                                  |
# | ROLA / ROLE: DRAT checker (Linux/CI).                               |
# | OPIS / DESC: Presence & signature; optional drat-trim if CNF exists.|
# +=====================================================================+
set -Eeuo pipefail

ART_DIR="${ART_DIR:-proof_artifacts}"
DRAT_NAME="${DRAT_NAME:-z3.drat}"
SHA_FILE="${SHA_FILE:-}"
CNF_PATH="${CNF_PATH:-}"

while getopts ":d:f:s:" opt; do
  case "$opt" in
    d) ART_DIR="$OPTARG" ;;
    f) DRAT_NAME="$OPTARG" ;;
    s) SHA_FILE="$OPTARG" ;;
    *) echo "Usage: $0 [-d artifacts_dir] [-f drat_file] [-s sha256sum_file]" >&2; exit 2 ;;
  esac
done

DRAT_PATH="${ART_DIR}/${DRAT_NAME}"
[[ -f "$DRAT_PATH" ]] || { echo "::error::DRAT not found: ${DRAT_PATH}"; exit 1; }
[[ -s "$DRAT_PATH" ]] || { echo "::error::DRAT is empty: ${DRAT_PATH}"; exit 1; }

# Optional SHA check
if [[ -n "${SHA_FILE}" ]]; then
  [[ -f "${SHA_FILE}" ]] || { echo "::error::SHA file not found: ${SHA_FILE}"; exit 1; }
  sha256sum -c "${SHA_FILE}" || { echo "::error::SHA256 mismatch for ${DRAT_PATH}"; exit 1; }
fi

# Signature sanity
grep -q 'PROOF' "$DRAT_PATH"       || { echo "::error file=${DRAT_PATH}::Missing PROOF signature"; exit 3; }
grep -q 'format=drat' "$DRAT_PATH" || { echo "::error file=${DRAT_PATH}::Missing format=drat"; exit 3; }

# External checker (optional)
if command -v drat-trim >/dev/null 2>&1; then
  if [[ -n "${CNF_PATH}" && -f "${CNF_PATH}" ]]; then
    echo "ℹ️ drat-trim: CNF found → running verification…"
    drat-trim "${CNF_PATH}" "${DRAT_PATH}" -q || { echo "::error::drat-trim verification failed"; exit 4; }
  else
    echo "ℹ️ drat-trim available, but CNF missing → skipping external verification."
  fi
else
  echo "ℹ️ drat-trim not found; basic checks only."
fi

echo "✅ DRAT check passed for ${DRAT_PATH}"

```


===== FILE: scripts/check_hashes.ps1 =====
```text
# +======================================================================+
# |                               CERTEUS                                |
# +======================================================================+
# | FILE / PLIK: scripts/check_hashes.ps1                                |
# | ROLA / ROLE:                                                          |
# |  PL: Oblicza SHA256 plików DRAT/LFSC w proof_artifacts.               |
# |  EN: Compute SHA256 for DRAT/LFSC files in proof_artifacts.           |
# +======================================================================+
Param(
    [string]$Dir = "proof_artifacts"
)
$drat = Join-Path $Dir "z3.drat"
$lfsc = Join-Path $Dir "cvc5.lfsc"
if (Test-Path $drat) { Get-FileHash $drat -Algorithm SHA256 }
if (Test-Path $lfsc) { Get-FileHash $lfsc -Algorithm SHA256 }

```


===== FILE: scripts/check_headers.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/check_headers.py                |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: scripts/check_headers.py                              |
# | ROLE: Pre-commit checker for CERTEUS header & module docstr |
# | PLIK: scripts/check_headers.py                              |
# | ROLA: Walidator nagłówka CERTEUS i docstringa modułu        |
# +-------------------------------------------------------------+
"""
PL: Skrypt uruchamiany przez pre-commit. Dla podanych plików .py
    sprawdza obecność nagłówka CERTEUS oraz module docstringa.
    Akceptuje kilka form nagłówka (baner z 'CERTEUS', lub linie
    zawierające 'FILE:'/'PLIK:'), żeby nie blokować developmentu.

EN: Pre-commit script. For given .py files, validates that a
    CERTEUS header and a module docstring are present. It accepts
    multiple header styles (banner with 'CERTEUS' or lines that
    contain 'FILE:'/'PLIK:') to avoid blocking development.
"""

from __future__ import annotations

# === stdlib imports (one-per-line for Ruff) ===================
import argparse
import ast
import sys
from collections.abc import Iterable
from pathlib import Path

# ==============================================================
# == BLOCK: detection helpers                                  =
# ==============================================================


def _read_head(path: Path, head_lines: int = 30) -> str:
    """Read only the first N lines of the file for header detection."""
    try:
        lines: list[str] = []
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            for i, line in enumerate(f):
                if i >= head_lines:
                    break
                lines.append(line)
        head: str = "".join(lines)
        return head
    except Exception:
        return ""


def _has_module_docstring(full_text: str) -> bool:
    """Parse AST and check for module docstring presence."""
    try:
        tree = ast.parse(full_text)
        return ast.get_docstring(tree) is not None
    except Exception:
        return False


def _has_certeus_header(head_text: str) -> bool:
    """
    Flexible header detection:
    - any line in head that contains 'CERTEUS' (banner or single line), OR
    - presence of 'FILE:' and/or 'PLIK:' markers, OR
    - SPDX license line if you decide to add one later.

    This matches the banner inserted by scripts/apply_headers.py.
    """
    head_lower: str = head_text.lower()
    if "certeus" in head_lower:
        return True
    if "file:" in head_lower or "plik:" in head_lower:
        return True
    if "spdx-license-identifier" in head_lower:
        return True
    return False


# ==============================================================
# == BLOCK: main check routine                                  =
# ==============================================================


def check_files(paths: Iterable[Path]) -> int:
    """
    Return exit code (0 ok, 1 failure). Prints issues as:
    [HEADER MISSING] <path>
    [DOCSTRING MISSING] <path>
    """
    missing: int = 0
    for p in paths:
        if p.suffix != ".py":
            continue
        if not p.exists():
            continue

        head: str = _read_head(p)
        try:
            full: str = p.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            full = head  # fallback

        # --- header
        if not _has_certeus_header(head):
            print(f"[HEADER MISSING] {p}")
            missing += 1

        # --- module docstring
        if not _has_module_docstring(full):
            print(f"[DOCSTRING MISSING] {p}")
            missing += 1

    return 0 if missing == 0 else 1


# ==============================================================
# == BLOCK: CLI entrypoint                                      =
# ==============================================================


def main() -> int:
    parser = argparse.ArgumentParser(
        description="CERTEUS header/docstring gate",
    )
    parser.add_argument("files", nargs="*", help="Files passed by pre-commit")
    args = parser.parse_args()

    paths = [Path(f) for f in args.files]
    return check_files(paths)


if __name__ == "__main__":
    sys.exit(main())

```


===== FILE: scripts/check_lfsc.sh =====
```text
#!/usr/bin/env bash
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE / PLIK: scripts/check_lfsc.sh                                  |
# | ROLA / ROLE: LFSC checker (Linux/CI).                               |
# | OPIS / DESC: Presence & signature; optional cvc5/lfsc-checker       |
# |             if SMT2 exists.                                         |
# +=====================================================================+
set -Eeuo pipefail

ART_DIR="${ART_DIR:-proof_artifacts}"
LFSC_NAME="${LFSC_NAME:-cvc5.lfsc}"
SHA_FILE="${SHA_FILE:-}"
SMT2_PATH="${SMT2_PATH:-}"

while getopts ":d:f:s:" opt; do
  case "$opt" in
    d) ART_DIR="$OPTARG" ;;
    f) LFSC_NAME="$OPTARG" ;;
    s) SHA_FILE="$OPTARG" ;;
    *) echo "Usage: $0 [-d artifacts_dir] [-f lfsc_file] [-s sha256sum_file]" >&2; exit 2 ;;
  esac
done

LFSC_PATH="${ART_DIR}/${LFSC_NAME}"
[[ -f "$LFSC_PATH" ]] || { echo "::error::LFSC not found: ${LFSC_PATH}"; exit 1; }
[[ -s "$LFSC_PATH" ]] || { echo "::error::LFSC is empty: ${LFSC_PATH}"; exit 1; }

# Optional SHA check
if [[ -n "${SHA_FILE}" ]]; then
  [[ -f "${SHA_FILE}" ]] || { echo "::error::SHA file not found: ${SHA_FILE}"; exit 1; }
  sha256sum -c "${SHA_FILE}" || { echo "::error::SHA256 mismatch for ${LFSC_PATH}"; exit 1; }
fi

# Signature sanity
grep -q 'PROOF' "$LFSC_PATH"       || { echo "::error file=${LFSC_PATH}::Missing PROOF signature"; exit 3; }
grep -q 'format=lfsc' "$LFSC_PATH" || { echo "::error file=${LFSC_PATH}::Missing format=lfsc"; exit 3; }

# External checker (optional)
if command -v cvc5 >/dev/null 2>&1; then
  if [[ -n "${SMT2_PATH}" && -f "${SMT2_PATH}" ]]; then
    echo "ℹ️ cvc5: SMT2 found → attempting proof check."
    cvc5 --proof-mode=lfsc --check-proofs "${SMT2_PATH}" >/dev/null || { echo "::error::cvc5 proof check failed"; exit 4; }
  else
    echo "ℹ️ cvc5 available, but SMT2 missing → skipping external verification."
  fi
elif command -v lfsc-checker >/dev/null 2>&1; then
  echo "ℹ️ lfsc-checker found; running…"
  lfsc-checker "${LFSC_PATH}" || { echo "::error::lfsc-checker failed"; exit 4; }
else
  echo "ℹ️ no LFSC checker found; basic checks only."
fi

echo "✅ LFSC check passed for ${LFSC_PATH}"

```


===== FILE: scripts/check_proofs.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/check_proofs.py                 |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |            Proof-Gate: Artifact Integrity Checker           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: scripts/check_proofs.py
# License: Apache-2.0
# Description (PL): Weryfikacja SHA256 artefaktów DRAT/LFSC.
# Description (EN): Verifies SHA256 of DRAT/LFSC artifacts.
# Style Guide: PL/EN docs, labeled blocks. (See README)
# ────────────────────────────────────────────────────────────────────────────────

"""
PL: Sprawdza istnienie plików {z3.drat,cvc5.lfsc} i ich plików *.sha256,
    porównuje wyliczone sumy SHA256 z zapisanymi. Zwraca kod 0/1.

EN: Checks presence of {z3.drat,cvc5.lfsc} and their *.sha256 files,
    compares computed SHA256 with recorded ones. Returns code 0/1.
"""

# [BLOCK: IMPORTS / IMPORTY]
from __future__ import annotations

import argparse
import hashlib
import sys
from pathlib import Path

# [BLOCK: CLI]
parser = argparse.ArgumentParser()
parser.add_argument(
    "--dir",
    default="proof-artifacts",
    help="PL: Katalog artefaktów | EN: Artifacts directory",
)
args = parser.parse_args()
d = Path(args.dir)

# [BLOCK: CHECK FILES / SPRAWDŹ PLIKI]
required = ["z3.drat", "z3.drat.sha256", "cvc5.lfsc", "cvc5.lfsc.sha256"]
missing = [name for name in required if not (d / name).exists()]
if missing:
    print(f"ERROR: Missing files: {', '.join(missing)}")
    sys.exit(1)


# [BLOCK: VERIFY / WERYFIKUJ]
def verify(path: Path, sha_path: Path) -> bool:
    recorded = sha_path.read_text(encoding="utf-8").strip().split()[0]
    computed = hashlib.sha256(path.read_bytes()).hexdigest()
    ok = computed == recorded
    print(f"{path.name}: {'OK' if ok else 'MISMATCH'} (computed={computed}, recorded={recorded})")
    return ok


ok1 = verify(d / "z3.drat", d / "z3.drat.sha256")
ok2 = verify(d / "cvc5.lfsc", d / "cvc5.lfsc.sha256")

sys.exit(0 if (ok1 and ok2) else 1)

```


===== FILE: scripts/ed25519_keytool.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE: scripts/ed25519_keytool.py                                    |
# | ROLE: Generate Ed25519 keypair and export public key (Base64URL).   |
# +=====================================================================+
from __future__ import annotations

import base64
from argparse import ArgumentParser
from pathlib import Path

from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey


def main() -> None:
    ap = ArgumentParser()
    ap.add_argument("--gen", action="store_true")
    ap.add_argument("--out-pem", default="ed25519-private.pem")
    ap.add_argument("--shell", choices=["bash", "powershell"], default="powershell")
    args = ap.parse_args()

    if args.gen:
        sk = Ed25519PrivateKey.generate()
        pem = sk.private_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PrivateFormat.PKCS8,
            encryption_algorithm=serialization.NoEncryption(),
        )
        Path(args.out_pem).write_bytes(pem)
        pk = sk.public_key().public_bytes(
            encoding=serialization.Encoding.Raw,
            format=serialization.PublicFormat.Raw,
        )
        b64u = base64.urlsafe_b64encode(pk).rstrip(b"=").decode()
        if args.shell == "bash":
            print(f'export ED25519_PRIVKEY_PEM="{Path(args.out_pem).resolve()}"')
            print(f'export ED25519_PUBKEY_B64URL="{b64u}"')
        else:
            print(f'$env:ED25519_PRIVKEY_PEM = "{Path(args.out_pem).resolve()}"')
            print(f"$env:ED25519_PUBKEY_B64URL = \"{b64u}\"")
    else:
        # Print Base64URL for an existing PEM (public key)
        ap = ArgumentParser()
        ap.add_argument("--pem", default="ed25519-public.pem")
        args = ap.parse_args()
        raw = Path(args.pem).read_bytes()
        pub = serialization.load_pem_public_key(raw)
        b = pub.public_bytes(
            encoding=serialization.Encoding.Raw,
            format=serialization.PublicFormat.Raw,
        )
        print(base64.urlsafe_b64encode(b).rstrip(b"=").decode())


if __name__ == "__main__":
    main()

```


===== FILE: scripts/fix_bom.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE: scripts/fix_bom.py                                            |
# | ROLE: Re-save *.json as UTF-8 (no BOM), fixing utf-8-sig            |
# +=====================================================================+
# ----Bloki----- IMPORTY
from __future__ import annotations

import json
import os
from pathlib import Path


# ----Bloki----- MAIN
def main() -> None:
    root = Path(os.getenv("PROOF_BUNDLE_DIR") or "./data/public_pco")
    for p in root.glob("*.json"):
        data = json.loads(p.read_text(encoding="utf-8-sig"))
        p.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"fixed: {p}")


if __name__ == "__main__":
    main()

```


===== FILE: scripts/generate_proofs.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE / PLIK: scripts/generate_proofs.py                             |
# | ROLE / ROLA:                                                         |
# |   EN: CLI to generate simulated/stub proof artifacts for Proof Gate |
# |   PL: CLI do generowania symulowanych/stubowych artefaktów dowodów  |
# | DATE: 2025-08-17                                                    |
# +=====================================================================+

"""
PL:
  Skrypt CLI generujący pliki dowodów do katalogu wyjściowego (parametr --out).
  Obsługuje tryby:
    - "simulate"  -> plik z treścią zawierającą losowy nonce (dla realizmu),
    - "stub"      -> minimalny stub bez nonce.
  Obsługiwane formaty:
    - "drat"  -> z3.drat
    - "lfsc"  -> cvc5.lfsc
  Dodatkowo może tworzyć pliki wejściowe:
    - input.smt2 (SMT-LIB),
    - input.cnf  (DIMACS),
  oraz opcjonalnie wygenerować pokwitowanie (receipt) z hashami SHA-256.

EN:
  CLI script that produces proof files into the given output directory (--out).
  Modes:
    - "simulate" -> file content includes a random nonce (more realistic),
    - "stub"     -> minimal stub content, no nonce.
  Supported formats:
    - "drat"  -> z3.drat
    - "lfsc"  -> cvc5.lfsc
  It can also emit input files:
    - input.smt2 (SMT-LIB),
    - input.cnf  (DIMACS),
  and optionally write a receipt (manifest) with SHA-256 hashes.
"""

from __future__ import annotations

# === IMPORTS =================================================== #
import argparse
import hashlib
import json
import os
import random
import string
import sys
from collections.abc import Iterable
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Final

# === CONSTANTS / KONSTANTY ===================================== #

FORMAT_TO_FILE: Final[dict[str, str]] = {
    "drat": "z3.drat",
    "lfsc": "cvc5.lfsc",
}

FORMAT_TO_SOLVER: Final[dict[str, str]] = {
    "drat": "z3",
    "lfsc": "cvc5",
}

SMT2_FILE: Final[str] = "input.smt2"
CNF_FILE: Final[str] = "input.cnf"
RECEIPT_FILE: Final[str] = "provenance_receipt_v1.json"


# === LOGGING (PL/EN) =========================================== #


def _lazy_console():
    """
    PL: Próbujemy użyć utils.console (Twój projekt). W razie braku – fallback na print.
    EN: Try to import project's utils.console; fall back to print-based logger if missing.
    """
    try:
        from utils.console import error, info, success  # type: ignore

        return info, success, error
    except Exception:

        def _info(msg: str) -> None:
            print(f"[INFO] {msg}")

        def _success(msg: str) -> None:
            print(f"[OK]   {msg}")

        def _error(msg: str) -> None:
            print(f"[ERR]  {msg}", file=sys.stderr)

        return _info, _success, _error


# === HELPERS / POMOCNICZE ===================================== #


def _write_text(path: Path, content: str) -> None:
    """
    PL: Zapisuje tekst UTF-8 z LF; tworzy katalogi pośrednie.
    EN: Writes UTF-8 text with LF; creates parent dirs.
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8", newline="\n")


def _write_bytes(path: Path, data: bytes) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "wb") as f:
        f.write(data)


def _sha256_hex(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def _rand_token(n: int = 24) -> str:
    alphabet = string.ascii_letters + string.digits
    return "".join(random.choices(alphabet, k=n))


def _normalize_formats(formats: list[str] | None) -> list[str]:
    if not formats:
        return list(FORMAT_TO_FILE.keys())
    out: list[str] = []
    for f in formats:
        lf = f.lower()
        if lf in FORMAT_TO_FILE:
            out.append(lf)
    return out


# === DATA / DANE =============================================== #


@dataclass
class Generated:
    """
    PL: Informacja o wygenerowanym pliku.
    EN: Info about a generated file.
    """

    kind: str  # "proof" | "input" | "receipt"
    format: str | None  # "drat"/"lfsc" for proofs, None for others
    path: Path
    sha256: str


# === CORE / LOGIKA ============================================= #


def _emit_proof_file(fmt: str, out_dir: Path, mode: str, seed: int | None) -> Path:
    """
    PL: Tworzy plik dowodu w zadanym formacie i trybie.
    EN: Creates a proof file for the given format and mode.
    """
    if seed is not None:
        random.seed(seed)

    solver = FORMAT_TO_SOLVER[fmt]
    fname = FORMAT_TO_FILE[fmt]
    dst = out_dir / fname

    if mode == "simulate":
        # UWAGA: poniższy komunikat "Created simulated proof with content:"
        # jest istotny dla istniejących testów E2E.
        payload = f"PROOF::format={fmt}::solver={solver}::nonce={_rand_token(24)}"
        _write_text(dst, payload)
    elif mode == "stub":
        payload = f"PROOF-STUB::format={fmt}::solver={solver}"
        _write_text(dst, payload)
    else:
        raise ValueError(f"Unknown mode: {mode}")

    return dst


def _emit_inputs(out_dir: Path, smt2: bool, cnf: bool) -> list[Path]:
    """
    PL: Opcjonalnie generuje pliki wejściowe SMT2 / CNF.
    EN: Optionally generates SMT2 / CNF input files.
    """
    created: list[Path] = []

    if smt2:
        smt_path = out_dir / SMT2_FILE
        smt_content = "\n".join(
            [
                "(set-logic ALL)",
                "(set-info :source |CERTEUS simulated SMT2 input|)",
                "(assert true)",
                "(check-sat)",
                "(exit)",
            ]
        )
        _write_text(smt_path, smt_content)
        created.append(smt_path)

    if cnf:
        cnf_path = out_dir / CNF_FILE
        # Minimal, valid DIMACS header; no clauses.
        _write_text(cnf_path, "p cnf 1 0\n")
        created.append(cnf_path)

    return created


def _emit_receipt(out_dir: Path, items: list[Generated]) -> Path:
    """
    PL: Zapisuje pokwitowanie (provenance receipt) z hashami i timestampem.
    EN: Writes a provenance receipt with hashes and timestamp.
    """
    now = datetime.now(timezone.utc).isoformat()
    manifest = {
        "version": "provenance_receipt_v1",
        "created_at": now,
        "artifacts": [
            {
                "kind": it.kind,
                "format": it.format,
                "path": str(Path(os.path.relpath(it.path, out_dir)).as_posix()),
                "sha256": it.sha256,
            }
            for it in items
        ],
    }
    dst = out_dir / RECEIPT_FILE
    _write_text(dst, json.dumps(manifest, ensure_ascii=False, indent=2))
    return dst


def generate_proofs(
    out: Path,
    formats: list[str] | None = None,
    mode: str = "simulate",
    with_inputs: bool = False,
    seed: int | None = None,
    write_receipt: bool = False,
) -> list[Path]:
    """
    PL:
      Generuje pliki dowodów w katalogu `out`. Zwraca listę utworzonych ścieżek.
      - formats: podzbiór {"drat","lfsc"}; None => oba.
      - mode: "simulate" | "stub"
      - with_inputs: czy tworzyć input.smt2 i input.cnf
      - seed: ustala ziarno losowości (deterministyczny nonce w simulate)
      - write_receipt: czy zapisać manifest z hashami (provenance_receipt_v1.json)

    EN:
      Generates proof files in `out`. Returns list of created paths.
      - formats: subset {"drat","lfsc"}; None => both.
      - mode: "simulate" | "stub"
      - with_inputs: whether to create input.smt2 and input.cnf
      - seed: seed for reproducible nonce in simulate mode
      - write_receipt: write a manifest with SHA-256 hashes
    """
    log_info, log_success, log_error = _lazy_console()

    fs = _normalize_formats(formats)
    if not fs:
        log_error("No valid formats provided.")
        return []

    out.mkdir(parents=True, exist_ok=True)

    created: list[Generated] = []

    # Proofs
    for fmt in fs:
        dst = _emit_proof_file(fmt, out, mode, seed)
        sha = _sha256_hex(dst)
        created.append(Generated(kind="proof", format=fmt, path=dst, sha256=sha))

        if mode == "simulate":
            # Ten log jest celowy (używany przez testy):
            log_success(f"Created simulated proof with content: {dst}")
        else:
            log_success(f"Created stub proof: {dst}")

    # Inputs
    if with_inputs:
        for ip in _emit_inputs(out, smt2=True, cnf=True):
            created.append(Generated(kind="input", format=None, path=ip, sha256=_sha256_hex(ip)))
            log_info(f"Created input file: {ip}")

    # Receipt
    if write_receipt:
        receipt_path = _emit_receipt(out, created)  # <-- KLUCZOWE: nowa nazwa zmiennej, nie nadpisuj write_receipt
        created.append(
            Generated(
                kind="receipt",
                format=None,
                path=receipt_path,
                sha256=_sha256_hex(receipt_path),
            )
        )
        log_info(f"Wrote provenance receipt: {receipt_path}")

    # Summary
    log_info(f"Total artifacts: {len(created)}")
    for it in created:
        log_info(f"Wrote: {it.path}")

    return [it.path for it in created]


# === CLI ======================================================= #


def _build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        prog="generate_proofs",
        description="Generate (simulated/stub) proof artifacts into an output directory.",
    )
    p.add_argument(
        "--out",
        type=Path,
        required=True,
        help="Output directory for generated files.",
    )
    p.add_argument(
        "--mode",
        choices=["simulate", "stub"],
        default="simulate",
        help='Generation mode (default: "simulate").',
    )
    p.add_argument(
        "--formats",
        nargs="*",
        choices=sorted(FORMAT_TO_FILE.keys()),
        default=None,
        help="Formats to generate (space-separated), e.g. --formats drat lfsc. Default: both.",
    )
    p.add_argument(
        "--with-inputs",
        action="store_true",
        help="Also create input.smt2 and input.cnf.",
    )
    p.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Optional RNG seed for reproducible nonce in simulate mode.",
    )
    p.add_argument(
        "--receipt",
        action="store_true",
        help=f"Write provenance receipt JSON ({RECEIPT_FILE}).",
    )
    return p


def main(argv: Iterable[str] | None = None) -> int:
    log_info, _log_success, log_error = _lazy_console()

    try:
        args = _build_arg_parser().parse_args(list(argv) if argv is not None else None)
        files = generate_proofs(
            args.out,
            formats=args.formats,
            mode=args.mode,
            with_inputs=args.with_inputs,
            seed=args.seed,
            write_receipt=args.receipt,
        )
        if not files:
            log_error("No artifacts generated.")
            return 2
        return 0
    except Exception as exc:  # pragma: no cover
        log_error(f"Generation failed: {exc}")
        return 1


if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: scripts/hooks/block_key_extensions.py =====
```text
#!/usr/bin/env python3
# +=============================================================+
# |                      CERTEUS — GUARD                        |
# +=============================================================+
# | FILE: scripts/hooks/block_key_extensions.py                 |
# | ROLE: Block committing key material by extension.           |
# | NOTE: Used by pre-commit. Fails commit if any staged file   |
# |       matches forbidden extensions (pem,key,der,pfx,p12).   |
# +=============================================================+
from __future__ import annotations

import sys
from pathlib import Path

FORBIDDEN_EXT = {".pem", ".key", ".der", ".pfx", ".p12"}


def main(argv: list[str]) -> int:
    bad: list[str] = []
    for a in argv:
        p = Path(a)
        if p.suffix.lower() in FORBIDDEN_EXT:
            bad.append(str(p))
    if bad:
        print("⛔  BLOCKED: key material must not be committed:")
        for b in bad:
            print(f"   - {b}")
        print("\nHint: keep keys local; use env/JWKS or secrets manager instead.")
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))

```


===== FILE: scripts/lexlog_eval_smoke.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/lexlog_eval_smoke.py            |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |               CERTEUS - LEXLOG Smoke Evaluator              |
# +-------------------------------------------------------------+
# | PLIK: lexlog_eval_smoke.py                                  |
# | ROLA: Szybki test spełnienia art. 286 k.k. na podstawie     |
# |       reguł z kk.lex i mapowania kk.mapping.json.          |
# +-------------------------------------------------------------+
#
# PL: Jeśli nie podasz pliku z flagami (--flags), skrypt użyje
#     domyślnych trzech ról faktów, wymaganych przez art. 286:
#       - intent_financial_gain
#       - act_deception
#       - detrimental_property_disposal
#
# EN: If you don’t pass a flags file (--flags), the script uses
#     the default three fact roles required by Art. 286:
#       - intent_financial_gain
#       - act_deception
#       - detrimental_property_disposal
"""
PL: Smoke-test Jądra Prawdy: wczytuje flags JSON i sprawdza spełnialność.
EN: Truth Engine smoke test: loads flags JSON and checks satisfiability.
"""

from __future__ import annotations

import argparse
import json
from collections.abc import Mapping
from pathlib import Path

from services.lexlog_parser.evaluator import evaluate_rule
from services.lexlog_parser.mapping import load_mapping
from services.lexlog_parser.parser import parse_lexlog

RULE_ID = "R_286_OSZUSTWO"
RULES_PATH = Path("packs/jurisdictions/PL/rules/kk.lex")
MAP_PATH = Path("packs/jurisdictions/PL/rules/kk.mapping.json")

DEFAULT_FLAGS: Mapping[str, bool] = {
    "intent_financial_gain": True,
    "act_deception": True,
    "detrimental_property_disposal": True,
}


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--flags", type=str, default="", help="Ścieżka do pliku JSON z kluczem 'flags'")
    args = ap.parse_args()

    if not RULES_PATH.exists():
        raise SystemExit(f"[FATAL] Missing rules file: {RULES_PATH}")
    if not MAP_PATH.exists():
        raise SystemExit(f"[FATAL] Missing mapping file: {MAP_PATH}")

    ast = parse_lexlog(RULES_PATH.read_text(encoding="utf-8"))
    ctx = load_mapping(MAP_PATH)

    if args.flags:
        flags_path = Path(args.flags)
        if not flags_path.exists():
            raise SystemExit(f"[ERROR] Missing flags file: {flags_path}")
        data = json.loads(flags_path.read_text(encoding="utf-8"))
        flags = data.get("flags", {})
    else:
        flags = dict(DEFAULT_FLAGS)

    # Now evaluate explicit rule id (4-arg signature).
    res = evaluate_rule(ast, RULE_ID, flags, ctx)

    if getattr(res, "satisfied", False):
        print("[SUCCESS] art. 286 satisfied")
    else:
        missing = getattr(res, "missing_premises", []) or []
        failing = getattr(res, "failing_excludes", []) or []
        print("[ERROR] art. 286 NOT satisfied")
        if missing:
            print("  missing_premises:", sorted(missing))
        if failing:
            print("  failing_excludes:", sorted(failing))


if __name__ == "__main__":
    main()

```


===== FILE: scripts/lexlog_eval_smoke_fallback.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/lexlog_eval_smoke_fallback.py   |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: scripts/lexlog_eval_smoke_fallback.py                 |
# | ROLE: Fallback smoke runner for LEXLOG evaluation           |
# | PLIK: scripts/lexlog_eval_smoke_fallback.py                 |
# | ROLA: Skrypt awaryjny: buduje flagi i uruchamia smoke eval  |
# +-------------------------------------------------------------+

"""
CERTEUS — Lexlog Smoke Eval (Fallback)
PL: Skrypt pomocniczy budujący plik flag (na podstawie mappingu) i
    uruchamiający test dymny oceny LEXLOG (art. 286 k.k.).
EN: Auxiliary script that builds the flags file (from mapping) and
    runs the LEXLOG smoke evaluation (Art. 286).
"""

import os
import subprocess
import sys

FLAGS_PATH = os.path.join("packs", "jurisdictions", "PL", "flags", "lexenith_results_latest.json")


def main():
    print("[INFO] Building flags from mapping for R_286_OSZUSTWO...")
    subprocess.run(
        [
            sys.executable,
            "scripts/build_flags_from_mapping.py",
            "--rule-id",
            "R_286_OSZUSTWO",
        ],
        check=True,
    )
    print("[INFO] Running smoke with generated flags...")
    subprocess.run(
        [sys.executable, "scripts/lexlog_eval_smoke.py", "--flags", FLAGS_PATH],
        check=True,
    )


if __name__ == "__main__":
    main()

```


===== FILE: scripts/make_merkle_sample.py =====
```text
#!/usr/bin/env python3
# +======================================================================+
# |                               CERTEUS                                |
# +======================================================================+
# | FILE / PLIK: scripts/make_merkle_sample.py                           |
# | ROLE / ROLA:                                                          |
# |  EN: Build demo public bundle (non-empty Merkle path) and sign it.   |
# |  PL: Buduje przykładowy publiczny bundle (niepusta ścieżka Merkle)   |
# |      i podpisuje go.                                                 |
# +======================================================================+
# Założenia:
# - czyta klucz prywatny Ed25519 z PEM: ED25519_PRIVKEY_PEM
# - zapisuje do $PROOF_BUNDLE_DIR (fallback: ./data/public_pco)
# - materiał demo: LFSC/SMT2 w razie braku plików podawanych flagami
# ----Bloki----- IMPORTY (PEP 8/PEP 585)
#   - max 120 znaków/linia

from __future__ import annotations

import base64
import hashlib
import json
import os
import sys
from argparse import ArgumentParser
from collections.abc import Iterable
from pathlib import Path
from typing import Any, TypedDict

from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey, Ed25519PublicKey


# ----Bloki----- TYPY
class MerkleStep(TypedDict):
    sibling: str  # hex
    dir: str  # 'L' | 'R'


# ----Bloki----- POMOCNICZE
DEFAULT_DIR = os.getenv("PROOF_BUNDLE_DIR", "./data/public_pco")


def _hx_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _is_hex64(x: str) -> bool:
    return isinstance(x, str) and len(x) == 64 and all(c in "0123456789abcdef" for c in x.lower())


def sha256_hex_utf8(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def compute_bundle_hash_hex(pub: dict[str, Any]) -> str:
    payload = {"smt2_hash": pub["smt2_hash"], "lfsc_sha256": sha256_hex_utf8(pub["lfsc"])}
    if pub.get("drat") is not None:
        payload["drat_sha256"] = sha256_hex_utf8(pub["drat"])
    blob = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
    return hashlib.sha256(blob).hexdigest()


def merkle_root_from_path(leaf_hex: str, path: Iterable[MerkleStep]) -> str:
    cur = bytes.fromhex(leaf_hex)
    for step in path:
        sib = bytes.fromhex(step["sibling"])
        if step["dir"] == "L":
            cur = hashlib.sha256(sib + cur).digest()
        elif step["dir"] == "R":
            cur = hashlib.sha256(cur + sib).digest()
        else:
            raise ValueError(f"Invalid dir: {step['dir']!r}")
    return cur.hex()


def canonical_digest_hex(pub: dict[str, Any], merkle_root_hex: str) -> str:
    parts = [
        hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest(),
        pub["smt2_hash"].lower(),
        sha256_hex_utf8(pub["lfsc"]),
    ]
    if pub.get("drat") is not None:
        parts.append(sha256_hex_utf8(pub["drat"]))
    parts.append(merkle_root_hex.lower())
    msg = b"".join(bytes.fromhex(x) for x in parts)
    return hashlib.sha256(msg).hexdigest()


def ed25519_from_b64url(x_b64u: str) -> Ed25519PublicKey:
    pad = "=" * (-len(x_b64u) % 4)
    raw = base64.urlsafe_b64decode(x_b64u + pad)
    return Ed25519PublicKey.from_public_bytes(raw)


# ----Bloki----- BUNDLE (demo)
def make_bundle(
    outdir: Path,
    rid: str,
    lfsc_path: str | None,
    smt2_path: str | None,
    signature_b64u: str | None,
    *,
    allow_missing: bool = True,
    lfsc_text: str | None = None,
    smt2_text: str | None = None,
) -> Path:
    outdir.mkdir(parents=True, exist_ok=True)

    # Materiał: LFSC/SMT2 – z pliku lub inline albo fallback demo
    lfsc = lfsc_text or (
        Path(lfsc_path).read_text(encoding="utf-8") if lfsc_path and Path(lfsc_path).exists() else None
    )
    smt2 = smt2_text or (
        Path(smt2_path).read_text(encoding="utf-8") if smt2_path and Path(smt2_path).exists() else None
    )
    if not allow_missing and (lfsc is None or smt2 is None):
        print("Missing LFSC/SMT2 and --allow-missing not set", file=sys.stderr)
        raise SystemExit(2)
    if lfsc is None:
        lfsc = "(lfsc proof for demo-002)"
    if smt2 is None:
        smt2 = "(set-logic ALL)\n(check-sat)"

    pub = {
        "rid": rid,
        "smt2_hash": hashlib.sha256(smt2.encode("utf-8")).hexdigest(),
        "lfsc": lfsc,
        "drat": None,
    }

    # Ścieżka Merkle – przykład niepusty
    path: list[MerkleStep] = [
        {"sibling": "a" * 64, "dir": "L"},
        {"sibling": "b" * 64, "dir": "R"},
    ]

    rid_hash_hex = hashlib.sha256(rid.encode("utf-8")).hexdigest()
    bundle_hash_hex = compute_bundle_hash_hex(pub)
    leaf_hex = hashlib.sha256(bytes.fromhex(rid_hash_hex) + bytes.fromhex(bundle_hash_hex)).hexdigest()
    merkle_root_hex = merkle_root_from_path(leaf_hex, path)

    digest_hex = canonical_digest_hex(pub | {"merkle_proof": path}, merkle_root_hex)

    # Podpis – jeśli nie podano detached signature, generujemy z PEM (ENV)
    if signature_b64u is None:
        pem_path = os.getenv("ED25519_PRIVKEY_PEM")
        if not pem_path or not Path(pem_path).exists():
            print("Missing ED25519_PRIVKEY_PEM", file=sys.stderr)
            raise SystemExit(2)
        sk_any = serialization.load_pem_private_key(Path(pem_path).read_bytes(), password=None)
        if not isinstance(sk_any, Ed25519PrivateKey):
            print("PEM is not Ed25519 private key", file=sys.stderr)
            raise SystemExit(2)
        sig = sk_any.sign(bytes.fromhex(digest_hex))
        signature_b64u = base64.urlsafe_b64encode(sig).rstrip(b"=").decode()

    out = {
        **pub,
        "merkle_proof": path,
        "signature": signature_b64u,
        "issued_at": "2025-08-19T12:00:00Z",
    }

    out_path = outdir / f"{rid}.json"
    out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
    return out_path


# --- blok --- CLI --------------------------------------------------------------
def _parse_args() -> Any:
    p = ArgumentParser(description="Build demo public bundle (non-empty Merkle path) and sign it.")
    p.add_argument("--rid", help="Resource ID (64-hex). If absent, derived from seed.")
    p.add_argument("--seed", help="Seed text for RID (SHA-256)", default=None)
    p.add_argument("--lfsc", dest="lfsc_path", help="Path to LFSC file", default=None)
    p.add_argument("--smt2", dest="smt2_path", help="Path to SMT2 file", default=None)
    p.add_argument("--signature", help="Detached signature (>= 40 chars)", default=None)
    p.add_argument("--outdir", help="Output dir (defaults to PROOF_BUNDLE_DIR)", default=DEFAULT_DIR)
    p.add_argument("--echo-url", action="store_true", help="Print GET URL for API")
    p.add_argument("--allow-missing", action="store_true", help="Use default text if --lfsc/--smt2 files are missing")
    p.add_argument("--lfsc-text", dest="lfsc_text", default=None, help="Inline LFSC text (overrides --lfsc file)")
    p.add_argument("--smt2-text", dest="smt2_text", default=None, help="Inline SMT2 text (overrides --smt2 file)")
    return p.parse_args()


def main() -> int:
    args = _parse_args()

    # RID resolve
    if args.rid:
        rid = args.rid.strip().lower()
        if not _is_hex64(rid):
            print("[RID] must be 64-hex (lowercase).", flush=True)
            return 2
    else:
        seed = args.seed or "rid-demo"
        rid = _hx_text(seed)

    outdir = Path(args.outdir)
    out_path = make_bundle(
        outdir,
        rid,
        args.lfsc_path,
        args.smt2_path,
        args.signature,
        allow_missing=args.allow_missing,
        lfsc_text=args.lfsc_text,
        smt2_text=args.smt2_text,
    )

    print(f"[OK] bundle written: {out_path}")
    if args.echo_url:
        print(f"[GET] http://127.0.0.1:8000/pco/public/{rid}")
    return 0


# --- blok --- Entrypoint -------------------------------------------------------
if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: scripts/make_pco_bundle.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: scripts/make_pco_bundle.py                          |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: Create a minimal public PCO bundle JSON under PROOF_BUNDLE_DIR |
# |      (0 PII). Lets you choose RID (seed/hex), LFSC/SMT2 sources and |
# |      signature.                                                     |
# |  PL: Tworzy minimalny publiczny bundle PCO (0 PII) w PROOF_BUNDLE_DIR|
# |      z możliwością wyboru RID (seed/hex), źródeł LFSC/SMT2 i podpisu.|
# +=====================================================================+

r"""
Użycie / Usage (PowerShell):
  # 1) na podstawie "seed" (RID = sha256(seed))
  uv run python scripts/make_pco_bundle.py --seed rid-demo

  # 2) z własnym RID (64-hex), LFSC i SMT2 z plików
  uv run python scripts/make_pco_bundle.py `
    --rid 0123...cafe `
    --lfsc F:\dowody\proof.lfsc `
    --smt2 F:\modele\model.smt2 `
    --signature 0f0f...

  # 3) od razu pokaż URL GET dla API
  uv run python scripts/make_pco_bundle.py --seed case-42 --echo-url

Domyślny katalog wyjściowy:
  PROOF_BUNDLE_DIR lub ./data/public_pco (utworzy się automatycznie)

Zawartość bundla (minimal):
  {
    "rid": <64-hex>,
    "smt2_hash": <sha256 hexdigest pliku/tekstu SMT2>,
    "lfsc": "(proof ...)" lub zawartość pliku LFSC,
    "merkle_proof": { "root": <leaf>, "leaf": <leaf>, "path": [] },
    "signature": "0"*64 (lub podany)
  }

Weryfikacja:
  curl http://127.0.0.1:8000/pco/public/<rid>
"""

# --- blok --- Importy ----------------------------------------------------------
from __future__ import annotations

import argparse
import json
import os
from hashlib import sha256
from pathlib import Path
from typing import Final

# --- blok --- Stałe ------------------------------------------------------------

DEFAULT_DIR: Final[str] = os.getenv("PROOF_BUNDLE_DIR", "./data/public_pco")


# --- blok --- Hash utils -------------------------------------------------------
def _hx_bytes(data: bytes) -> str:
    """sha256 -> hex."""
    return sha256(data).hexdigest()


def _hx_text(text: str) -> str:
    """sha256(utf-8) -> hex."""
    return _hx_bytes(text.encode("utf-8"))


def _is_hex64(s: str) -> bool:
    """Czy ciąg jest 64-znakowym heksadecymalnym ID (lowercase)."""
    return len(s) == 64 and all(c in "0123456789abcdef" for c in s)


def _compute_leaf(rid_hex: str, smt2_hex: str) -> str:
    """
    Minimalny wariant z testów E2E:
    leaf = sha256( (rid_hex + smt2_hex).encode("utf-8") )
    """
    return _hx_text(rid_hex + smt2_hex)


# --- blok --- Bundle build -----------------------------------------------------
def _read_or_default(
    path: str | None,
    default_text: str,
    *,
    allow_missing: bool = False,
    inline_text: str | None = None,
) -> tuple[str, str]:
    """
    Zwraca (content_text, sha256_hex).
    Priorytet: inline_text > plik(path) > default_text (gdy allow_missing lub brak path).
    """
    if inline_text is not None:
        return inline_text, _hx_text(inline_text)

    if path:
        p = Path(path)
        try:
            text = p.read_text(encoding="utf-8")
            return text, _hx_text(text)
        except FileNotFoundError:
            if allow_missing:
                text = default_text
                return text, _hx_text(text)
            raise

    text = default_text
    return text, _hx_text(text)


def make_bundle(
    outdir: Path,
    rid: str,
    lfsc_path: str | None,
    smt2_path: str | None,
    signature: str | None,
    *,
    allow_missing: bool = False,
    lfsc_text: str | None = None,
    smt2_text: str | None = None,
) -> Path:
    outdir.mkdir(parents=True, exist_ok=True)

    # LFSC & SMT2 (treść + hash SMT2)
    lfsc_text_val, _ = _read_or_default(
        lfsc_path,
        "(proof ...)",
        allow_missing=allow_missing,
        inline_text=lfsc_text,
    )
    _, smt2_hex = _read_or_default(
        smt2_path,
        "(set-logic QF_UF) ...",
        allow_missing=allow_missing,
        inline_text=smt2_text,
    )

    # Merkle minimal: root == leaf, brak ścieżki
    leaf = _compute_leaf(rid, smt2_hex)
    merkle_proof = {"root": leaf, "leaf": leaf, "path": []}

    # Podpis: >= 40 znaków (domyślnie 64 zera)
    sig = signature if (signature and len(signature) >= 40) else ("0" * 64)

    bundle = {
        "rid": rid,
        "smt2_hash": smt2_hex,
        "lfsc": lfsc_text_val,
        "merkle_proof": merkle_proof,
        "signature": sig,
    }

    out_path = outdir / f"{rid}.json"
    out_path.write_text(
        json.dumps(bundle, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )
    return out_path


# --- blok --- CLI --------------------------------------------------------------


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        prog="make_pco_bundle",
        description="Create a minimal public PCO bundle JSON (0 PII).",
    )
    rid_grp = p.add_mutually_exclusive_group(required=False)
    rid_grp.add_argument("--rid", help="RID (64-hex). If not provided, use --seed")
    rid_grp.add_argument("--seed", help="Seed string -> RID = sha256(seed)")

    p.add_argument("--lfsc", dest="lfsc_path", help="Path to LFSC file", default=None)
    p.add_argument("--smt2", dest="smt2_path", help="Path to SMT2 file", default=None)
    p.add_argument("--signature", help="Detached signature (>= 40 chars)", default=None)
    p.add_argument("--outdir", help="Output dir (defaults to PROOF_BUNDLE_DIR)", default=DEFAULT_DIR)
    p.add_argument("--echo-url", action="store_true", help="Print GET URL for API")
    p.add_argument("--allow-missing", action="store_true", help="Use default text if --lfsc/--smt2 files are missing")
    p.add_argument("--lfsc-text", dest="lfsc_text", default=None, help="Inline LFSC text (overrides --lfsc file)")
    p.add_argument("--smt2-text", dest="smt2_text", default=None, help="Inline SMT2 text (overrides --smt2 file)")
    return p.parse_args()


def main() -> int:
    args = _parse_args()

    # RID resolve
    if args.rid:
        rid = args.rid.strip().lower()
        if not _is_hex64(rid):
            print("[RID] must be 64-hex (lowercase).", flush=True)
            return 2
    else:
        seed = args.seed or "rid-demo"
        rid = _hx_text(seed)

    outdir = Path(args.outdir)
    out_path = make_bundle(
        outdir,
        rid,
        args.lfsc_path,
        args.smt2_path,
        args.signature,
        allow_missing=args.allow_missing,
        lfsc_text=args.lfsc_text,
        smt2_text=args.smt2_text,
    )

    print(f"[OK] bundle written: {out_path}")
    if args.echo_url:
        print(f"[GET] http://127.0.0.1:8000/pco/public/{rid}")
    return 0


# --- blok --- Entrypoint -------------------------------------------------------

if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: scripts/pem_to_b64url.py =====
```text
# +======================================================================+
# |                               CERTEUS                                |
# +======================================================================+
# | FILE / PLIK: scripts/pem_to_b64url.py                                |
# | ROLA / ROLE:                                                          |
# |  PL: Konwersja PEM Ed25519 (public) -> Base64URL (pole 'x' do JWKS).  |
# |  EN: Convert Ed25519 public PEM -> Base64URL (JWKS 'x' value).        |
# +======================================================================+
# Opis:
# - Wczytuje klucz publiczny Ed25519 w PEM i wypisuje Base64URL bez '='.
# Użycie:
#   uv run python scripts/pem_to_b64url.py --in ed25519-public.pem
# Wymagania:
#   - Python 3.11+, cryptography
# ----Bloki----- IMPORTY
from __future__ import annotations

import argparse
import base64
from pathlib import Path

from cryptography.hazmat.primitives import serialization


# ----Bloki----- MAIN
def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--in", dest="inp", required=True)
    args = ap.parse_args()

    raw = Path(args.inp).read_bytes()
    pk = serialization.load_pem_public_key(raw)
    # Ed25519 only:
    b = pk.public_bytes(
        encoding=serialization.Encoding.Raw,
        format=serialization.PublicFormat.Raw,
    )
    print(base64.urlsafe_b64encode(b).rstrip(b"=").decode())


if __name__ == "__main__":
    main()

```


===== FILE: scripts/sign_bundle.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE: scripts/sign_bundle.py                                        |
# | ROLE: Sign public PCO bundle (Ed25519 detached signature)           |
# +=====================================================================+
# ----Bloki----- IMPORTY
from __future__ import annotations

import base64
import hashlib
import json
import os
from argparse import ArgumentParser
from pathlib import Path
from typing import Any, cast

from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey


# ----Bloki----- FUNKCJE
def sha256_hex_utf8(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def compute_bundle_hash_hex(pub: dict[str, Any]) -> str:
    payload = {"smt2_hash": pub["smt2_hash"], "lfsc_sha256": sha256_hex_utf8(pub["lfsc"])}
    if pub.get("drat") is not None:
        payload["drat_sha256"] = sha256_hex_utf8(pub["drat"])
    blob = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
    return hashlib.sha256(blob).hexdigest()


def canonical_digest_hex(pub: dict[str, Any], merkle_root_hex: str) -> str:
    parts = [
        hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest(),
        pub["smt2_hash"].lower(),
        sha256_hex_utf8(pub["lfsc"]),
    ]
    if pub.get("drat") is not None:
        parts.append(sha256_hex_utf8(pub["drat"]))
    parts.append(merkle_root_hex.lower())
    msg = b"".join(bytes.fromhex(x) for x in parts)
    return hashlib.sha256(msg).hexdigest()


# ----Bloki----- MAIN
def main() -> None:
    ap = ArgumentParser(description="Sign public PCO bundle (Ed25519 detached signature).")
    ap.add_argument("--rid", required=True)
    ap.add_argument("--bundle-dir", default=os.getenv("PROOF_BUNDLE_DIR", "./data/public_pco"))
    ap.add_argument("--key", default="ed25519-private.pem", help="PEM private key (PKCS8, no password)")
    args = ap.parse_args()

    bundle_path = Path(args.bundle_dir) / f"{args.rid}.json"
    # Czytaj 'utf-8-sig' (leczy BOM); zapisz bez BOM
    pub = json.loads(bundle_path.read_text(encoding="utf-8-sig"))

    # Merkle (MVP): path=[] → root = leaf
    bundle_hash_hex = compute_bundle_hash_hex(pub)
    rid_hash_hex = hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest()
    leaf_hex = hashlib.sha256(bytes.fromhex(rid_hash_hex) + bytes.fromhex(bundle_hash_hex)).hexdigest()
    merkle_root_hex = leaf_hex

    digest_hex = canonical_digest_hex(pub, merkle_root_hex)

    sk_any = serialization.load_pem_private_key(Path(args.key).read_bytes(), password=None)
    if not isinstance(sk_any, Ed25519PrivateKey):
        raise TypeError("Loaded key is not Ed25519 (expected Ed25519PrivateKey).")
    sk = cast(Ed25519PrivateKey, sk_any)

    sig = sk.sign(bytes.fromhex(digest_hex))
    pub["signature"] = base64.urlsafe_b64encode(sig).rstrip(b"=").decode()

    bundle_path.write_text(json.dumps(pub, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"OK: signed {bundle_path}")


if __name__ == "__main__":
    main()

```


===== FILE: scripts/smoke286.ps1 =====
```text
# +-------------------------------------------------------------+
# |                 CERTEUS - smoke286 helper                   |
# +-------------------------------------------------------------+
# | FILE: scripts/smoke286.ps1                                  |
# | ROLE: One-liner to run build->smoke->tests                  |
# +-------------------------------------------------------------+

param()

uv run python scripts/lexlog_eval_smoke_fallback.py
uv run pytest -q tests/services/test_lexlog_parser.py

```


===== FILE: scripts/smoke_ingest.ps1 =====
```text
# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |                 Smoke test for /v1/ingest                   |
# +-------------------------------------------------------------+

Set-StrictMode -Version Latest
$ErrorActionPreference = "Stop"

# Files live only in working dir; we do not git-add them.
Set-Content -NoNewline -Encoding ASCII .\sample.txt 'hello CERTEUS'
Set-Content -NoNewline -Encoding ASCII .\sample.pdf '%PDF-1.4 dummy'

Write-Host "[INFO] Health"
curl.exe -sS http://127.0.0.1:8000/health

Write-Host "[INFO] Ingest TXT"
curl.exe -sS -i --form "file=@sample.txt;type=text/plain" http://127.0.0.1:8000/v1/ingest | Select-String -Pattern "HTTP/1.1 200"

Write-Host "[INFO] Ingest PDF"
curl.exe -sS -i --form "file=@sample.pdf;type=application/pdf" http://127.0.0.1:8000/v1/ingest | Select-String -Pattern "HTTP/1.1 200"

Write-Host "[INFO] Done"

```


===== FILE: scripts/smoke_ingest.sh =====
```text
# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |                 Smoke test for /v1/ingest                   |
# +-------------------------------------------------------------+

set -euo pipefail

printf '%s' 'hello CERTEUS' > sample.txt
printf '%s' '%PDF-1.4 dummy' > sample.pdf

echo "[INFO] Health"
curl -sS http://127.0.0.1:8000/health

echo "[INFO] Ingest TXT"
curl -sS -i -F "file=@sample.txt;type=text/plain" http://127.0.0.1:8000/v1/ingest | grep "HTTP/1.1 200"

echo "[INFO] Ingest PDF"
curl -sS -i -F "file=@sample.pdf;type=application/pdf" http://127.0.0.1:8000/v1/ingest | grep "HTTP/1.1 200"

echo "[INFO] Done"

```


===== FILE: scripts/temporal_fortress.py =====
```text
# +=====================================================================+
# |                          CERTEUS — HEART                            |
# +=====================================================================+
# | FILE: scripts/temporal_fortress.py                                  |
# | ROLE:                                                               |
# |  PL: Re-proof po TTL (TTDE) per domena.                             |
# |  EN: TTDE re-proof per domain TTL.                                  |
# +=====================================================================+

"""PL: Harmonogram odświeżania dowodów (prawo=365, med=90, fin=7).
EN: Re-proof scheduler (law=365, med=90, fin=7).
"""

from __future__ import annotations

from datetime import datetime, timedelta, timezone

TTL_DAYS: dict[str, int] = {"prawo": 365, "med": 90, "fin": 7}


def due(domain: str, last_proof_at: datetime) -> bool:
    """PL: Czy wygasło TTL? EN: TTL expired?"""
    days = TTL_DAYS.get(domain, 365)
    return datetime.now(timezone.utc) - last_proof_at > timedelta(days=days)


def run() -> None:
    """PL: Znajdź kapsuły do re-proof i zleć zadania. EN: Enqueue due re-proofs."""
    # Integracja z magazynem kapsuł (do uzupełnienia zgodnie z repo).
    # Keeping architecture unchanged.
    return

```


===== FILE: scripts/validate_policy_pack.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: scripts/validate_policy_pack.py                     |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: Validate PCO Policy Pack (YAML) vs JSON Schema;                |
# |      enforce invariants (no-PII, required fields, endpoint pattern, |
# |      drat_required) and provide CLI/report.                         |
# |  PL: Walidacja Policy Pack (YAML) względem Schema; egzekwowanie     |
# |      inwariantów (brak PII, wymagane pola, wzorzec endpointu,       |
# |      drat_required) + CLI/raport.                                   |
# +=====================================================================+
from __future__ import annotations

# stdlib
import argparse
import json
import os
import re
import sys
from pathlib import Path
from typing import Any

# third-party
import yaml  # type: ignore
from jsonschema import Draft7Validator, Draft201909Validator, Draft202012Validator

# ----Bloki----- STAŁE
DEFAULT_SCHEMA = Path("policies/pco/policy_pack.schema.v0.1.json")
DEFAULT_PACK = Path("policies/pco/policy_pack.v0.1.yaml")
ENV_SCHEMA = "PCO_POLICY_PACK_SCHEMA"
ENV_PACK = "PCO_POLICY_PACK_PATH"

# Denylista PII (klucze)
PII_FIELD_NAMES: set[str] = {
    "name",
    "first_name",
    "last_name",
    "pesel",
    "email",
    "phone",
    "address",
    "dob",
    "ssn",
    "patient_id",
    "person_id",
    "user_id",
}

# Dozwolone klucze w publicznym payload
ALLOWED_PUBLIC_FIELDS: set[str] = {
    "rid",
    "smt2_hash",
    "lfsc",
    "drat",
    "merkle_proof",
    "signature",
}

# Minimalny zestaw wymaganych pól
REQUIRED_PUBLIC_FIELDS: set[str] = {
    "rid",
    "smt2_hash",
    "lfsc",
    "merkle_proof",
    "signature",
}

ENDPOINT_PATTERN = re.compile(r"^/pco/public/\{case_id\}$")


# ----Bloki----- I/O
def _read_json(path: Path) -> dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def _read_yaml(path: Path) -> dict[str, Any]:
    data = yaml.safe_load(path.read_text(encoding="utf-8"))
    if not isinstance(data, dict):
        raise TypeError("YAML must decode to an object (mapping)")
    return data


# ----Bloki----- Dobór walidatora
def _pick_validator(schema: dict[str, Any]):
    ident = str(schema.get("$schema", "")).lower()
    if "draft-07" in ident:
        return Draft7Validator
    if "2019-09" in ident:
        return Draft201909Validator
    return Draft202012Validator


# ----Bloki----- Inwarianty
def _ensure_no_pii(fields: list[str], ctx: str, messages: list[dict[str, Any]]) -> None:
    lowered = {f.replace("?", "").lower() for f in fields}
    forbidden = sorted(lowered.intersection(PII_FIELD_NAMES))
    if forbidden:
        messages.append(
            {
                "level": "error",
                "code": "PII_FORBIDDEN",
                "where": ctx,
                "detail": f"PII fields not allowed: {', '.join(forbidden)}",
            }
        )


def _check_required_fields(fields: list[str], ctx: str, messages: list[dict[str, Any]]) -> None:
    s = set(x.replace("?", "") for x in fields)
    missing = sorted(REQUIRED_PUBLIC_FIELDS - s)
    if missing:
        messages.append(
            {
                "level": "error",
                "code": "REQUIRED_MISSING",
                "where": ctx,
                "detail": f"Missing required fields: {', '.join(missing)}",
            }
        )


def _check_unknown_fields(fields: list[str], ctx: str, messages: list[dict[str, Any]]) -> None:
    s = set(x.replace("?", "") for x in fields)
    unknown = sorted(s - ALLOWED_PUBLIC_FIELDS)
    if unknown:
        messages.append(
            {
                "level": "warning",
                "code": "UNKNOWN_FIELD",
                "where": ctx,
                "detail": f"Unknown fields present: {', '.join(unknown)}",
            }
        )


def _check_endpoint_pattern(endpoint: str, ctx: str, messages: list[dict[str, Any]]) -> None:
    if not ENDPOINT_PATTERN.fullmatch(endpoint):
        messages.append(
            {
                "level": "error",
                "code": "ENDPOINT_PATTERN",
                "where": ctx,
                "detail": f"Endpoint must match '^/pco/public/{{case_id}}$', got: {endpoint}",
            }
        )


def run_invariants(pack: dict[str, Any]) -> list[dict[str, Any]]:
    msgs: list[dict[str, Any]] = []

    use_cases = pack.get("use_cases", {})
    if not isinstance(use_cases, dict):
        msgs.append({"level": "error", "code": "USE_CASES_TYPE", "where": "use_cases", "detail": "must be object"})
        return msgs

    for uc_name, uc in use_cases.items():
        if not isinstance(uc, dict):
            msgs.append(
                {"level": "error", "code": "UC_TYPE", "where": f"use_cases.{uc_name}", "detail": "must be object"}
            )
            continue

        publish = uc.get("publish", {})
        if not isinstance(publish, dict):
            msgs.append(
                {
                    "level": "error",
                    "code": "PUBLISH_TYPE",
                    "where": f"use_cases.{uc_name}.publish",
                    "detail": "must be object",
                }
            )
            continue

        endpoint = str(publish.get("endpoint", ""))
        _check_endpoint_pattern(endpoint, f"use_cases.{uc_name}.publish.endpoint", msgs)

        fields = publish.get("fields", [])
        if not isinstance(fields, list) or not all(isinstance(x, str) for x in fields):
            msgs.append(
                {
                    "level": "error",
                    "code": "FIELDS_TYPE",
                    "where": f"use_cases.{uc_name}.publish.fields",
                    "detail": "must be array of strings",
                }
            )
            continue

        _ensure_no_pii(fields, f"use_cases.{uc_name}.publish.fields", msgs)
        _check_required_fields(fields, f"use_cases.{uc_name}.publish.fields", msgs)
        _check_unknown_fields(fields, f"use_cases.{uc_name}.publish.fields", msgs)

        # NEW: drat_required => wymagamy 'drat' w polach publikacji
        drat_required = bool(uc.get("drat_required", False))
        if drat_required and "drat" not in {f.replace("?", "") for f in fields}:
            msgs.append(
                {
                    "level": "error",
                    "code": "DRAT_REQUIRED",
                    "where": f"use_cases.{uc_name}.publish.fields",
                    "detail": "drat_required=true but 'drat' field not present",
                }
            )

    return msgs


# ----Bloki----- CLI
def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        prog="validate_policy_pack",
        description="Validate PCO Policy Pack (schema + invariants).",
    )
    p.add_argument("--schema", type=Path, default=Path(os.getenv(ENV_SCHEMA) or DEFAULT_SCHEMA))
    p.add_argument("--pack", type=Path, default=Path(os.getenv(ENV_PACK) or DEFAULT_PACK))
    p.add_argument("--format", choices=["text", "json"], default="text")
    p.add_argument("--strict", action="store_true")
    p.add_argument("--list-use-cases", action="store_true")
    return p.parse_args()


def _emit_text(schema_errors: list[str], messages: list[dict[str, Any]], use_cases: list[str]) -> None:
    if use_cases:
        print("[use_cases] " + ", ".join(use_cases))
    for e in schema_errors:
        print(f"[SCHEMA] {e}", file=sys.stderr)
    for m in messages:
        lvl = str(m.get("level", "info")).upper()
        code = str(m.get("code", "MSG"))
        where = str(m.get("where", "-"))
        detail = str(m.get("detail", ""))
        print(f"[{lvl}] {code} @ {where}: {detail}")


def _emit_json(schema_errors: list[str], messages: list[dict[str, Any]], use_cases: list[str]) -> None:
    out = {"use_cases": use_cases, "schema_errors": schema_errors, "messages": messages}
    print(json.dumps(out, ensure_ascii=False, indent=2))


# ----Bloki----- MAIN
def main() -> int:
    args = _parse_args()
    try:
        schema = _read_json(args.schema)
        pack = _read_yaml(args.pack)
    except Exception:
        return 4

    Validator = _pick_validator(schema)
    schema_errs = [
        f"{'/'.join(map(str, e.path))}: {e.message}"
        for e in sorted(Validator(schema).iter_errors(pack), key=lambda e: e.path)
    ]  # type: ignore[arg-type]

    messages = run_invariants(pack)

    uc_names: list[str] = []
    if args.list_use_cases and isinstance(pack.get("use_cases"), dict):
        uc_names = list(pack["use_cases"].keys())  # type: ignore[index]

    if args.format == "json":
        _emit_json(schema_errs, messages, uc_names)
    else:
        _emit_text(schema_errs, messages, uc_names)

    has_schema_errors = bool(schema_errs)
    has_errors = has_schema_errors or any(m.get("level") == "error" for m in messages)
    has_warnings = any(m.get("level") == "warning" for m in messages)

    if has_errors:
        return 2
    if has_warnings and args.strict:
        return 2
    if has_warnings:
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: scripts/validate_schemas.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/validate_schemas.py             |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                    CERTEUS - Schema Validator               |
# +-------------------------------------------------------------+
# | PLIK / FILE: scripts/validate_schemas.py                    |
# | ROLA / ROLE: Waliduje składnię i jakość wszystkich          |
# |             schematów JSON w projekcie.                     |
# +-------------------------------------------------------------+
# | STYLE: PL-first, headers & notes dual-language (PL/EN).     |
# +-------------------------------------------------------------+

"""
PL: Ten moduł zapewnia, że wszystkie kontrakty danych (schematy) są
    zgodne ze standardem JSON Schema Draft 7 oraz spełniają minimalne
    wymogi jakości (title/description, spójność required/properties).

EN: Ensures all data contracts (schemas) comply with JSON Schema Draft 7
    and pass minimal quality gates (title/description presence,
    required/properties consistency).
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

from jsonschema import Draft7Validator  # Only what we truly use

SCHEMA_DIR = Path("schemas")


def _check_quality(name: str, schema: dict[str, Any]) -> list[str]:
    """
    PL: Dodatkowe bramki jakości dla schematów.
    EN: Additional quality gates for schemas.
    """
    errs: list[str] = []

    # Wymagamy tytułu i opisu
    if not schema.get("title"):
        errs.append("Missing 'title'")
    if not schema.get("description"):
        errs.append("Missing 'description'")

    # Spójność required/properties na poziomie root
    props = schema.get("properties", {})
    required = schema.get("required", [])
    for key in required:
        if key not in props:
            errs.append(f"'required' contains '{key}' not present in 'properties'")

    # Restrykcyjność: preferujemy additionalProperties:false na root
    if schema.get("additionalProperties", True) is not False:
        errs.append("Root 'additionalProperties' should be false for stricter contracts")

    return errs


def main() -> None:
    """
    PL: Główna funkcja — weryfikuje syntaksę schematów i bramki jakości.
    EN: Main routine — verifies schema syntax and quality gates.
    """
    print(f"🔎 Validating schemas in: {SCHEMA_DIR.absolute()}")
    has_errors = False

    schema_files = sorted(SCHEMA_DIR.glob("*.json"))
    if not schema_files:
        print("⚠️ No schemas found to validate.")
        raise SystemExit(0)

    for schema_path in schema_files:
        print(f"--- Checking: {schema_path.name} ---")
        try:
            schema_instance = json.loads(schema_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError as e:
            print(f"❌ ERROR: Invalid JSON in {schema_path.name}: {e}")
            has_errors = True
            continue

        # Walidacja składniowa samego schematu
        try:
            Draft7Validator.check_schema(schema_instance)
            print("✅ Schema syntax is valid.")
        except Exception as e:  # noqa: BLE001
            print(f"❌ ERROR: Schema is invalid in {schema_path.name}: {e}")
            has_errors = True
            continue

        # Bramka jakości
        q_errs = _check_quality(schema_path.name, schema_instance)
        if q_errs:
            has_errors = True
            for q in q_errs:
                print(f"❌ QUALITY: {q}")
        else:
            print("✨ Quality gate passed.")

    if has_errors:
        print("\n💥 Validation failed for one or more schemas.")
        raise SystemExit(1)

    print("\n🎉 All schemas are syntactically valid and passed quality checks!")


if __name__ == "__main__":
    main()

```


===== FILE: scripts/verify_bundle.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE: scripts/verify_bundle.py                                      |
# | ROLE: Verify public PCO bundle (Merkle + canonical digest + Ed25519)|
# +=====================================================================+
# Opis:
# - Weryfikuje publiczny bundle PCO (0 PII): struktura, Merkle, podpis.
# - Obsługa Merkle path: MVP [] oraz pełna ścieżka [{sibling, dir:'L'|'R'}].
# - Kanoniczny digest: sha256( rid_hash || smt2_hash || lfsc_sha256 || [drat_sha256] || merkle_root ).
# - Podpis: Ed25519 (Base64URL, bez "="); klucz publiczny z --pub-b64url lub ENV ED25519_PUBKEY_B64URL.
# - Zwraca kod wyjścia: 0 OK, 2 błąd weryfikacji/IO.
#
# Użycie (CLI):
#   python scripts/verify_bundle.py --rid demo-001 \
#       --bundle-dir ./data/public_pco \
#       --pub-b64url $env:ED25519_PUBKEY_B64URL
#
# Wymagania:
#   - Python 3.11+, pakiet 'cryptography'
#   - JSON bez BOM (lub czytany 'utf-8-sig')
#
# Konwencje:
#   - future → stdlib (import, from) → third-party (import, from)
#   - typy: dict[str, Any], list[str] itd. (PEP 585)
#   - linia: max 120
# ----Bloki----- IMPORTY
from __future__ import annotations

import base64
import hashlib
import json
import os
import sys
from argparse import ArgumentParser
from collections.abc import Iterable
from pathlib import Path
from typing import Any, TypedDict, cast

from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PublicKey


# ----Bloki----- TYPY
class MerkleStep(TypedDict):
    sibling: str  # hex
    dir: str  # 'L' | 'R'


# ----Bloki----- POMOCNICZE
def sha256_hex_utf8(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def is_hex64(x: str) -> bool:
    return isinstance(x, str) and len(x) == 64 and all(c in "0123456789abcdef" for c in x.lower())


def compute_bundle_hash_hex(pub: dict[str, Any]) -> str:
    payload = {"smt2_hash": pub["smt2_hash"], "lfsc_sha256": sha256_hex_utf8(pub["lfsc"])}
    if pub.get("drat") is not None:
        payload["drat_sha256"] = sha256_hex_utf8(pub["drat"])
    blob = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
    return hashlib.sha256(blob).hexdigest()


def merkle_root_from_path(leaf_hex: str, path: Iterable[MerkleStep]) -> str:
    cur = bytes.fromhex(leaf_hex)
    for step in path:
        sib = bytes.fromhex(step["sibling"])
        if step["dir"] == "L":
            cur = hashlib.sha256(sib + cur).digest()
        elif step["dir"] == "R":
            cur = hashlib.sha256(cur + sib).digest()
        else:
            raise ValueError(f"Invalid dir: {step['dir']!r}")
    return cur.hex()


def canonical_digest_hex(pub: dict[str, Any], merkle_root_hex: str) -> str:
    parts = [
        hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest(),
        pub["smt2_hash"].lower(),
        sha256_hex_utf8(pub["lfsc"]),
    ]
    if pub.get("drat") is not None:
        parts.append(sha256_hex_utf8(pub["drat"]))
    parts.append(merkle_root_hex.lower())
    msg = b"".join(bytes.fromhex(x) for x in parts)
    return hashlib.sha256(msg).hexdigest()


def ed25519_from_b64url(x_b64u: str) -> Ed25519PublicKey:
    pad = "=" * (-len(x_b64u) % 4)
    raw = base64.urlsafe_b64decode(x_b64u + pad)
    return Ed25519PublicKey.from_public_bytes(raw)


# ----Bloki----- MAIN
def main() -> int:
    ap = ArgumentParser(description="Verify public PCO bundle (Merkle + Ed25519 signature).")
    ap.add_argument("--rid", required=True)
    ap.add_argument("--bundle-dir", default=os.getenv("PROOF_BUNDLE_DIR", "./data/public_pco"))
    ap.add_argument("--pub-b64url", default=os.getenv("ED25519_PUBKEY_B64URL"))
    args = ap.parse_args()

    if not args.pub_b64url:
        print("ERR: missing public key (use --pub-b64url or ED25519_PUBKEY_B64URL)", file=sys.stderr)
        return 2

    bundle_path = Path(args.bundle_dir) / f"{args.rid}.json"
    try:
        pub = json.loads(bundle_path.read_text(encoding="utf-8-sig"))
    except Exception as e:  # noqa: BLE001
        print(f"ERR: cannot read bundle: {e}", file=sys.stderr)
        return 2

    # Sanity checks
    if not is_hex64(pub.get("smt2_hash", "")):
        print("ERR: smt2_hash must be 64 hex chars", file=sys.stderr)
        return 2

    # 1) leaf := sha256( rid_hash || bundle_hash )
    rid_hash_hex = hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest()
    bundle_hash_hex = compute_bundle_hash_hex(pub)
    leaf_hex = hashlib.sha256(bytes.fromhex(rid_hash_hex) + bytes.fromhex(bundle_hash_hex)).hexdigest()

    # 2) merkle_root (MVP: path może być [] lub L/R ścieżka)
    path = pub.get("merkle_proof") or []
    merkle_root_hex = merkle_root_from_path(leaf_hex, cast(Iterable[MerkleStep], path))

    # 3) canonical digest
    digest_hex = canonical_digest_hex(pub, merkle_root_hex)

    # 4) verify signature
    try:
        pk = ed25519_from_b64url(cast(str, args.pub_b64url))
        sig_b64u = pub.get("signature", "")
        pad = "=" * (-len(sig_b64u) % 4)
        sig = base64.urlsafe_b64decode(sig_b64u + pad)
        pk.verify(sig, bytes.fromhex(digest_hex))
    except Exception as e:  # noqa: BLE001
        print(f"ERR: signature invalid: {e}", file=sys.stderr)
        return 2

    print(f"OK: {bundle_path} verified")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: security/proofs.md =====
```text
External Proof Checking (DRAT/LFSC) – zasady publikacji.

```


===== FILE: security/rbac_policies.md =====
```text
Role: Architect, Proof‑Engineer, LKEW, Ops.

```


===== FILE: services/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/__init__.py                    |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
PL: Pakiet inicjalizacyjny modułu.
EN: Package initializer.
"""

```


===== FILE: services/api_gateway/app_e2e.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/app_e2e.py         |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Moduł systemu CERTEUS.
EN: CERTEUS system module.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

from fastapi import FastAPI
from pydantic import BaseModel, Field

from kernel.truth_engine import DualCoreVerifier
from services.exporter_service.exporter import ExporterService

__all__ = ["app"]

# ──────────────────────────────────────────────────────────────────────
# App & services
# ──────────────────────────────────────────────────────────────────────

app = FastAPI(title="CERTEUS E2E", version="1.1.0")

# Provide explicit constructor args (fixes: missing template_dir/output_dir)
_exporter = ExporterService(template_dir="templates", output_dir="exports/e2e")
_verifier = DualCoreVerifier()

# ──────────────────────────────────────────────────────────────────────
# Schemas
# ──────────────────────────────────────────────────────────────────────


class SimpleFact(BaseModel):
    """Minimalny model wejściowy do E2E solve."""

    case_id: str = Field(..., description="Case identifier")
    smt2: str = Field(..., description="SMT-LIB2 formula")
    export: bool = Field(False, description="Export report file after solve")
    force_mismatch: bool = Field(False, description="Flip Core-2 to trigger mismatch protocol (testing)")


class SolveResponse(BaseModel):
    status: str
    time_ms: float | None = None
    model: dict[str, Any] | None = None
    error: str | None = None
    report_path: str | None = None
    version: str | None = None


# ──────────────────────────────────────────────────────────────────────
# Routes
# ──────────────────────────────────────────────────────────────────────


@app.get("/health")
def health() -> dict[str, Any]:
    return {"status": "ok", "services": ["verifier", "exporter"]}


@app.post("/e2e/solve", response_model=SolveResponse)
def e2e_solve(payload: SimpleFact) -> SolveResponse:
    # 1) verify with DualCore
    result = _verifier.verify(
        payload.smt2,
        lang="smt2",
        case_id=payload.case_id,
        force_mismatch=payload.force_mismatch,
    )
    # 2) optional export
    report_path: str | None = None
    if payload.export:
        out_path = _exporter.export_report(payload.case_id, result)
        report_path = str(Path(out_path))

    return SolveResponse(
        status=str(result.get("status", "unknown")),
        time_ms=result.get("time_ms"),
        model=result.get("model"),
        error=result.get("error"),
        report_path=report_path,
        version=result.get("version"),
    )

```


===== FILE: services/api_gateway/main.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/api_gateway/main.py                        |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: API Gateway bootstrap. Mounts static, registers routers,       |
# |      enables DEV CORS, exposes health and root redirect.            |
# |  PL: Bootstrap bramy API. Montuje statyczne zasoby, rejestruje      |
# |      routery, włącza CORS (DEV), wystawia health i redirect root.   |
# +=====================================================================+

"""
PL: Główna aplikacja FastAPI dla CERTEUS: statyki, routery, CORS (DEV), health.
EN: Main FastAPI app for CERTEUS: statics, routers, CORS (DEV), health.
"""

# --- blok --- Importy ----------------------------------------------------------
from __future__ import annotations

# stdlib
from contextlib import asynccontextmanager
from pathlib import Path

# third-party
from fastapi import FastAPI, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse
from fastapi.staticfiles import StaticFiles

# local (rozbite na pojedyncze linie — łatwiej sortować i Ruff nie marudzi)
from services.api_gateway.routers import (
    export,
    ledger,
    mismatch,
    pco_public,
    preview,
    system,  # /v1/ingest, /v1/analyze, /v1/sipp
    verify,
)
from services.api_gateway.routers.well_known_jwks import router as jwks_router
from services.ingest_service.adapters.contracts import Blob
from services.ingest_service.adapters.registry import get_llm, get_preview

# --- blok --- Ścieżki i katalogi ----------------------------------------------

ROOT = Path(__file__).resolve().parents[2]
STATIC_DIR = ROOT / "static"
STATIC_PREVIEWS = STATIC_DIR / "previews"
CLIENTS_WEB = ROOT / "clients" / "web"  # expects /app/proof_visualizer/index.html

STATIC_PREVIEWS.mkdir(parents=True, exist_ok=True)
CLIENTS_WEB.mkdir(parents=True, exist_ok=True)

APP_TITLE = "CERTEUS API Gateway"
APP_VERSION = "1.1.5"

# --- blok --- Lifespan (inicjalizacja adapterów) -------------------------------


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    PL: Leniwe utworzenie singletonów adapterów przy starcie.
    EN: Lazily create adapter singletons on startup.
    """
    _ = (get_preview(), get_llm())
    yield


# --- blok --- Aplikacja i middleware -------------------------------------------

app = FastAPI(
    title=APP_TITLE,
    version=APP_VERSION,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan,
)


# statyki
app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")
app.mount("/app", StaticFiles(directory=str(CLIENTS_WEB)), name="app")

# CORS (DEV) – szeroko, bez credentials
DEV_ORIGINS: list[str] = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=DEV_ORIGINS,
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- blok --- Rejestr routerów -------------------------------------------------

app.include_router(system.router)
app.include_router(preview.router)
app.include_router(pco_public.router)
app.include_router(export.router)
app.include_router(ledger.router)
app.include_router(mismatch.router)
app.include_router(verify.router)
app.include_router(jwks_router)

# --- blok --- Health i root redirect -------------------------------------------


@app.get("/health")
def health() -> dict[str, object]:
    """PL: Liveness; EN: Liveness."""
    return {"status": "ok", "version": APP_VERSION}


@app.get("/")
def root_redirect() -> RedirectResponse:
    """
    PL: W DEV kierujemy na UI wizualizatora.
    EN: In DEV, redirect to the proof visualizer UI.
    """
    return RedirectResponse(url="/app/proof_visualizer/index.html", status_code=307)


# --- blok --- Pomocnicze -------------------------------------------------------


def _make_blob(upload: UploadFile, data: bytes) -> Blob:
    """
    PL: Buduje Blob z UploadFile.
    EN: Build Blob from UploadFile.
    """
    return Blob(
        filename=upload.filename or "file",
        content_type=upload.content_type or "application/octet-stream",
        data=data,
    )

```


===== FILE: services/api_gateway/routers/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/__init__.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: services/api_gateway/routers/__init__.py              |
# | ROLE: Make 'routers' a package (no implicit re-exports).    |
# | PLIK: services/api_gateway/routers/__init__.py              |
# | ROLA: Czyni 'routers' pakietem (bez niejawnych re-eksportów).|
# +-------------------------------------------------------------+
"""
PL: Minimalne __init__, by uniknąć ostrzeżeń Pylance/Ruff o „unused import”.
EN: Minimal __init__ to avoid Pylance/Ruff 'unused import' warnings.
"""

# [UWAGA]
# Nie re-eksportujemy tutaj verify/system/export/ledger, żeby nie generować
# F401/unused-import. Moduły importujemy bezpośrednio w main.py.

```


===== FILE: services/api_gateway/routers/export.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/api_gateway/routers/export.py              |
# | DATE / DATA: 2025-08-17                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: /v1/export – validate payload and write a TXT report           |
# |      named `raport_{case_id}.txt`. Enrich response with provenance  |
# |      (sha256, timestamp_utc, artifacts).                            |
# |  PL: /v1/export – walidacja ładunku i zapis raportu TXT             |
# |      `raport_{case_id}.txt`. Odpowiedź wzbogacona o provenance      |
# |      (sha256, timestamp_utc, artifacts).                            |
# +=====================================================================+

"""
PL: Endpoint eksportu. Przyjmuje `case_id` i `analysis_result`, zapisuje raport
    tekstowy do katalogu `exports/`, po czym zwraca ścieżkę oraz provenance
    zawierające hash SHA-256 pliku raportu, znacznik czasu UTC i listę artefaktów.
EN: Export endpoint. Accepts `case_id` and `analysis_result`, writes a text
    report under `exports/`, then returns the path and provenance with the
    report file's SHA-256, UTC timestamp, and artifacts list.
"""

from __future__ import annotations

import hashlib
import json
from collections.abc import Mapping
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

router = APIRouter(prefix="", tags=["export"])


# === MODELS / MODELE ===
class ExportPayload(BaseModel):
    case_id: str = Field(..., description="Public case id, e.g. 'pl-286kk-0001'")
    analysis_result: Mapping[str, Any] = Field(default_factory=dict)
    fmt: str = Field("report", description="Output format (tests use 'report').")


class ExportResponse(BaseModel):
    path: str
    message: str
    provenance: dict[str, Any] | None = None  # optional to keep tests happy


# === HELPERS ===
def _hash_file_sha256(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def _now_iso_utc() -> str:
    return datetime.now(timezone.utc).isoformat()


def _write_report(case_id: str, analysis_result: Mapping[str, Any], out_dir: Path) -> Path:
    """
    PL: Zapis raportu jako .txt z podstawowym podsumowaniem i pretty JSON.
    EN: Write .txt report with a tiny header + pretty JSON of the analysis.
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    filename = f"raport_{case_id}.txt"
    path = out_dir / filename

    lines: list[str] = []
    lines.append("# CERTEUS – Raport analityczny / Analytical Report")
    lines.append(f"Case: {case_id}")
    try:
        pretty = json.dumps(analysis_result, ensure_ascii=False, indent=2, sort_keys=True)
    except Exception:
        pretty = str(analysis_result)
    lines.append("")
    lines.append("=== ANALIZA / ANALYSIS ===")
    lines.append(pretty)

    path.write_text("\n".join(lines), encoding="utf-8")
    return path


# === ENDPOINT ===
@router.post("/v1/export", response_model=ExportResponse)
def export_endpoint(payload: ExportPayload) -> ExportResponse:
    """
    PL: Generuje raport i zwraca ścieżkę + provenance (hash, timestamp, artifacts).
    EN: Generate report and return path + provenance (hash, timestamp, artifacts).
    """
    case_id = payload.case_id.strip()
    if not case_id:
        raise HTTPException(status_code=400, detail="case_id required")

    out_dir = Path("exports")
    path = _write_report(case_id, payload.analysis_result, out_dir)

    # Build provenance / Budowa provenance
    prov: dict[str, Any] = {
        "hash_sha256": _hash_file_sha256(path),
        "timestamp_utc": _now_iso_utc(),
        "artifacts": {
            "report": str(path),
        },
    }

    return ExportResponse(
        path=str(path),
        message=f"Report generated at {path}",
        provenance=prov,
    )

```


===== FILE: services/api_gateway/routers/ledger.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/ledger.py  |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: services/api_gateway/routers/ledger.py                |
# | ROLE: Public API for the ledger (record/list/prove)         |
# | PLIK: services/api_gateway/routers/ledger.py                |
# | ROLA: Publiczne API dla księgi (record/list/prove)          |
# +-------------------------------------------------------------+

"""
PL: Router FastAPI dla księgi pochodzenia (ledger):
    - /record-input       : rejestruje dokument wejściowy,
    - /{case_id}/records  : pobiera wpisy dla sprawy,
    - /{case_id}/prove    : buduje i (opcjonalnie) waliduje paragon pochodzenia.
EN: FastAPI router for the provenance ledger:
    - /record-input       : record an input document,
    - /{case_id}/records  : list entries for a case,
    - /{case_id}/prove    : build and (optionally) validate a provenance receipt.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Protocol, cast

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

# Import the module, then obtain a working singleton/instance.
import services.ledger_service.ledger as ledger_mod  # noqa: F401


# --- Protocol to satisfy type checker (methods used by this router) ---
class LedgerLike(Protocol):
    def record_input(self, *, case_id: str, document_hash: str) -> dict[str, Any]: ...
    def get_records_for_case(self, *, case_id: str) -> list[dict[str, Any]]: ...
    def build_provenance_receipt(self, *, case_id: str) -> dict[str, Any]: ...


# Prefer existing singleton; else instantiate a fresh Ledger from the module.
ledger_service: LedgerLike = cast(
    LedgerLike,
    getattr(ledger_mod, "ledger_service", None) or ledger_mod.Ledger(),  # type: ignore[attr-defined, call-arg]
)

# Optional JSON Schema validation (soft dependency)
try:
    from jsonschema import Draft7Validator  # type: ignore
except Exception:  # pragma: no cover
    Draft7Validator = None  # type: ignore[assignment]

router = APIRouter()

# Repo paths
REPO_ROOT = Path(__file__).resolve().parents[3]
SCHEMAS_DIR = REPO_ROOT / "schemas"

# Lazy schema/validator (not hard constants)
_provenance_schema: dict[str, Any] | None = None
_provenance_validator: Any | None = None
if Draft7Validator is not None:
    schema_path = SCHEMAS_DIR / "provenance_receipt_v1.json"
    if schema_path.exists():
        try:
            _provenance_schema = json.loads(schema_path.read_text(encoding="utf-8"))
            _provenance_validator = Draft7Validator(_provenance_schema)  # type: ignore[call-arg]
        except Exception:
            _provenance_schema = None
            _provenance_validator = None


# === MODELS ===
class RecordInputRequest(BaseModel):
    """
    PL: Wejście do zarejestrowania dokumentu.
    EN: Input to record a document ingestion.
    """

    case_id: str = Field(..., min_length=1, description="PL: Id sprawy. / EN: Case identifier.")
    document_hash: str = Field(
        ...,
        min_length=7,
        description="PL: Np. 'sha256:<hex>'. / EN: e.g., 'sha256:<hex>'.",
    )


class RecordInputResponse(BaseModel):
    """
    PL: Odpowiedź na zarejestrowanie dokumentu.
    EN: Response for recorded document ingestion.
    """

    event_id: int
    type: str
    case_id: str
    document_hash: str | None
    timestamp: str
    chain_prev: str | None
    chain_self: str


# === ENDPOINTS ===
@router.post("/record-input", response_model=RecordInputResponse, tags=["Ledger"])
def record_input(payload: RecordInputRequest) -> RecordInputResponse:
    """
    PL: Rejestruje nowy dokument w księdze (INPUT_INGESTION).
    EN: Records a new document in the ledger (INPUT_INGESTION).
    """
    result = ledger_service.record_input(case_id=payload.case_id, document_hash=payload.document_hash)
    return RecordInputResponse(**result)


@router.get("/{case_id}/records", tags=["Ledger"])
def get_records(case_id: str) -> list[RecordInputResponse]:
    """
    PL: Zwraca listę wpisów dla danego case_id.
    EN: Returns all entries for the given case_id.
    """
    items = ledger_service.get_records_for_case(case_id=case_id)
    return [RecordInputResponse(**it) for it in items]


@router.get("/{case_id}/prove", tags=["Ledger"])
def prove_case(case_id: str) -> dict[str, Any]:
    """
    PL: Generuje i (jeśli możliwe) waliduje Provenance Receipt dla sprawy.
    EN: Generates and (if available) validates the Provenance Receipt for a case.
    """
    try:
        receipt = ledger_service.build_provenance_receipt(case_id=case_id)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e)) from e

    if _provenance_validator is not None:
        try:
            _provenance_validator.validate(receipt)  # type: ignore[union-attr]
        except Exception as e:
            # 500: service error (receipt doesn't match contract)
            raise HTTPException(
                status_code=500,
                detail=f"Provenance receipt schema validation failed: {e}",
            ) from e

    return receipt

```


===== FILE: services/api_gateway/routers/mismatch.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/mismatch.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Router FastAPI dla usług CERTEUS.
EN: FastAPI router for CERTEUS services.
"""

from __future__ import annotations

from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from services.mismatch_service.models import TicketPriority
from services.mismatch_service.service import mismatch_service

router = APIRouter(prefix="/mismatch", tags=["mismatch"])


class EngineResult(BaseModel):
    status: str
    time_ms: float | None = None
    model: dict[str, Any] | None = None
    error: str | None = None
    version: str | None = None


class MismatchCreateRequest(BaseModel):
    case_id: str
    formula_str: str
    results: dict[str, EngineResult]
    formula_ast: dict[str, Any] | None = None
    priority: TicketPriority | None = Field(default=None)


@router.post("/tickets")
def create_ticket(req: MismatchCreateRequest) -> dict[str, Any]:
    t = mismatch_service.create_ticket(
        case_id=req.case_id,
        formula_str=req.formula_str,
        results={k: v.model_dump() for k, v in req.results.items()},
        formula_ast=req.formula_ast,
        priority=req.priority,
    )
    return t.model_dump()


@router.get("/tickets/{ticket_id}")
def get_ticket(ticket_id: str) -> dict[str, Any]:
    t = mismatch_service.get_ticket(ticket_id)
    if not t:
        raise HTTPException(status_code=404, detail="Ticket not found")
    return t.model_dump()

```


===== FILE: services/api_gateway/routers/pco_public.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/api_gateway/routers/pco_public.py          |
# | DATA: 2025-08-19                                                    |
# +=====================================================================+
# | ROLA:                                                               |
# |  EN: Public PCO (zero PII). Verifies Merkle (MVP/real) + Ed25519.   |
# |  PL: Publiczny PCO (0 PII). Weryfikuje Merkle (MVP/real) + Ed25519. |
# +=====================================================================+

from __future__ import annotations

# stdlib
import hashlib
import json
import os
from pathlib import Path
from typing import Any

# third-party
from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel, Field, field_validator

# core (kanoniczne funkcje i crypto)
from core.pco.crypto import (
    b64u_encode,
    canonical_bundle_hash_hex,
    canonical_digest_hex,
    compute_leaf_hex,
    ed25519_verify_b64u,
    load_pubkey_bytes_from_env,
    sha256_hex,
)

router = APIRouter(prefix="/pco/public", tags=["pco"])


# ──────────────────────────────────────────────────────────────────────────────
# Pomocnicze aliasy 1:1 pod testy (nie zmieniaj nazw)
# tests/test_pco_public.py importuje te symbole z tego modułu
def _hex(s: str) -> str:
    """SHA-256 hex of UTF-8 text (alias pod testy)."""
    return sha256_hex(s)


def _b64u(data: bytes) -> str:
    """Base64URL bez '=' (alias pod testy)."""
    return b64u_encode(data)


def _bundle_hash_hex(smt2_hash_hex: str, lfsc_text: str, drat_text: str | None = None) -> str:
    """Alias pod testy → wywołuje kanoniczną funkcję."""
    return canonical_bundle_hash_hex(smt2_hash_hex, lfsc_text, drat_text)


def _canonical_digest_hex(pub: dict[str, Any], merkle_root_hex: str) -> str:
    """Alias pod testy → wywołuje kanoniczną funkcję."""
    return canonical_digest_hex(
        rid=str(pub["rid"]),
        smt2_hash_hex=str(pub["smt2_hash"]),
        lfsc_text=str(pub.get("lfsc", "")),
        drat_text=str(pub["drat"]) if pub.get("drat") is not None else None,
        merkle_root_hex=merkle_root_hex,
    )


# ──────────────────────────────────────────────────────────────────────────────
# zero-PII: prosta denylista kluczy
FORBIDDEN_KEYS = {
    "name",
    "first_name",
    "last_name",
    "pesel",
    "email",
    "phone",
    "address",
    "dob",
    "ssn",
    "patient_id",
    "person_id",
    "user_id",
    "ip",
    "session_id",
    "headers",
}

DEFAULT_BUNDLE_DIR_FALLBACK = Path("./data/public_pco")


def _bundle_dir() -> Path:
    """Resolve bundle dir at call time, honoring current ENV."""
    return Path(os.getenv("PROOF_BUNDLE_DIR") or DEFAULT_BUNDLE_DIR_FALLBACK)


# ──────────────────────────────────────────────────────────────────────────────
# MODELE
class MerkleStep(BaseModel):
    sibling: str  # hex
    dir: str  # "L" or "R"


class PublicPCO(BaseModel):
    rid: str = Field(..., min_length=3)
    smt2_hash: str = Field(..., min_length=64, max_length=64)  # hex sha256 of SMT2
    lfsc: str = Field(..., min_length=2)  # LFSC (plain text)
    drat: str | None = None  # DRAT (plain text, optional)
    merkle_proof: list[MerkleStep] = Field(default_factory=list)  # list or empty
    signature: str = Field(..., min_length=40)  # detached (base64url, bez '=')

    @field_validator("smt2_hash")
    @classmethod
    def _hex64(cls, v: str) -> str:
        int(v, 16)  # raises on invalid hex
        return v.lower()


# ──────────────────────────────────────────────────────────────────────────────
# MERKLE helpers
def _h(b: bytes) -> bytes:
    return hashlib.sha256(b).digest()


def _apply_merkle_path(leaf_hex: str, path: list[MerkleStep]) -> str:
    # Pusta ścieżka ⇒ root == leaf (dokładnie jak w teście)
    if not path:
        return leaf_hex.lower()
    cur = bytes.fromhex(leaf_hex)
    for step in path:
        sib = bytes.fromhex(step.sibling)
        if step.dir == "L":
            cur = _h(sib + cur)
        elif step.dir == "R":
            cur = _h(cur + sib)
        else:
            raise HTTPException(status_code=400, detail=f"Invalid merkle step.dir: {step.dir}")
    return cur.hex()


def _parse_merkle_proof(raw: object) -> list[MerkleStep]:
    """
    Akceptuj:
      • [] (MVP)
      • [{"sibling":..., "dir":"L|R"}]
      • {"path":[...]} (zgodność wstecz)
      • alias 'position' ≡ 'dir'
    """
    if raw is None:
        return []
    if isinstance(raw, dict) and "path" in raw:
        raw = raw["path"]
    if isinstance(raw, list):
        norm: list[MerkleStep] = []
        for step in raw:
            if isinstance(step, MerkleStep):  # już zparsowany
                norm.append(step)
                continue
            if not isinstance(step, dict):
                raise HTTPException(status_code=400, detail="Invalid merkle step type")
            d = step.get("dir") or step.get("position")
            sib = step.get("sibling")
            if d not in ("L", "R"):
                raise HTTPException(
                    status_code=400,
                    detail="Invalid merkle step.dir/position",
                )
            if not isinstance(sib, str) or not sib:
                raise HTTPException(
                    status_code=400,
                    detail="Invalid merkle step: missing 'sibling'",
                )
            norm.append(MerkleStep(sibling=sib, dir=str(d)))
        return norm
    raise HTTPException(status_code=400, detail="merkle_proof must be list or {path:[...] }")


# ──────────────────────────────────────────────────────────────────────────────
# STORAGE
def _bundle_path(rid: str) -> Path:
    return _bundle_dir() / f"{rid}.json"


def _load_public_bundle_from_fs(rid: str) -> dict[str, Any]:
    p = _bundle_path(rid)
    if not p.exists():
        raise HTTPException(status_code=404, detail="PCO bundle not found")
    try:
        raw = p.read_text(encoding="utf-8")
        data = json.loads(raw)
        if not isinstance(data, dict):
            raise ValueError("Bundle is not an object")
        return data
    except Exception as e:  # pragma: no cover
        raise HTTPException(status_code=500, detail=f"Cannot read bundle: {e}") from e


# ──────────────────────────────────────────────────────────────────────────────
# PII
def _assert_no_pii(bundle: dict[str, Any]) -> None:
    bad = sorted(set(bundle.keys()) & FORBIDDEN_KEYS)
    if bad:
        raise HTTPException(status_code=422, detail=f"PII field(s) present: {bad}")


# ──────────────────────────────────────────────────────────────────────────────
# ENTRYPOINT
@router.get("/{rid}", response_model=PublicPCO)
def get_public_pco(rid: str, request: Request) -> PublicPCO:
    """
    EN/PL: Returns public PCO; validates zero-PII, Merkle proof, and Ed25519 signature.
    """
    pub = _load_public_bundle_from_fs(rid)
    _assert_no_pii(pub)

    # 1) Bundle hash → leaf → merkle root
    bundle_hash_hex = _bundle_hash_hex(
        str(pub["smt2_hash"]),
        str(pub.get("lfsc", "")),
        str(pub["drat"]) if pub.get("drat") is not None else None,
    )
    leaf_hex = compute_leaf_hex(str(pub["rid"]), bundle_hash_hex)
    path = _parse_merkle_proof(pub.get("merkle_proof"))
    merkle_root_hex = _apply_merkle_path(leaf_hex, path)

    # 2) Canonical digest (dokładnie jak w testach)
    digest_hex = _canonical_digest_hex(pub, merkle_root_hex)

    # 3) Ed25519 (detached, base64url) nad bytes.fromhex(digest_hex)
    try:
        ed25519_verify_b64u(
            load_pubkey_bytes_from_env(),
            str(pub["signature"]),
            digest_hex,
        )
    except Exception as e:
        # nie ujawniamy szczegółów kryptograficznych na zewnątrz
        raise HTTPException(status_code=400, detail="Invalid signature") from e

    # Pydantic dodatkowo sanityzuje typy/formaty
    return PublicPCO(**{**pub, "merkle_proof": path})

```


===== FILE: services/api_gateway/routers/preview.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/api_gateway/routers/preview.py             |
# | ROLE / ROLA:                                                         |
# |  EN: Stub preview endpoint for uploaded files (PDF/DOCX/TXT/IMG).   |
# |  PL: Endpoint podglądu wgranych plików (PDF/DOCX/TXT/IMG).          |
# +=====================================================================+

"""PL: Router podglądu plików. EN: Preview router."""

from __future__ import annotations

import shutil
import uuid
from pathlib import Path
from typing import Annotated

from fastapi import APIRouter, File, UploadFile
from fastapi.responses import JSONResponse

router = APIRouter()

STATIC_PREV = Path("static/previews")
STATIC_PREV.mkdir(parents=True, exist_ok=True)


@router.post("/v1/preview")
async def preview(file: Annotated[UploadFile, File(...)]) -> JSONResponse:
    """
    PL: Zwraca URL do podglądu pliku. Na razie zapisuje pod /static/previews/.
    EN: Returns a URL for preview. For now writes into /static/previews/.
    """
    raw_name: str = file.filename or "upload.bin"  # UploadFile.filename can be None
    ext = Path(raw_name).suffix.lower()

    safe_name = f"{uuid.uuid4().hex}{ext}"
    dst = STATIC_PREV / safe_name

    try:
        with dst.open("wb") as out:
            shutil.copyfileobj(file.file, out)
    finally:
        await file.close()

    return JSONResponse({"url": f"/static/previews/{safe_name}"})

```


===== FILE: services/api_gateway/routers/publish.py =====
```text
# +=====================================================================+
# |                          CERTEUS — HEART                            |
# +=====================================================================+
# | FILE: services/api_gateway/routers/publish.py                       |
# | ROLE:                                                               |
# |  PL: Publiczny endpoint publikacji (kontrakt Publication V1).       |
# |  EN: Public publication endpoint (Publication V1 contract).         |
# +=====================================================================+

"""PL: Mapuje wynik rdzenia na kontrakt publikacji. EN: Map core to publication contract."""

from __future__ import annotations

from typing import Any

from fastapi import APIRouter, Header

from core.truthops.engine import post_solve, pre_solve
from runtime.proof_queue import PROOF_QUEUE

router = APIRouter()


@router.post("/defx/reason")
def reason(
    body: dict[str, Any],
    x_norm_pack_id: str = Header(..., alias="X-Norm-Pack-ID"),
    x_jurisdiction: str = Header(..., alias="X-Jurisdiction"),
) -> dict[str, Any]:
    """PL: Zwraca status + PCO/plan/eta_hint. EN: Returns status + PCO/plan/eta_hint."""
    pre = pre_solve(body, policy_profile="default")
    if pre.heat != "HOT":
        task = PROOF_QUEUE.enqueue(
            tenant=body.get("tenant", "anon"), heat=pre.heat, payload=body, sla=body.get("sla", "basic")
        )
        return {
            "status": "PENDING",
            "proof_task_id": task.id,
            "eta_hint": task.eta_hint,
            "pco.plan": pre.plan,
            "headers": {"X-Norm-Pack-ID": x_norm_pack_id, "X-Jurisdiction": x_jurisdiction},
        }

    artifacts: dict[str, Any] = {}  # plug: wyniki z szybkich solverów
    decision, meta = post_solve(artifacts, policy_profile="default")
    resp: dict[str, Any] = {
        "status": decision,
        "headers": {"X-Norm-Pack-ID": x_norm_pack_id, "X-Jurisdiction": x_jurisdiction},
    }
    if decision == "PUBLISH":
        resp["pco"] = meta.get("pco", {})
    else:
        resp["pco.plan"] = meta.get("plan", {})
    return resp

```


===== FILE: services/api_gateway/routers/system.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/system.py |
# | DATE:    2025-08-17                                                 |
# +=====================================================================+

"""
PL: Router narzędziowy (systemowy). Udostępnia:
    • /v1/ingest  – lekki stub OCR → FACTLOG (z nagłówkiem łańcucha, limitami, MIME),
    • /v1/analyze – minimalny stub analizy E2E (SAT),
    • /v1/sipp/snapshot/{act_id} – migawka aktu prawnego (z `_certeus`).

EN: System/utility router exposing:
    • /v1/ingest  – light OCR → FACTLOG stub (with chain header, limits, MIME),
    • /v1/analyze – minimal E2E analysis stub (SAT),
    • /v1/sipp/snapshot/{act_id} – legal act snapshot (with `_certeus`).
"""

from __future__ import annotations

import json
from datetime import datetime, timezone
from hashlib import sha256
from typing import Annotated, Any

from fastapi import APIRouter, File, HTTPException, Response, UploadFile
from pydantic import BaseModel, Field

from services.ingest_service.adapters.contracts import Blob
from services.ingest_service.adapters.ocr_injector import build_ocr_preview

router = APIRouter(tags=["system"])

# ---------------------------------------------------------------------
# /v1/ingest
# ---------------------------------------------------------------------


class IngestResult(BaseModel):
    kind: str = Field(default="fact")
    role: str
    source: str
    value: str
    fact_id: str


@router.post("/v1/ingest")
async def ingest_document(
    response: Response,
    file: Annotated[UploadFile, File(...)],
) -> list[dict[str, Any]]:
    """
    PL: Zwraca dwa deterministyczne fakty (z `fact_id`, `thesis`, `confidence_score`),
        waliduje MIME/rozmiar i ustawia nagłówek łańcucha `X-CERTEUS-Ledger-Chain`.
        Dodatkowo (nieinwazyjnie) umieszcza skrót OCR w nagłówku
        `X-CERTEUS-OCR-Preview` dla PDF/obrazów.
    EN: Returns two deterministic facts (with `fact_id`, `thesis`, `confidence_score`),
        validates MIME/size, and sets `X-CERTEUS-Ledger-Chain` header. Also (non-
        invasive) puts OCR snippet into `X-CERTEUS-OCR-Preview` header for PDF/images.
    """
    MAX_BYTES = 10 * 1024 * 1024  # 10 MiB
    allowed_mime = "application/pdf"

    if (file.content_type or "") != allowed_mime:
        raise HTTPException(status_code=415, detail="Only application/pdf is supported")

    content = await file.read()
    if len(content) > MAX_BYTES:
        raise HTTPException(status_code=413, detail="File too large")

    src = file.filename or "document.pdf"

    def make_fact(role: str, value: str) -> dict[str, Any]:
        base = f"{src}|{role}|{value}".encode()
        fid = "fact-" + sha256(base).hexdigest()[:12]
        return {
            "kind": "fact",
            "role": role,
            "source": src,
            "value": value,
            "fact_id": fid,
        }

    facts = [
        make_fact("claim_contract_date", "2023-10-01"),
        make_fact("evidence_payment", "TAK"),
    ]

    # Uzupełnij wymagane pola (thesis + confidence)
    for f in facts:
        if f["role"] == "claim_contract_date":
            f["thesis"] = "Umowa została zawarta 2023-10-01."
            f["confidence_score"] = 0.98
        elif f["role"] == "evidence_payment":
            f["thesis"] = "Istnieje dowód wpłaty."
            f["confidence_score"] = 0.99

    # Nagłówek łańcucha: "sha256:<...>;sha256:<...>"
    chain_parts: list[str] = []
    for f in facts:
        payload = json.dumps(f, ensure_ascii=False, sort_keys=True).encode("utf-8")
        chain_parts.append("sha256:" + sha256(payload).hexdigest())
    response.headers["X-CERTEUS-Ledger-Chain"] = ";".join(chain_parts)

    # DODATEK: OCR preview jako nagłówek (bez zmiany body).
    try:
        blob = Blob(
            filename=src,
            content_type=file.content_type or "application/octet-stream",
            data=content,
        )
        ocr = await build_ocr_preview(blob, case_id=None, max_chars=160)
        preview = ocr.get("ocr_preview")
        if preview:
            # Krótki, jednowierszowy nagłówek (bez znaków nowych linii).
            response.headers["X-CERTEUS-OCR-Preview"] = " ".join(preview.split())
    except Exception:
        # Bezpieczne pominięcie OCR w razie błędu stubu.
        pass

    return facts


# ---------------------------------------------------------------------
# /v1/analyze
# ---------------------------------------------------------------------


@router.post("/v1/analyze")
async def analyze(case_id: str, file: Annotated[UploadFile, File(...)]) -> dict[str, Any]:
    """
    Minimalny stub E2E: przyjmuje PDF i zwraca wynik SAT z prostym modelem.
    Zgodne z tests/e2e/test_e2e_pl_286kk_0001.py.
    """
    await file.read()  # nieużywane w stubie
    return {
        "case_id": case_id,
        "analysis_result": {"status": "sat", "model": "[x=True]"},
    }


# ---------------------------------------------------------------------
# /v1/sipp/snapshot/{act_id}
# ---------------------------------------------------------------------


@router.get("/v1/sipp/snapshot/{act_id}")
async def get_snapshot(act_id: str) -> dict[str, Any]:
    """Zwraca minimalny snapshot (dict), aby dozwolić klucz `_certeus`."""
    text = (
        "Art. 286 k.k.: Kto, w celu osiągnięcia korzyści majątkowej, doprowadza inną osobę "
        "do niekorzystnego rozporządzenia mieniem za pomocą wprowadzenia w błąd..."
    ).strip()
    digest = "sha256:" + sha256(text.encode("utf-8")).hexdigest()
    snap_ts = datetime.now(timezone.utc).isoformat(timespec="seconds")

    return {
        "act_id": act_id,
        "version_id": "2023-10-01",
        "title": "Kodeks karny – art. 286",
        "source_url": ("https://isap.sejm.gov.pl/isap.nsf/DocDetails.xsp?id=WDU19970880553"),
        "text": text,
        "text_sha256": digest,
        "at": None,
        "snapshot_timestamp": snap_ts,
        "_certeus": {"snapshot_timestamp_utc": snap_ts},
    }

```


===== FILE: services/api_gateway/routers/verify.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/verify.py  |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: services/api_gateway/routers/verify.py                |
# | ROLE: Expose /v1/verify endpoint (Truth Engine).            |
# | PLIK: services/api_gateway/routers/verify.py                |
# | ROLA: Udostępnia endpoint /v1/verify (Silnik Prawdy).       |
# +-------------------------------------------------------------+
"""
PL: Publiczny endpoint do weryfikacji formuł SMT-LIB2 przez Silnik Prawdy.
EN: Public endpoint to verify SMT-LIB2 formulas via the Truth Engine.
"""

from __future__ import annotations

# === IMPORTY / IMPORTS ======================================== #
from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from kernel.mismatch_protocol import MismatchError
from kernel.truth_engine import DualCoreVerifier

# === ROUTER / ROUTER ========================================== #
router = APIRouter(prefix="/v1", tags=["Truth Engine"])
_verifier = DualCoreVerifier()


# === DTO / MODELE DANYCH ====================================== #
class VerificationRequest(BaseModel):
    """
    PL: Wejściowy DTO do weryfikacji (MVP: tylko 'smt2').
    EN: Input DTO for verification (MVP: 'smt2' only).
    """

    formula: str
    lang: str = "smt2"


# === ENDPOINTY / ENDPOINTS ==================================== #
@router.post("/verify")
def verify_formula(req: VerificationRequest) -> dict[str, Any]:
    """
    PL: Weryfikuje formułę. Zwraca sat/unsat/unknown oraz artefakty (model/proof).
    EN: Verifies the formula. Returns sat/unsat/unknown and artifacts (model/proof).
    """
    try:
        # lang w DualCoreVerifier.verify jest parametrem *keyword-only*.
        return _verifier.verify(req.formula, lang=req.lang)
    except MismatchError as e:
        raise HTTPException(
            status_code=409,
            detail={"requires_human": True, "message": str(e)},
        ) from e
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e)) from e
    except Exception as e:  # pragma: no cover
        raise HTTPException(status_code=500, detail=f"Unexpected error: {e}") from e


# === KONIEC / END ============================================= #

```


===== FILE: services/api_gateway/routers/well_known_jwks.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE: services/api_gateway/routers/.well_known_jwks.py            |
# | DATE:   2025-08-19                                                  |
# +=====================================================================+
"""
PL: Endpoint publikujący klucz publiczny Ed25519 w formacie JWKS (/.well-known/jwks.json).
EN: Endpoint exposing Ed25519 public key via JWKS (/.well-known/jwks.json).
"""

# ----Bloki----- IMPORTY
from __future__ import annotations

import base64
import hashlib
import os

from fastapi import APIRouter, HTTPException

# ----Bloki----- KONFIGURACJA
router = APIRouter(prefix="", tags=["well-known"])


def _b64u(data: bytes) -> str:
    return base64.urlsafe_b64encode(data).rstrip(b"=").decode("ascii")


def _load_pubkey_bytes() -> bytes:
    b64u = os.getenv("ED25519_PUBKEY_B64URL")
    if b64u:
        pad = "=" * (-len(b64u) % 4)
        return base64.urlsafe_b64decode(b64u + pad)
    hexv = os.getenv("ED25519_PUBKEY_HEX")
    if hexv:
        return bytes.fromhex(hexv)
    raise RuntimeError("Brak klucza publicznego: ustaw ED25519_PUBKEY_B64URL lub ED25519_PUBKEY_HEX")


def _kid_from_key(pub: bytes) -> str:
    return hashlib.sha256(pub).hexdigest()[:16]


# ----Bloki----- ENTRYPOINT
@router.get("/.well-known/jwks.json")
def jwks():
    try:
        pub = _load_pubkey_bytes()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e)) from e
    kid = _kid_from_key(pub)
    jwk = {
        "kty": "OKP",
        "crv": "Ed25519",
        "x": _b64u(pub),
        "kid": kid,
        "use": "sig",
        "alg": "EdDSA",
    }
    return {"keys": [jwk]}

```


===== FILE: services/exporter_service/__init__.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/exporter_service/__init__.py   |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Pakiet inicjalizacyjny modułu.
EN: Package initializer.
"""

from __future__ import annotations

from .exporter import ExporterService, export_answer, export_answer_to_txt

__all__ = ["ExporterService", "export_answer_to_txt", "export_answer"]

```


===== FILE: services/exporter_service/exporter.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/exporter_service/exporter.py   |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Eksport raportów i artefaktów procesu.
EN: Report/artefact exporter.
"""

from __future__ import annotations

import json
from collections.abc import Mapping
from pathlib import Path
from typing import Any

# (hash helpers są w ledger_service, ale nie są tu potrzebne do samego eksportu)

TEMPL_REPORT = """# CERTEUS Report
Case: {case_id}
Status: {status}
Model: {model}
"""


class ExporterService:
    def __init__(self, template_dir: str, output_dir: str) -> None:
        self.template_dir = Path(template_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def export_report(self, case_id: str, analysis: dict[str, Any]) -> Path:
        status = str(analysis.get("status", "")).upper()
        model = analysis.get("model", "")
        content = TEMPL_REPORT.format(case_id=case_id, status=status, model=model)
        out = self.output_dir / f"{case_id}.txt"
        out.write_text(content, encoding="utf-8")
        return out


def export_answer_to_txt(answer: Mapping[str, Any], *, out_path: str, create_ledger_entry: bool = False) -> str:
    p = Path(out_path)
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(answer, indent=2, sort_keys=True), encoding="utf-8")
    return str(p)


def export_answer(answer: Mapping[str, Any], *, fmt: str, output_dir: Path | None = None):
    """
    - fmt="json": return pretty json string
    - fmt="file": write <case_id>.json to output_dir, return Path
    - fmt="docx": write placeholder .docx (text), return Path
    """
    if fmt == "json":
        return json.dumps(answer, indent=2, sort_keys=True)

    outdir = output_dir or Path("build/exports")
    outdir.mkdir(parents=True, exist_ok=True)
    case_id = str(answer.get("case_id", "case"))

    if fmt == "file":
        p = outdir / f"{case_id}.json"
        p.write_text(json.dumps(answer, indent=2, sort_keys=True), encoding="utf-8")
        return p

    if fmt == "docx":
        p = outdir / f"{case_id}.docx"
        p.write_text(json.dumps(answer, indent=2, sort_keys=True), encoding="utf-8")
        return p

    raise ValueError(f"Unsupported fmt: {fmt}")

```


===== FILE: services/ingest_service/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ingest_service/__init__.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: services/ingest_service/__init__.py
# License: Apache-2.0
# Description (PL): Pakiet serwisu ingestii (FACTLOG): eksportuje główne typy.
# Description (EN): Ingestion service package (FACTLOG): exports main types.
# Style Guide: ASCII header, PL/EN docs, labeled code blocks. (See README)
# ────────────────────────────────────────────────────────────────────────────────

"""
PL: Pakiet serwisu ingestii. Ten moduł oznacza pakiet Pythona oraz
    udostępnia publiczny interfejs (eksport) najczęściej używanych typów.
EN: Ingestion service package. This module marks the Python package and
    exposes a public interface (exports) of the most commonly used types.
"""

# [BLOCK: PUBLIC API EXPORTS / EKSPORT INTERFEJSU PUBLICZNEGO]
from .models import Fact, FactRole

__all__ = ["Fact", "FactRole"]

```


===== FILE: services/ingest_service/adapters/__init__.py =====
```text
# =============================================================================
#  CERTEUS — adapters package
#  PL: Adaptery (Preview/OCR/Drive/LLM).
#  EN: Adapters (Preview/OCR/Drive/LLM).
# =============================================================================
"""
PL: Pakiet adapterów (Preview/OCR/Drive/LLM).
EN: Adapters package (Preview/OCR/Drive/LLM).
"""

```


===== FILE: services/ingest_service/adapters/contracts.py =====
```text
# =============================================================================
#  CERTEUS — Adapters Contracts
#  Preview/OCR/Drive/LLM interfaces (Protocols) + lightweight DTOs
# =============================================================================
#  PL
#  Ten moduł definiuje kontrakty (interfejsy) adapterów używanych przez API:
#  - PreviewAdapter: DOCX→PDF (i ogólny preview), zwraca PreviewResult
#  - OCRAdapter: ekstrakcja tekstu z PDF/obrazów (stub na start)
#  - DriveAdapter: prosty storage (np. lokalny /static albo chmura)
#  - LLMAdapter: analiza/inspekcja treści (stub ALI pod /v1/analyze)
#
#  Zasady:
#  - Proste DTO z dataclasses (bez zależności pydantic).
#  - Asynchroniczne metody (spójne z FastAPI).
#  - Linie do 100 znaków, LF, brak trailing whitespace.
#  - Wyjątki dziedziczą po AdapterError.
#
#  EN
#  This module defines adapter contracts used by the API layer:
#  - PreviewAdapter: DOCX→PDF (generic preview), returns PreviewResult
#  - OCRAdapter: text extraction from PDFs/images (stub initially)
#  - DriveAdapter: storage abstraction (local /static or cloud)
#  - LLMAdapter: analysis/inspection (ALI stub used by /v1/analyze)
#
#  Rules:
#  - Lightweight DTOs via dataclasses (no pydantic dependency).
#  - Async methods (compatible with FastAPI).
#  - 100-char lines, LF endings, no trailing whitespace.
#  - Exceptions derive from AdapterError.
# =============================================================================
"""
PL: Kontrakty adapterów i lekkie DTO (Preview/OCR/Drive/LLM) używane przez API.
EN: Adapter contracts and lightweight DTOs (Preview/OCR/Drive/LLM) used by the API.
"""

from __future__ import annotations

from collections.abc import Iterable, Mapping, Sequence
from dataclasses import dataclass, field
from typing import Any, Literal, Protocol

# ----------------------------- Common DTOs ----------------------------------


@dataclass(slots=True)
class Blob:
    """
    PL: Surowe dane pliku.
    EN: Raw file payload.
    """

    filename: str
    content_type: str
    data: bytes


@dataclass(slots=True)
class PreviewRequest:
    """
    PL: Wejście do adaptera preview. 'target_format' np. 'application/pdf'.
    EN: Input for preview adapter. 'target_format' e.g. 'application/pdf'.
    """

    blob: Blob
    case_id: str
    target_format: str = "application/pdf"
    deterministic: bool = True


@dataclass(slots=True)
class PreviewResult:
    """
    PL: Wynik generowania podglądu. 'url' wskazuje zasób serwowany przez API.
    EN: Preview generation result. 'url' points to API-served resource.
    """

    url: str
    content_type: str
    pages: int | None = None
    size_bytes: int | None = None
    meta: Mapping[str, Any] = field(default_factory=dict)


@dataclass(slots=True)
class OCRPage:
    """
    PL: Tekst i proste metadane strony po OCR.
    EN: OCR'ed page text and basic metadata.
    """

    index: int
    text: str
    width_px: int | None = None
    height_px: int | None = None
    meta: Mapping[str, Any] = field(default_factory=dict)


@dataclass(slots=True)
class OCRRequest:
    """
    PL: Wejście do OCR; jeśli PDF → stronicowanie wewnątrz adaptera.
    EN: OCR input; if PDF → paging handled inside the adapter.
    """

    blob: Blob
    lang_hint: str = "pl+en"
    dpi: int = 300
    max_pages: int | None = None
    case_id: str | None = None


@dataclass(slots=True)
class DriveSaveResult:
    """
    PL: Wynik zapisu do storage (lokalny/chmura).
    EN: Storage save result (local/cloud).
    """

    file_id: str
    url: str | None = None
    size_bytes: int | None = None
    content_type: str | None = None
    meta: Mapping[str, Any] = field(default_factory=dict)


@dataclass(slots=True)
class Attachment:
    """
    PL: Załącznik do analizy LLM (np. tekst OCR).
    EN: Attachment for LLM analysis (e.g., OCR text).
    """

    name: str
    kind: Literal["text", "preview_url", "binary"]
    content: str | bytes
    content_type: str | None = None


@dataclass(slots=True)
class LLMRequest:
    """
    PL: Wejście do adaptera LLM.
    EN: Input for LLM adapter.
    """

    prompt: str
    attachments: Sequence[Attachment] = ()
    case_id: str | None = None
    model_hint: str | None = None
    temperature: float = 0.0
    max_tokens: int | None = None


@dataclass(slots=True)
class LLMResponse:
    """
    PL: Zunifikowany wynik LLM do /v1/analyze (stub ALI).
    EN: Unified LLM result for /v1/analyze (ALI stub).
    """

    status: Literal["ok", "error"]
    model: str
    answer: Mapping[str, Any]
    trace: Sequence[str] = ()
    provenance: Mapping[str, Any] = field(default_factory=dict)
    error: str | None = None


# ------------------------------- Errors -------------------------------------


class AdapterError(RuntimeError):
    """PL: Błąd ogólny adapterów. EN: Generic adapters error."""


class PreviewError(AdapterError):
    """PL/EN: Preview adapter error."""


class OCRError(AdapterError):
    """PL/EN: OCR adapter error."""


class DriveError(AdapterError):
    """PL/EN: Drive/storage adapter error."""


class LLMError(AdapterError):
    """PL/EN: LLM adapter error."""


# ------------------------------ Interfaces ----------------------------------


class PreviewAdapter(Protocol):
    """
    PL:
      Interfejs generowania podglądu (np. DOCX→PDF).
      Kontrakt:
        - Wejście: PreviewRequest
        - Wyjście: PreviewResult (URL serwowany przez API, np. /static/previews/..)
        - Determinizm: dla tych samych danych i case_id ścieżka może być stała.
    EN:
      Preview generation interface (e.g., DOCX→PDF).
      Contract:
        - Input: PreviewRequest
        - Output: PreviewResult (API-served URL, e.g., /static/previews/..)
        - Determinism: stable path for same input + case_id (optional).
    """

    async def generate(self, request: PreviewRequest) -> PreviewResult:
        """PL/EN: Produce preview for given blob."""
        ...


class OCRAdapter(Protocol):
    """
    PL:
      Interfejs OCR dla PDF/obrazów.
      Kontrakt:
        - Wejście: OCRRequest
        - Wyjście: Iterable[OCRPage] (kolejność stron gwarantowana)
        - Minimalna gwarancja stubu: zwróć 1 stronę z prostym 'text' (echo/heurystyka)
    EN:
      OCR interface for PDF/images.
      Contract:
        - Input: OCRRequest
        - Output: Iterable[OCRPage] (page order guaranteed)
        - Stub minimum guarantee: return 1 page with basic 'text' (echo/heuristic)
    """

    async def extract(self, request: OCRRequest) -> Iterable[OCRPage]:
        """PL/EN: Extract text pages from the blob (PDF/image)."""
        ...


class DriveAdapter(Protocol):
    """
    PL:
      Abstrakcja storage (lokalny katalog /static, S3, GDrive, itd.).
      Kontrakt:
        - save_bytes: zapisuje dane i zwraca logiczne ID oraz (opcjonalnie) URL
        - read_bytes: pobiera blob po file_id
        - url_for: zwraca publiczny URL jeśli dostępny
    EN:
      Storage abstraction (local /static, S3, GDrive, etc.).
      Contract:
        - save_bytes: persists payload and returns logical ID and optional URL
        - read_bytes: fetches payload by file_id
        - url_for: returns public URL if available
    """

    async def save_bytes(
        self,
        data: bytes,
        *,
        filename: str,
        content_type: str,
        case_id: str | None = None,
        deterministic: bool = True,
    ) -> DriveSaveResult:
        """PL/EN: Persist a payload and return its handle."""
        ...

    async def read_bytes(self, file_id: str) -> bytes:
        """PL/EN: Retrieve raw bytes by logical file id."""
        ...

    async def url_for(self, file_id: str) -> str | None:
        """PL/EN: Optional public URL for given file id."""
        ...


class LLMAdapter(Protocol):
    """
    PL:
      Interfejs do warstwy analitycznej LLM (stub ALI).
      Kontrakt:
        - analyze: zwraca LLMResponse zgodny z oczekiwaniem /v1/analyze
        - Implementacja stub: deterministyczny 'model', krótki 'trace', 'answer'
    EN:
      Interface to LLM analytical layer (ALI stub).
      Contract:
        - analyze: returns LLMResponse aligned with /v1/analyze expectations
        - Stub impl: deterministic 'model', short 'trace', 'answer'
    """

    async def analyze(self, request: LLMRequest) -> LLMResponse:
        """PL/EN: Perform analysis over prompt + attachments."""
        ...


# ----------------------------- Helper Contracts -----------------------------


def infer_extension(content_type: str, fallback: str = ".bin") -> str:
    """
    PL: Prosty mapping MIME→rozszerzenie dla ścieżek deterministycznych.
    EN: Simple MIME→extension mapping for deterministic paths.
    """
    mapping = {
        "application/pdf": ".pdf",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
        "text/plain": ".txt",
        "image/png": ".png",
        "image/jpeg": ".jpg",
        "image/jpg": ".jpg",
    }
    return mapping.get(content_type.lower(), fallback)


__all__ = [
    "Blob",
    "PreviewRequest",
    "PreviewResult",
    "OCRPage",
    "OCRRequest",
    "DriveSaveResult",
    "Attachment",
    "LLMRequest",
    "LLMResponse",
    "AdapterError",
    "PreviewError",
    "OCRError",
    "DriveError",
    "LLMError",
    "PreviewAdapter",
    "OCRAdapter",
    "DriveAdapter",
    "LLMAdapter",
    "infer_extension",
]

```


===== FILE: services/ingest_service/adapters/local_impl.py =====
```text
# =============================================================================
#  CERTEUS — Local Adapters (stubs)
#  Preview/OCR/Drive/LLM — deterministic, no-cloud implementations
# =============================================================================
#  PL
#  Proste, lokalne implementacje interfejsów adapterów:
#   - LocalDriveAdapter: zapis/odczyt pod ./static
#   - StubPreviewAdapter: "DOCX→PDF" (1-str. PDF placeholder)
#   - StubOCRAdapter: zwraca 1 stronę z heurystycznym tekstem
#   - StubLLMAdapter: deterministyczny ALI-stub do /v1/analyze
#
#  EN
#  Simple local implementations of adapter interfaces:
#   - LocalDriveAdapter: save/read under ./static
#   - StubPreviewAdapter: "DOCX→PDF" (1-page PDF placeholder)
#   - StubOCRAdapter: returns 1 page with heuristic text
#   - StubLLMAdapter: deterministic ALI stub for /v1/analyze
#
#  Zasady/Rules:
#  - Deterministyczne ID: sha256(data, filename, content_type, case_id, salt)
#  - Tylko stdlib. Linie ≤ 100 znaków, LF, brak trailing spaces.
# =============================================================================
"""
PL: Lokalne implementacje stubów adapterów (bez chmury).
EN: Local stub implementations of adapters (no cloud).
"""

from __future__ import annotations

import hashlib
from collections.abc import Iterable, Sequence
from dataclasses import asdict
from pathlib import Path

from .contracts import (
    DriveAdapter,
    DriveError,
    DriveSaveResult,
    LLMAdapter,
    LLMRequest,
    LLMResponse,
    OCRAdapter,
    OCRError,
    OCRPage,
    OCRRequest,
    PreviewAdapter,
    PreviewError,
    PreviewRequest,
    PreviewResult,
    infer_extension,
)

# ------------------------------ Helpers -------------------------------------


def _stable_hexdigest(*parts: str | bytes) -> str:
    """PL/EN: Deterministic short sha256 hex."""
    h = hashlib.sha256()
    for p in parts:
        if isinstance(p, str):
            p = p.encode("utf-8", "ignore")
        h.update(p)
    return h.hexdigest()[:24]


def _ensure_dir(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


# --------------------------- LocalDriveAdapter ------------------------------


class LocalDriveAdapter(DriveAdapter):
    """
    PL: Lokalny storage zapisujący do ./static; zwraca URL pod /static.
    EN: Local storage saving to ./static; returns URLs under /static.
    """

    def __init__(self, base_dir: str | Path = "static", base_url_prefix: str = "/static"):
        self.base_dir = Path(base_dir)
        self.base_url_prefix = base_url_prefix.rstrip("/")

    async def save_bytes(
        self,
        data: bytes,
        *,
        filename: str,
        content_type: str,
        case_id: str | None = None,
        deterministic: bool = True,
    ) -> DriveSaveResult:
        try:
            salt = "det" if deterministic else _stable_hexdigest(filename)
            file_id = _stable_hexdigest(data, filename, content_type, case_id or "", salt)
            ext = infer_extension(content_type)
            rel = Path("uploads") / (case_id or "general") / f"{file_id}{ext}"
            dst = self.base_dir / rel
            _ensure_dir(dst)
            dst.write_bytes(data)
            url = f"{self.base_url_prefix}/{rel.as_posix()}"
            return DriveSaveResult(
                file_id=str(rel),
                url=url,
                size_bytes=len(data),
                content_type=content_type,
                meta={"deterministic": deterministic},
            )
        except Exception as e:
            raise DriveError("LocalDriveAdapter.save_bytes failed") from e

    async def read_bytes(self, file_id: str) -> bytes:
        try:
            path = self.base_dir / Path(file_id)
            return path.read_bytes()
        except Exception as e:
            raise DriveError(f"LocalDriveAdapter.read_bytes failed: {file_id}") from e

    async def url_for(self, file_id: str) -> str | None:
        # PL: Dla lokalnego storage URL jest deterministyczny.
        # EN: Deterministic URL for local storage.
        rel = Path(file_id)
        return f"{self.base_url_prefix}/{rel.as_posix()}"


# --------------------------- StubPreviewAdapter -----------------------------


class StubPreviewAdapter(PreviewAdapter):
    """
    PL:
      Tworzy minimalny PDF (1 strona) jako placeholder podglądu.
      Nie wykonuje realnej konwersji DOCX→PDF; to most do UI (/v1/preview).
    EN:
      Produces a minimal PDF (1 page) as preview placeholder.
      No real DOCX→PDF; just a bridge to the UI (/v1/preview).
    """

    def __init__(self, drive: LocalDriveAdapter):
        self.drive = drive

    @staticmethod
    def _minimal_pdf(title: str) -> bytes:
        # PL: Mini-PDF wystarczający do smoke-testu w przeglądarce.
        # EN: Tiny PDF good enough for a browser smoke test.
        # Uwaga: nieidealny, ale poprawny syntaktycznie.
        pdf = (
            b"%PDF-1.4\n"
            b"1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\n"
            b"2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\n"
            b"3 0 obj<</Type/Page/Parent 2 0 R/MediaBox[0 0 612 792]/Contents 4 0 R"
            b"/Resources<</Font<</F1 5 0 R>>>>>>endobj\n"
            b"4 0 obj<</Length 55>>stream\nBT\n/F1 24 Tf\n72 720 Td\n("
            + title.encode("latin-1", "ignore")
            + b") Tj\nET\nendstream\nendobj\n"
            b"5 0 obj<</Type/Font/Subtype/Type1/BaseFont/Helvetica>>endobj\n"
            b"xref\n0 6\n0000000000 65535 f \n"
            b"0000000010 00000 n \n0000000060 00000 n \n0000000116 00000 n \n"
            b"0000000276 00000 n \n0000000401 00000 n \n"
            b"trailer<</Size 6/Root 1 0 R>>\nstartxref\n480\n%%EOF\n"
        )
        return pdf

    async def generate(self, request: PreviewRequest) -> PreviewResult:
        try:
            title = f"Preview: {request.blob.filename}"
            pdf_bytes = self._minimal_pdf(title)
            saved = await self.drive.save_bytes(
                pdf_bytes,
                filename=request.blob.filename,
                content_type="application/pdf",
                case_id=request.case_id,
                deterministic=request.deterministic,
            )
            url = await self.drive.url_for(saved.file_id)
            return PreviewResult(
                url=url or "",
                content_type="application/pdf",
                pages=1,
                size_bytes=len(pdf_bytes),
                meta={
                    "source_content_type": request.blob.content_type,
                    "note": "stub preview",
                },
            )
        except Exception as e:
            raise PreviewError("StubPreviewAdapter.generate failed") from e


# ----------------------------- StubOCRAdapter -------------------------------


class StubOCRAdapter(OCRAdapter):
    """
    PL: Zwraca jedną stronę z prostym tekstem na podstawie metadanych pliku.
    EN: Returns single page with simple text based on file metadata.
    """

    async def extract(self, request: OCRRequest) -> Iterable[OCRPage]:
        try:
            text = (
                f"[STUB_OCR]\nfilename={request.blob.filename}\n"
                f"content_type={request.blob.content_type}\n"
                f"bytes={len(request.blob.data)}\nlang_hint={request.lang_hint}\n"
            )
            return [OCRPage(index=0, text=text, meta={"stub": True})]
        except Exception as e:
            raise OCRError("StubOCRAdapter.extract failed") from e


# ----------------------------- StubLLMAdapter -------------------------------


class StubLLMAdapter(LLMAdapter):
    """
    PL: Deterministyczny stub. Nie łączy się z żadnym LLM.
    EN: Deterministic stub. Does not call any LLM.
    """

    def __init__(self, model_name: str = "ALI-Stub-0"):
        self.model_name = model_name

    async def analyze(self, request: LLMRequest) -> LLMResponse:
        try:
            att_summary: list[str] = []
            for a in request.attachments:
                if isinstance(a.content, bytes | bytearray):
                    size = len(a.content)
                else:
                    size = len(str(a.content))
                att_summary.append(f"{a.kind}:{a.name}:{size}")

            answer = {
                "summary": {"prompt_len": len(request.prompt), "attachments": att_summary},
                "satisfied": [],
                "missing": [],
            }
            trace: Sequence[str] = ("stub:init", "stub:analyze", "stub:done")
            provenance = {
                "adapter": "StubLLMAdapter",
                "request": {**asdict(request), "attachments": None},
            }
            return LLMResponse(
                status="ok",
                model=self.model_name,
                answer=answer,
                trace=trace,
                provenance=provenance,
            )
        except Exception as e:
            raise DriveError("StubLLMAdapter.analyze failed") from e

```


===== FILE: services/ingest_service/adapters/ocr_injector.py =====
```text
# =============================================================================
#  CERTEUS — OCR Injector (helper for /v1/ingest)
# =============================================================================
#  PL:
#    Lekki helper do pobrania podglądu OCR (pierwsza strona) i zbudowania meta,
#    które można bezpiecznie dołączyć do odpowiedzi /v1/ingest jako "ocr_preview".
#    Domyślnie nie zmienia istniejących pól (2 fakty + X-CERTEUS-Ledger-Chain).
#
#  EN:
#    Lightweight helper to fetch OCR preview (first page) and build a meta dict
#    that can be safely merged into /v1/ingest response under "ocr_preview".
#    It does not alter existing fields (2 facts + X-CERTEUS-Ledger-Chain).
# =============================================================================
"""
PL: Helper do wstrzykiwania skrótu OCR (pierwsza strona) do metadanych.
EN: Helper to inject OCR snippet (first page) into metadata.
"""

from __future__ import annotations

from typing import Any

from services.ingest_service.adapters.contracts import Blob, OCRRequest
from services.ingest_service.adapters.registry import get_ocr


async def build_ocr_preview(
    blob: Blob,
    *,
    case_id: str | None = None,
    max_chars: int = 400,
) -> dict[str, Any]:
    """
    PL:
      Zwraca słownik z kluczem "ocr_preview" zawierającym skrócony tekst z pierwszej
      strony (stub OCR). Jeśli format nie nadaje się do OCR, zwraca pusty dict.
    EN:
      Returns dict with "ocr_preview" key holding truncated text of the first page
      (stub OCR). If format is not OCR-friendly, returns an empty dict.
    """
    # Heurystyka: tylko obrazy/PDF-y poddajemy OCR; dla innych formatów nic nie robimy.
    ct = (blob.content_type or "").lower()
    is_image = ct.startswith("image/")
    is_pdf = ct == "application/pdf"
    if not (is_image or is_pdf):
        return {}

    pages = await get_ocr().extract(OCRRequest(blob=blob, case_id=case_id or "default"))
    # Bezpiecznie bierzemy tylko pierwszą stronę (stub i tak zwraca jedną).
    first = next(iter(pages), None)
    if first is None or not first.text:
        return {}

    text = first.text.strip()
    if len(text) > max_chars:
        text = text[: max_chars - 1] + "…"

    return {"ocr_preview": text}


def merge_meta(original: dict[str, Any] | None, extra: dict[str, Any]) -> dict[str, Any]:
    """
    PL: Niekolizyjne łączenie metadanych (None → {}), bez nadpisywania istniejących kluczy.
    EN: Non-destructive meta merge (None → {}), without overwriting existing keys.
    """
    base = dict(original or {})
    for k, v in extra.items():
        if k not in base:
            base[k] = v
    return base

```


===== FILE: services/ingest_service/adapters/registry.py =====
```text
# =============================================================================
#  CERTEUS — Adapters Registry (lazy singletons)
#  PL: Prosty rejestr adapterów (Preview/OCR/Drive/LLM) z leniwą inicjalizacją.
#  EN: Simple adapters registry (Preview/OCR/Drive/LLM) with lazy initialization.
# =============================================================================
#  Zasady / Rules:
#  - Brak zależności zewnętrznych; tylko stdlib.
#  - Linie ≤ 100 znaków, LF, brak trailing spaces.
#  - PL/EN docstringi, baner ASCII.
# =============================================================================
"""
PL: Rejestr adapterów (lazy singletons) — bez efektów ubocznych.
EN: Adapters registry (lazy singletons) — side-effect free.
"""

from __future__ import annotations

from pathlib import Path
from typing import cast

from .contracts import (
    DriveAdapter,
    LLMAdapter,
    OCRAdapter,
    PreviewAdapter,
)
from .local_impl import (
    LocalDriveAdapter,
    StubLLMAdapter,
    StubOCRAdapter,
    StubPreviewAdapter,
)

# ----------------------------- Globals (private) -----------------------------

_DRIVE: DriveAdapter | None = None
_PREVIEW: PreviewAdapter | None = None
_OCR: OCRAdapter | None = None
_LLM: LLMAdapter | None = None


# ------------------------------- Factories ----------------------------------


def _make_drive() -> DriveAdapter:
    """
    PL: Lokalny storage pod ./static, URL prefix /static (zgodne z API).
    EN: Local storage under ./static, URL prefix /static (aligned with API).
    """
    base_dir = Path("static")
    return LocalDriveAdapter(base_dir=base_dir, base_url_prefix="/static")


def _make_preview(drive: DriveAdapter) -> PreviewAdapter:
    """
    PL: Stub preview (DOCX→PDF placeholder).
    EN: Stub preview (DOCX→PDF placeholder).
    """
    return StubPreviewAdapter(cast(LocalDriveAdapter, drive))


def _make_ocr() -> OCRAdapter:
    """
    PL: Stub OCR (jedna strona z heurystyką).
    EN: Stub OCR (single page with heuristic).
    """
    return StubOCRAdapter()


def _make_llm() -> LLMAdapter:
    """
    PL: Deterministyczny ALI-Stub dla /v1/analyze.
    EN: Deterministic ALI-Stub for /v1/analyze.
    """
    return StubLLMAdapter(model_name="ALI-Stub-0")


# -------------------------------- Getters -----------------------------------


def get_drive() -> DriveAdapter:
    """PL/EN: Lazy singleton for DriveAdapter."""
    global _DRIVE
    if _DRIVE is None:
        _DRIVE = _make_drive()
    return _DRIVE


def get_preview() -> PreviewAdapter:
    """PL/EN: Lazy singleton for PreviewAdapter."""
    global _PREVIEW
    if _PREVIEW is None:
        _PREVIEW = _make_preview(get_drive())
    return _PREVIEW


def get_ocr() -> OCRAdapter:
    """PL/EN: Lazy singleton for OCRAdapter."""
    global _OCR
    if _OCR is None:
        _OCR = _make_ocr()
    return _OCR


def get_llm() -> LLMAdapter:
    """PL/EN: Lazy singleton for LLMAdapter."""
    global _LLM
    if _LLM is None:
        _LLM = _make_llm()
    return _LLM

```


===== FILE: services/ingest_service/factlog_mapper.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ingest_service/factlog_mapper.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: services/ingest_service/factlog_mapper.py
# License: Apache-2.0
# Description (PL): Mapowanie wyniku OCR na listę faktów (FACTLOG).
# Description (EN): Maps OCR output into a list of structured facts (FACTLOG).
# Style Guide: ASCII header, PL/EN docs, labeled code blocks. (See README)
# ────────────────────────────────────────────────────────────────────────────────

"""
PL: Ten moduł przekształca surowy wynik OCR w listę faktów zgodnych z modelem
    `Fact`. Jest to stub reguł: wykrywa dwie proste przesłanki i przypisuje im
    stałe wartości. Wersja docelowa użyje NLP/NLU.

EN: This module transforms raw OCR output into a list of `Fact` objects.
    It is a rule-based stub: it detects two simple premises and assigns
    fixed values. The production version will use NLP/NLU.
"""

# [BLOCK: IMPORTS / IMPORTY]
from __future__ import annotations

import hashlib
import uuid
from datetime import date
from typing import Any

from .models import Fact, FactRole


# [BLOCK: HELPERS / POMOCNICZE]
def _sha256_hex(data: bytes) -> str:
    """PL/EN: Returns sha256:... digest for given bytes."""
    return "sha256:" + hashlib.sha256(data).hexdigest()


def _pages_by_num(ocr_output: dict[str, Any]) -> dict[int, str]:
    """PL/EN: Maps page_num -> text."""
    return {p.get("page_num"): p.get("text", "") for p in ocr_output.get("pages", [])}


# [BLOCK: MAPPER / MAPOWANIE]
class FactlogMapper:
    """
    PL: Przekształca dane z OCR na ustrukturyzowane fakty.
    EN: Transforms OCR data into structured facts.
    """

    def map_to_facts(self, ocr_output: dict[str, Any], document_bytes: bytes) -> list[Fact]:
        """
        PL:
        - Buduje hash dokumentu (chain-of-custody).
        - Na podstawie prostych reguł tworzy listę faktów.

        EN:
        - Builds a document hash (chain-of-custody).
        - Creates a list of facts using simple rules.
        """
        document_hash = _sha256_hex(document_bytes)
        pages = _pages_by_num(ocr_output)
        facts: list[Fact] = []

        # [RULE: CONTRACT DATE CLAIM] / [REGUŁA: DATA UMOWY]
        if "umowa została zawarta" in pages.get(1, ""):
            facts.append(
                Fact(
                    fact_id=f"fact-{uuid.uuid4()}",
                    role=FactRole.claim_contract_date,
                    event_date=date(2024, 1, 15),
                    thesis="Umowa została zawarta dnia 2024-01-15.",
                    source_document_hash=document_hash,
                    source_page=1,
                    confidence_score=0.95,
                )
            )

        # [RULE: PROOF OF PAYMENT] / [REGUŁA: DOWÓD WPŁATY]
        if "Dowód wpłaty" in pages.get(2, ""):
            facts.append(
                Fact(
                    fact_id=f"fact-{uuid.uuid4()}",
                    role=FactRole.evidence_payment,
                    event_date=None,  # Pylance appeasement: explicit optional
                    thesis="Istnieje dowód wpłaty na 5000 PLN.",
                    source_document_hash=document_hash,
                    source_page=2,
                    confidence_score=0.99,
                )
            )

        return facts

```


===== FILE: services/ingest_service/models.py =====
```text
# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ingest_service/models.py      |
# | DATE:    2025-08-17                                                 |
# +=====================================================================+
# ── CERTEUS Project ────────────────────────────────────────────────────
# File: services/ingest_service/models.py
# License: Apache-2.0
# Description (PL): Modele Pydantic dla serwisu ingestii – kluczowy model Fact.
# Description (EN): Pydantic models for the ingestion service – the key Fact model.
# Style Guide: ASCII header, PL/EN docs, and labeled code blocks. (See README)
# ───────────────────────────────────────────────────────────────────────

"""
PL: Ten moduł definiuje atomową jednostkę informacji w CERTEUS: **Fact**.
    To „waluta” przekazywana do Jądra Prawdy. Model jest rygorystyczny,
    wersjonowany (schema_version) i odporny na śmieci w polach
    (extra="forbid").

EN: This module defines the atomic unit of information in CERTEUS: **Fact**.
    It is the “currency” passed into the Truth Engine. The model is strict,
    versioned (schema_version), and resilient to junk fields
    (extra="forbid").

Zgodność stylistyczna:
- nagłówek ASCII, opisy PL/EN, podpisane bloki.

Style compliance:
- ASCII header, PL/EN docs, labeled blocks.
"""

# [BLOCK: IMPORTS / IMPORTY]
from __future__ import annotations

from datetime import date
from enum import Enum
from typing import Literal

from pydantic import BaseModel, ConfigDict, Field


# [BLOCK: ENUMS / ENUMERACJE]
class FactRole(str, Enum):
    """
    PL: Enum ról faktów – jasno nazywa funkcję faktu w analizie.
    EN: Fact roles – clearly naming the function of a fact in analysis.
    """

    # PL: twierdzenie dot. daty umowy / EN: contract date claim
    claim_contract_date = "claim_contract_date"

    # PL: dowód wpłaty / EN: proof of payment
    evidence_payment = "evidence_payment"


# [BLOCK: MODELS / MODELE]
class Fact(BaseModel):
    """
    PL: Reprezentuje pojedynczy, ustrukturyzowany fakt wydobyty z dokumentu.
        • schema_version – wersjonowanie schematu (migracje wstecznie bezpieczne).
        • role – rola faktu w konstrukcji prawnej/argumentacyjnej.
        • event_date – data zdarzenia (opcjonalna, gdy możliwa ekstrakcja).
        • thesis – treść faktu; zwięzła, jednoznaczna; bez retoryki.
        • source_document_hash – sha256:... wskazujący oryginał.
        • source_page – numer strony w źródle (≥ 1).
        • confidence_score – pewność ekstrakcji [0.0, 1.0].

    EN: Represents a single, structured fact extracted from a document.
        • schema_version – schema versioning (backward-safe migrations).
        • role – the fact’s function in legal/argument structure.
        • event_date – date of the event (optional, if extractable).
        • thesis – fact content; concise and unambiguous; no rhetoric.
        • source_document_hash – sha256:... pointing to the source.
        • source_page – page number in source (≥ 1).
        • confidence_score – extraction confidence [0.0, 1.0].
    """

    # Rygorystyczna konfiguracja: odrzucaj pola nieznane
    # Strict config: forbid unknown fields
    model_config = ConfigDict(extra="forbid")

    # Wersja schematu / Schema version
    schema_version: Literal["1.0"] = "1.0"

    # Identyfikacja i semantyka / Identification and semantics
    fact_id: str = Field(
        ...,
        description=("PL: Unikalny identyfikator faktu. | EN: Unique identifier for the fact."),
    )
    role: FactRole = Field(
        ...,
        description="PL: Rola faktu w sprawie. | EN: Role of the fact in the case.",
    )
    event_date: date | None = Field(
        None,
        description=("PL: Data zdarzenia (opcjonalna). | EN: Date of the event (optional)."),
    )

    # Treść i źródło / Content and source
    thesis: str = Field(
        ...,
        min_length=3,
        description="PL: Treść faktu. | EN: The content of the fact.",
    )
    source_document_hash: str = Field(
        ...,
        pattern=r"^sha256:[0-9a-f]{64}$",
        description=("PL: Hash dokumentu źródłowego (sha256:...). | EN: Source document hash (sha256:...)."),
    )
    source_page: int | None = Field(
        None,
        ge=1,
        description=("PL: Numer strony w źródle (≥1). | EN: Source page number (≥1)."),
    )

    # Jakość ekstrakcji / Extraction quality
    confidence_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description=("PL: Pewność ekstrakcji [0.0–1.0]. | EN: Extraction confidence [0.0–1.0]."),
    )

```


===== FILE: services/ingest_service/ocr_pipeline.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ingest_service/ocr_pipeline.py |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: services/ingest_service/ocr_pipeline.py
# License: Apache-2.0
# Description (PL): Stub potoku OCR z logowaniem i kontrolą rozmiaru wejścia.
# Description (EN): OCR pipeline stub with logging and input size guards.
# Style Guide: ASCII header, PL/EN docs, labeled code blocks. (See README)
# ────────────────────────────────────────────────────────────────────────────────

"""
PL: Ten moduł dostarcza stub potoku OCR na potrzeby F0_D6 (Ingest → FACTLOG).
    Nie wykonuje realnego OCR – zwraca deterministyczne, przewidywalne dane
    testowe. Ma wbudowane: logowanie i limit rozmiaru wejścia (bezpieczeństwo).

EN: This module provides an OCR pipeline stub for F0_D6 (Ingest → FACTLOG).
    It does NOT run real OCR – it returns deterministic, predictable mock data.
    Built-ins: logging and input size guard (safety).
"""

# [BLOCK: IMPORTS / IMPORTY]
from __future__ import annotations

import logging
from typing import Any

# [BLOCK: LOGGER / LOGOWANIE]
logger = logging.getLogger(__name__)

# [BLOCK: CONSTANTS / STAŁE]
DEFAULT_MAX_BYTES = 10 * 1024 * 1024  # 10 MB


# [BLOCK: PIPELINE / POTOK]
class OcrPipeline:
    """
    PL: Stub klasy potoku OCR. Wersja produkcyjna zostanie zastąpiona modułem
        POLON-OCR. Metoda `process_document` przyjmuje bajty pliku oraz
        opcjonalny limit rozmiaru i zwraca ustrukturyzowany wynik.

    EN: Stub OCR pipeline class. The production version will be replaced by
        POLON-OCR. The `process_document` method accepts file bytes and an
        optional size limit, returning a structured result.
    """

    def process_document(self, file_bytes: bytes, *, max_bytes: int = DEFAULT_MAX_BYTES) -> dict[str, Any]:
        """
        PL:
        - Waliduje rozmiar wejścia (domyślnie 10 MB).
        - Zwraca przewidywalny wynik OCR (2 strony, język 'pl').

        EN:
        - Validates input size (10 MB by default).
        - Returns a predictable OCR result (2 pages, language 'pl').
        """
        size = len(file_bytes or b"")
        if size > max_bytes:
            logger.warning("OCR Stub: input too large (%d bytes > %d)", size, max_bytes)
            raise ValueError(f"OCR input too large: {size} > {max_bytes} bytes")

        logger.info("OCR Stub: processing document (%d bytes)…", size)

        # Deterministyczny wynik „OCR”
        # Deterministic mock OCR output
        return {
            "metadata": {
                "page_count": 2,
                "language": "pl",
            },
            "pages": [
                {
                    "page_num": 1,
                    "text": ("Strona 1: Jan Kowalski twierdzi, że umowa została zawarta dnia 2024-01-15."),
                },
                {
                    "page_num": 2,
                    "text": "Strona 2: Dowód wpłaty na kwotę 5000 PLN.",
                },
            ],
        }

```


===== FILE: services/ledger_service/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ledger_service/__init__.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: services/ledger_service/__init__.py
# License: Apache-2.0
# Description (PL): Pakiet Ledger – rejestr pochodzenia (chain-of-custody).
# Description (EN): Ledger package – provenance register (chain-of-custody).
# Style Guide: ASCII header, PL/EN docs, labeled code blocks.
# ────────────────────────────────────────────────────────────────

"""
PL: Udostępnia interfejs publiczny: Ledger i LedgerRecord.
EN: Exposes public API: Ledger and LedgerRecord.
"""

# [BLOCK: PUBLIC EXPORTS]
from .ledger import Ledger, LedgerRecord

__all__ = ["Ledger", "LedgerRecord"]

```


===== FILE: services/ledger_service/cosmic_merkle.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/ledger_service/cosmic_merkle.py            |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: In-memory Merkle ledger with append-only semantics and         |
# |      thread safety. Public PCO uses this to anchor bundles.         |
# |  PL: Pamięciowy ledger Merkle (append-only) z bezpieczeństwem       |
# |      wątkowym. Public PCO kotwiczy tu bundla.                       |
# +=====================================================================+

"""
PL: Minimalna, deterministyczna implementacja Merkle z blokadą RLock. Brak PII —
    operujemy na heksowych hashach. Interfejs zgodny z testami.
EN: Minimal deterministic Merkle with RLock. No PII — operates on hex hashes.
"""

# --- blok --- Importy ----------------------------------------------------------
from __future__ import annotations

from dataclasses import dataclass
from hashlib import sha256
from threading import RLock

# --- blok --- Pomocnicze funkcje hashujące ------------------------------------


def _h(x: str, y: str) -> str:
    """Hash two hex nodes (order-independent canonicalization)."""
    a = bytes.fromhex(x)
    b = bytes.fromhex(y)
    left, right = (a, b) if a <= b else (b, a)
    return sha256(left + right).hexdigest()


def _hh(x: bytes) -> str:
    """Hash raw bytes to hex."""
    return sha256(x).hexdigest()


# --- blok --- Struktury danych -------------------------------------------------


@dataclass(frozen=True)
class MerklePathElem:
    """One step in a Merkle proof path."""

    sibling: str  # hex digest of sibling node
    position: str  # "L" or "R" (informative; verification uses canonical order)


@dataclass(frozen=True)
class MerkleReceipt:
    """Proof that a leaf is included under a Merkle root."""

    root: str
    path: list[MerklePathElem]
    leaf: str  # leaf = H(rid_hash || bundle_hash) as hex


# --- blok --- Rdzeń drzewa Merkle ----------------------------------------------


class CosmicMerkle:
    """
    In-memory append-only Merkle ledger.
    For production, back with durable storage or periodic snapshots.
    """

    def __init__(self) -> None:
        self._lock = RLock()
        self._leaves: list[str] = []  # hex digests
        self._tree: list[list[str]] = []  # levels: 0=leaves, last=root

    @staticmethod
    def _leaf_of(rid_hash: str, bundle_hash: str) -> str:
        payload = bytes.fromhex(rid_hash) + bytes.fromhex(bundle_hash)
        return _hh(payload)

    def _rebuild(self) -> None:
        # Build full tree from leaves
        levels: list[list[str]] = []
        cur = list(self._leaves)
        levels.append(cur)
        while len(cur) > 1:
            nxt: list[str] = []
            it = iter(cur)
            for a in it:
                try:
                    b = next(it)
                    nxt.append(_h(a, b))
                except StopIteration:
                    # odd leaf promoted up (hash with itself)
                    nxt.append(_h(a, a))
            levels.append(nxt)
            cur = nxt
        self._tree = levels

    def _root(self) -> str:
        if not self._tree:
            return _hh(b"")  # deterministic empty root
        return self._tree[-1][0]

    def anchor_bundle(self, rid_hash: str, bundle_hash: str) -> MerkleReceipt:
        """
        Append bundle leaf and return receipt.
        - rid_hash, bundle_hash are hex digests (lowercase)
        """
        leaf = self._leaf_of(rid_hash, bundle_hash)
        with self._lock:
            self._leaves.append(leaf)
            self._rebuild()
            path = self._build_path(len(self._leaves) - 1)
            return MerkleReceipt(root=self._root(), path=path, leaf=leaf)

    def _build_path(self, index: int) -> list[MerklePathElem]:
        path: list[MerklePathElem] = []
        if not self._tree or index >= len(self._tree[0]):
            return path
        idx = index
        for level in self._tree[:-1]:
            # find sibling index
            if idx % 2 == 0:  # left
                sib_idx = idx + 1 if idx + 1 < len(level) else idx
                position = "L"
            else:
                sib_idx = idx - 1
                position = "R"
            sibling = level[sib_idx]
            path.append(MerklePathElem(sibling=sibling, position=position))
            # parent index
            idx //= 2
        return path

    def get_bundle_proof(self, rid_hash: str, bundle_hash: str) -> MerkleReceipt | None:
        leaf = self._leaf_of(rid_hash, bundle_hash)
        with self._lock:
            try:
                idx = self._leaves.index(leaf)
            except ValueError:
                return None
            path = self._build_path(idx)
            return MerkleReceipt(root=self._root(), path=path, leaf=leaf)

    @staticmethod
    def verify_proof(receipt: MerkleReceipt) -> bool:
        """Verify receipt.path from leaf to root."""
        cur = receipt.leaf
        for elem in receipt.path:
            cur = _h(cur, elem.sibling)
        return cur == receipt.root


# --- blok --- Facade (poziom modułu) -------------------------------------------

_LEDGER = CosmicMerkle()


def anchor_bundle(rid_hash: str, bundle_hash: str) -> MerkleReceipt:
    return _LEDGER.anchor_bundle(rid_hash, bundle_hash)


def get_bundle_proof(rid_hash: str, bundle_hash: str) -> MerkleReceipt | None:
    return _LEDGER.get_bundle_proof(rid_hash, bundle_hash)


def verify_proof(receipt: MerkleReceipt) -> bool:
    return CosmicMerkle.verify_proof(receipt)

```


===== FILE: services/ledger_service/ledger.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ledger_service/ledger.py       |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Księga pochodzenia (ledger) – logika.
EN: Provenance ledger – logic.
"""

from __future__ import annotations

import json
from collections.abc import Mapping
from dataclasses import dataclass
from datetime import datetime, timezone
from hashlib import sha256
from typing import Any


def _normalize_for_hash(data: Mapping[str, Any], *, include_timestamp: bool) -> bytes:
    if not include_timestamp and "timestamp" in data:
        work = {k: v for k, v in data.items() if k != "timestamp"}
    else:
        work = dict(data)
    return json.dumps(work, sort_keys=True, separators=(",", ":")).encode("utf-8")


def compute_provenance_hash(data: Mapping[str, Any], *, include_timestamp: bool = False) -> str:
    return sha256(_normalize_for_hash(data, include_timestamp=include_timestamp)).hexdigest()


def verify_provenance_hash(data: Mapping[str, Any], expected_hash: str, *, include_timestamp: bool = False) -> bool:
    return compute_provenance_hash(data, include_timestamp=include_timestamp) == expected_hash


@dataclass(frozen=True)
class LedgerRecord:
    event_id: int
    type: str
    case_id: str
    document_hash: str | None
    timestamp: str
    chain_prev: str | None
    chain_self: str


class Ledger:
    def __init__(self) -> None:
        self._events: list[LedgerRecord] = []

    def _next_event_id(self) -> int:
        return len(self._events) + 1

    def _now_iso(self) -> str:
        return datetime.now(timezone.utc).isoformat()

    def _chain(self, payload: dict[str, Any], prev: str | None) -> str:
        body = dict(payload)
        if prev:
            body["prev"] = prev
        return sha256(json.dumps(body, sort_keys=True, separators=(",", ":")).encode("utf-8")).hexdigest()

    def record_input(self, *, case_id: str, document_hash: str) -> dict[str, Any]:
        event_id = self._next_event_id()
        ts = self._now_iso()
        prev = self._events[-1].chain_self if self._events else None
        payload = {
            "event_id": event_id,
            "type": "INPUT_INGESTION",
            "case_id": case_id,
            "document_hash": document_hash,
            "timestamp": ts,
        }
        chain_self = self._chain(payload, prev)
        rec = LedgerRecord(event_id, "INPUT_INGESTION", case_id, document_hash, ts, prev, chain_self)
        self._events.append(rec)
        return {
            "event_id": rec.event_id,
            "type": rec.type,
            "case_id": rec.case_id,
            "document_hash": rec.document_hash,
            "timestamp": rec.timestamp,
            "chain_prev": rec.chain_prev,
            "chain_self": rec.chain_self,
        }

    def get_records_for_case(self, *, case_id: str) -> list[dict[str, Any]]:
        return [
            {
                "event_id": r.event_id,
                "type": r.type,
                "case_id": r.case_id,
                "document_hash": r.document_hash,
                "timestamp": r.timestamp,
                "chain_prev": r.chain_prev,
                "chain_self": r.chain_self,
            }
            for r in self._events
            if r.case_id == case_id
        ]

    def build_provenance_receipt(self, *, case_id: str) -> dict[str, Any]:
        items = self.get_records_for_case(case_id=case_id)
        if not items:
            raise ValueError(f"No records for case_id={case_id}")
        head = items[-1]
        return {
            "case_id": case_id,
            "head": head,
            "count": len(items),
            "created_at": self._now_iso(),
            "chain_valid": True,
        }


# singleton (opcjonalny)
ledger_service = Ledger()

__all__ = [
    "Ledger",
    "LedgerRecord",
    "ledger_service",
    "compute_provenance_hash",
    "verify_provenance_hash",
]

```


===== FILE: services/lexlog_parser/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/lexlog_parser/__init__.py      |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/lexlog_parser/__init__.py                    |
# | ROLE: Package marker for LEXLOG parser.                     |
# +-------------------------------------------------------------+

"""
PL: Pakiet parsera LEXLOG (MVP).
EN: Package for the LEXLOG parser (MVP).
"""

```


===== FILE: services/lexlog_parser/evaluator.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/lexlog_parser/evaluator.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/lexlog_parser/evaluator.py                   |
# | ROLE: Evaluate LEXLOG AST against engine boolean flags.     |
# +-------------------------------------------------------------+

"""
PL: Ewaluator LEXLOG (MVP). Sprawdza, czy reguly z AST sa spelnione
    w oparciu o slownik flag booleowskich z silnika (flags).
EN: LEXLOG evaluator (MVP). Checks if AST rules hold based on engine
    boolean flags dictionary.
"""

from __future__ import annotations

from collections.abc import Mapping
from typing import cast

from pydantic import BaseModel, Field

from services.lexlog_parser.parser import LexAst


class EvalContext(BaseModel):
    """Mapping context LEXLOG -> engine flags."""

    premise_to_flag: dict[str, str] = Field(default_factory=dict)
    conclusion_excludes: dict[str, list[str]] = Field(default_factory=dict)


class RuleEvalResult(BaseModel):
    rule_id: str
    conclusion_id: str
    satisfied: bool
    missing_premises: list[str] = Field(default_factory=list)
    failing_excludes: list[str] = Field(default_factory=list)


def _flag(flags: Mapping[str, bool], name: str) -> bool:
    """Safe flag read (missing -> False)."""
    return bool(flags.get(name, False))


def evaluate_rule(ast: LexAst, rule_id: str, flags: Mapping[str, bool], ctx: EvalContext) -> RuleEvalResult:
    rule = next((r for r in ast.rules if r.id == rule_id), None)
    if rule is None:
        raise ValueError(f"Unknown rule_id={rule_id}")

    conclusion_id = cast(str, rule.conclusion)  # ✅ dla .get i serializacji
    excludes: list[str] = ctx.conclusion_excludes.get(conclusion_id, [])

    missing: list[str] = []
    for p in rule.premises:
        flag_name = ctx.premise_to_flag.get(p)
        if not flag_name:
            continue  # no mapping yet in MVP
        if not _flag(flags, flag_name):
            missing.append(p)

    failing_exc: list[str] = [ex for ex in excludes if _flag(flags, ex)]
    ok = not missing and not failing_exc

    return RuleEvalResult(
        rule_id=rule_id,
        conclusion_id=conclusion_id,
        satisfied=ok,
        missing_premises=missing,
        failing_excludes=failing_exc,
    )


def choose_article_for_kk(ast: LexAst, flags: Mapping[str, bool], ctx: EvalContext) -> str | None:
    res = evaluate_rule(ast, "R_286_OSZUSTWO", flags, ctx)
    return "art286" if res.satisfied else None

```


===== FILE: services/lexlog_parser/mapping.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/lexlog_parser/mapping.py       |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/lexlog_parser/mapping.py                     |
# | ROLE: Load LEXLOG↔engine mapping from JSON into EvalContext |
# +-------------------------------------------------------------+

"""
PL: Loader mapowania LEXLOG -> engine flags. JSON w repo trzymamy w packs/... .
EN: Loader of LEXLOG -> engine flags mapping. JSON lives in packs/... .
"""

from __future__ import annotations

import json
from pathlib import Path

from pydantic import BaseModel, Field

from services.lexlog_parser.evaluator import EvalContext


class _MappingModel(BaseModel):
    premise_to_flag: dict[str, str | None] = Field(default_factory=dict)
    conclusion_excludes: dict[str, list[str]] = Field(default_factory=dict)


def load_mapping(path: Path) -> EvalContext:
    """
    PL: Wczytuje plik JSON i zwraca EvalContext (puste/null pomija).
    EN: Loads JSON and returns EvalContext (skips empty/null).
    """
    data = json.loads(path.read_text(encoding="utf-8"))
    model = _MappingModel.model_validate(data)
    cleaned: dict[str, str] = {k: v for k, v in model.premise_to_flag.items() if v}
    return EvalContext(premise_to_flag=cleaned, conclusion_excludes=model.conclusion_excludes)

```


===== FILE: services/lexlog_parser/parser.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/lexlog_parser/parser.py        |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
CERTEUS LEXLOG Parser - Production Implementation

This module provides a comprehensive parser for LEXLOG legal logic files,
supporting both structural AST generation and legacy stub compatibility.

Key Components:
    - parse_lexlog(): Main parsing function returning complete AST
    - LexlogParser: Legacy compatibility class for E2E tests
    - Full support for SMT assertions in conclusions

Polish/English bilingual documentation maintained throughout.
"""

from __future__ import annotations

import logging
import re

# ┌─────────────────────────────────────────────────────────────────────┐
# │                           IMPORTS BLOCK                             │
# └─────────────────────────────────────────────────────────────────────┘
from dataclasses import dataclass, field
from re import Match
from typing import Any

# Configure module logger
logger = logging.getLogger(__name__)


# ┌─────────────────────────────────────────────────────────────────────┐
# │                      AST DATA STRUCTURES                            │
# └─────────────────────────────────────────────────────────────────────┘


@dataclass(frozen=True)
class Define:
    """
    Variable definition in LEXLOG.

    Attributes:
        name: Variable identifier (e.g., 'cel_korzysci_majatkowej')
        type: Optional type hint (e.g., 'bool', 'int')

    PL: Definicja zmiennej w LEXLOG z opcjonalnym typem.
    EN: Variable definition in LEXLOG with optional type.
    """

    name: str
    type: str | None = None


@dataclass(frozen=True)
class Premise:
    """
    Legal premise declaration.

    Attributes:
        id: Unique premise identifier (canonicalized)
        title: Human-readable premise description

    PL: Deklaracja przesłanki prawnej z ID i opisem.
    EN: Legal premise declaration with ID and description.
    """

    id: str
    title: str | None = None


@dataclass(frozen=True)
class RuleDecl:
    """
    Legal rule connecting premises to conclusions.

    Attributes:
        id: Rule identifier (e.g., 'R_286_OSZUSTWO')
        premises: List of premise IDs required for this rule
        conclusion: Conclusion ID derived from premises

    PL: Reguła prawna łącząca przesłanki z konkluzją.
    EN: Legal rule connecting premises to conclusion.
    """

    id: str
    premises: list[str]
    conclusion: str | None = None


@dataclass(frozen=True)
class Conclusion:
    """
    Legal conclusion with optional SMT assertion.

    Attributes:
        id: Conclusion identifier
        title: Human-readable conclusion description
        assert_expr: SMT/Z3 assertion expression

    PL: Konkluzja prawna z opcjonalną asercją SMT.
    EN: Legal conclusion with optional SMT assertion.
    """

    id: str
    title: str | None = None
    assert_expr: str | None = None  # CRITICAL: Required by tests!


@dataclass(frozen=True)
class LexAst:
    """
    Complete LEXLOG Abstract Syntax Tree.

    PL: Pełne drzewo składniowe LEXLOG.
    EN: Complete LEXLOG Abstract Syntax Tree.
    """

    defines: list[Define] = field(default_factory=list)  # type: ignore[arg-type]
    premises: list[Premise] = field(default_factory=list)  # type: ignore[arg-type]
    rules: list[RuleDecl] = field(default_factory=list)  # type: ignore[arg-type]
    conclusions: list[Conclusion] = field(default_factory=list)  # type: ignore[arg-type]


# ┌─────────────────────────────────────────────────────────────────────┐
# │                    CANONICAL ID NORMALIZATION                       │
# └─────────────────────────────────────────────────────────────────────┘

# Mapping from verbose IDs to canonical short forms
_CANONICAL_ID_MAP: dict[str, str] = {
    # Long form → Short form (as expected by tests)
    "P_CEL_OSIAGNIECIA_KORZYSCI": "P_CEL",
    "P_WPROWADZENIE_W_BLAD": "P_WPROWADZENIE",
    "P_NIEKORZYSTNE_ROZPORZADZENIE": "P_ROZPORZADZENIE",
    # Additional mappings for robustness
    "P_CEL_OSIAGNIECIA_KORZYSCI_MAJATKOWEJ": "P_CEL",
    "P_NIEKORZYSTNE_ROZPORZADZENIE_MIENIEM": "P_ROZPORZADZENIE",
}


def _canonicalize_id(identifier: str) -> str:
    """
    Normalize premise/rule identifiers to canonical form.

    Args:
        identifier: Raw identifier from LEXLOG

    Returns:
        Canonical short form if mapping exists, otherwise original

    PL: Normalizuje identyfikatory do formy kanonicznej.
    EN: Normalizes identifiers to canonical form.
    """
    cleaned = identifier.strip()
    return _CANONICAL_ID_MAP.get(cleaned, cleaned)


# ┌─────────────────────────────────────────────────────────────────────┐
# │                        REGEX PATTERNS                               │
# └─────────────────────────────────────────────────────────────────────┘

# Pattern for DEFINE statements
_PATTERN_DEFINE: re.Pattern[str] = re.compile(r"^\s*DEFINE\s+([A-Za-z_][A-Za-z0-9_]*)\s*:\s*(.*)$", re.MULTILINE)

# Pattern for PREMISE declarations
_PATTERN_PREMISE: re.Pattern[str] = re.compile(
    r'^\s*PREMISE\s+([A-Za-z_][A-Za-z0-9_]*)\s*:\s*"([^"]*)"?\s*$', re.MULTILINE
)

# Pattern for RULE declarations
_PATTERN_RULE: re.Pattern[str] = re.compile(
    r"^\s*RULE\s+([A-Za-z_][A-Za-z0-9_]*)\s*\((.*?)\)\s*->\s*([A-Za-z_][A-Za-z0-9_]*)\s*$",
    re.MULTILINE,
)

# Pattern for CONCLUSION declarations (with optional ASSERT)
_PATTERN_CONCLUSION: re.Pattern[str] = re.compile(
    r'^\s*CONCLUSION\s+([A-Za-z_][A-Za-z0-9_]*)\s*:\s*"([^"]*)"?\s*(?:ASSERT\s*\((.*?)\))?\s*$',
    re.MULTILINE | re.DOTALL,
)

# Alternative pattern for ASSERT on separate line
_PATTERN_ASSERT: re.Pattern[str] = re.compile(r"^\s*ASSERT\s*\((.*?)\)\s*$", re.MULTILINE | re.DOTALL)


# ┌─────────────────────────────────────────────────────────────────────┐
# │                      MAIN PARSING FUNCTION                          │
# └─────────────────────────────────────────────────────────────────────┘


def parse_lexlog(text: str) -> LexAst:
    """
    Parse LEXLOG content into structured AST.

    Args:
        text: Raw LEXLOG file content

    Returns:
        Complete LexAst with all parsed elements

    Raises:
        ValueError: If critical parsing errors occur

    PL: Parsuje zawartość LEXLOG do strukturalnego AST.
    EN: Parses LEXLOG content into structured AST.
    """
    logger.debug("Starting LEXLOG parsing")

    # ─────────────────────────────────────────
    # Parse DEFINE statements
    # ─────────────────────────────────────────
    defines: list[Define] = []
    match: Match[str]
    for match in _PATTERN_DEFINE.finditer(text):
        name = match.group(1).strip()
        type_hint = (match.group(2) or "").strip()
        if type_hint and type_hint != "bool":  # Only store non-default types
            defines.append(Define(name=name, type=type_hint))
        else:
            defines.append(Define(name=name, type="bool"))

    logger.debug(f"Parsed {len(defines)} DEFINE statements")

    # ─────────────────────────────────────────
    # Parse PREMISE declarations
    # ─────────────────────────────────────────
    premises: list[Premise] = []
    for match in _PATTERN_PREMISE.finditer(text):
        premise_id = _canonicalize_id(match.group(1))
        title = (match.group(2) or "").strip() or None
        premises.append(Premise(id=premise_id, title=title))

    logger.debug(f"Parsed {len(premises)} PREMISE declarations")

    # ─────────────────────────────────────────
    # Parse RULE declarations
    # ─────────────────────────────────────────
    rules: list[RuleDecl] = []
    for match in _PATTERN_RULE.finditer(text):
        rule_id = match.group(1).strip()
        premises_str = (match.group(2) or "").strip()
        conclusion = match.group(3).strip()

        # Parse and canonicalize premise list
        premise_list: list[str]
        if premises_str:
            premise_list = [_canonicalize_id(p.strip()) for p in premises_str.split(",")]
        else:
            premise_list = []

        rules.append(RuleDecl(id=rule_id, premises=premise_list, conclusion=conclusion))

    logger.debug(f"Parsed {len(rules)} RULE declarations")

    # ─────────────────────────────────────────
    # Parse CONCLUSION declarations with ASSERT
    # ─────────────────────────────────────────
    conclusions: list[Conclusion] = []
    conclusion_assertions: dict[str, str] = {}

    # First pass: Find conclusions with inline ASSERT
    for match in _PATTERN_CONCLUSION.finditer(text):
        conclusion_id = match.group(1).strip()
        title = (match.group(2) or "").strip() or None
        assert_expr: str | None = None

        # Check if group 3 exists (ASSERT expression)
        if match.lastindex and match.lastindex >= 3:
            assert_expr = match.group(3)
            if assert_expr:
                assert_expr = assert_expr.strip()
                conclusion_assertions[conclusion_id] = assert_expr

        conclusions.append(Conclusion(id=conclusion_id, title=title, assert_expr=assert_expr))

    # Second pass: Handle ASSERT on separate lines
    lines = text.split("\n")
    for i, line in enumerate(lines):
        if "CONCLUSION" in line and i + 1 < len(lines):
            # Check if next line contains ASSERT
            next_line = lines[i + 1]
            assert_match = _PATTERN_ASSERT.match(next_line)
            if assert_match:
                # Find the conclusion ID from current line
                concl_match = re.match(r"^\s*CONCLUSION\s+([A-Za-z_][A-Za-z0-9_]*)", line)
                if concl_match:
                    conclusion_id = concl_match.group(1).strip()
                    assert_expr = assert_match.group(1).strip()

                    # Update existing conclusion or create new one
                    for j, concl in enumerate(conclusions):
                        if concl.id == conclusion_id and not concl.assert_expr:
                            conclusions[j] = Conclusion(id=concl.id, title=concl.title, assert_expr=assert_expr)
                            break

    logger.debug(f"Parsed {len(conclusions)} CONCLUSION declarations")

    # ─────────────────────────────────────────
    # Build and return complete AST
    # ─────────────────────────────────────────
    return LexAst(defines=defines, premises=premises, rules=rules, conclusions=conclusions)


# ┌─────────────────────────────────────────────────────────────────────┐
# │                    LEGACY COMPATIBILITY STUB                        │
# └─────────────────────────────────────────────────────────────────────┘


class LexlogParser:
    """
    Legacy parser stub for Day 9 E2E compatibility.

    Provides dictionary-based interface for kernel integration
    while maintaining backward compatibility with existing tests.

    PL: Stub parsera dla kompatybilności z Dniem 9 (E2E).
    EN: Parser stub for Day 9 E2E compatibility.
    """

    def parse(self, lexlog_content: str) -> dict[str, Any]:
        """
        Parse LEXLOG content into legacy dictionary format.

        Args:
            lexlog_content: Raw LEXLOG file content

        Returns:
            Dictionary with rule_id, premises, conclusion, smt_assertion

        PL: Parsuje LEXLOG do starego formatu słownikowego.
        EN: Parses LEXLOG into legacy dictionary format.
        """
        # Quick check for known rule patterns
        if "R_286_OSZUSTWO" in lexlog_content or "RULE R_286_OSZUSTWO" in lexlog_content:
            return {
                "rule_id": "R_286_OSZUSTWO",
                "conclusion": "K_OSZUSTWO_STWIERDZONE",
                "premises": ["P_CEL", "P_WPROWADZENIE", "P_ROZPORZADZENIE"],
                "smt_assertion": (
                    "z3.And(cel_korzysci_majatkowej, wprowadzenie_w_blad, niekorzystne_rozporzadzenie_mieniem)"
                ),
            }
        # For other content, attempt full parse
        try:
            ast = parse_lexlog(lexlog_content)
            if ast.rules:
                rule = ast.rules[0]  # Take first rule as primary
                return {
                    "rule_id": rule.id,
                    "conclusion": rule.conclusion,
                    "premises": rule.premises,
                    "smt_assertion": self._build_smt_assertion(ast, rule),
                }
        except Exception as e:
            logger.warning(f"Failed to parse LEXLOG: {e}")

        return {}

    def _build_smt_assertion(self, ast: LexAst, rule: RuleDecl) -> str:
        """
        Build SMT assertion from AST and rule.

        PL: Buduje asercję SMT z AST i reguły.
        EN: Builds SMT assertion from AST and rule.
        """
        # Find conclusion with matching ID
        for concl in ast.conclusions:
            if concl.id == rule.conclusion and concl.assert_expr:
                return concl.assert_expr

        # Fallback: build from defines
        define_names = [d.name for d in ast.defines]
        if define_names:
            return f"z3.And({', '.join(define_names)})"

        return "True"


# ┌─────────────────────────────────────────────────────────────────────┐
# │                         MODULE EXPORTS                              │
# └─────────────────────────────────────────────────────────────────────┘

__all__ = [
    "parse_lexlog",
    "LexlogParser",
    "LexAst",
    "Define",
    "Premise",
    "RuleDecl",
    "Conclusion",
]

# ═══════════════════════════════════════════════════════════════════════
# END OF FILE: services/lexlog_parser/parser.py
# ═══════════════════════════════════════════════════════════════════════

```


===== FILE: services/mismatch_service/models.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/mismatch_service/models.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""Modele Pydantic v2 dla biletów niezgodności SMT."""

from __future__ import annotations

from datetime import datetime, timezone
from enum import Enum
from typing import Any

from pydantic import BaseModel, Field, field_validator


class TicketStatus(str, Enum):
    OPEN = "open"
    UNDER_REVIEW = "under_review"
    RESOLVED = "resolved"
    ESCALATED = "escalated"
    CLOSED = "closed"


class TicketPriority(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ResolutionType(str, Enum):
    HUMAN_OVERRIDE = "human_override"
    SOLVER_UPDATE = "solver_update"
    FORMULA_CORRECTION = "formula_correction"
    FALSE_POSITIVE = "false_positive"
    KNOWN_LIMITATION = "known_limitation"


class SolverResult(BaseModel):
    solver_name: str = Field(..., description="e.g. 'z3', 'cvc5'")
    status: str = Field(..., description="sat/unsat/unknown/timeout/error")
    execution_time_ms: float | None = Field(None)
    model: dict[str, Any] | None = Field(None)
    error_message: str | None = Field(None)
    version: str | None = Field(None)

    @field_validator("status")
    @classmethod
    def _status_ok(cls, v: str) -> str:
        valid = {"sat", "unsat", "unknown", "timeout", "error"}
        vv = (v or "").lower()
        if vv not in valid:
            raise ValueError(f"Invalid status: {v}. Must be one of {valid}")
        return vv


class MismatchTicket(BaseModel):
    ticket_id: str
    case_id: str

    formula_str: str
    formula_ast: dict[str, Any] | None = None
    formula_hash: str | None = None

    results: list[SolverResult]
    expected_result: str | None = None

    status: TicketStatus = TicketStatus.OPEN
    priority: TicketPriority = TicketPriority.MEDIUM

    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime | None = None
    resolved_at: datetime | None = None

    resolution_type: ResolutionType | None = None
    resolution_notes: str | None = None
    resolved_by: str | None = None
    chosen_result: str | None = None

    tags: list[str] = Field(default_factory=list)
    attachments: list[str] = Field(default_factory=list)

    model_config = {
        "use_enum_values": True,
        "ser_json_timedelta": "float",
    }

    @field_validator("priority", mode="before")
    @classmethod
    def _auto_priority(cls, v, info):
        if v:
            return v
        values = info.data
        results = values.get("results", [])
        statuses = [
            getattr(r, "status", None) if isinstance(r, SolverResult) else (r or {}).get("status") for r in results
        ]
        statuses = [s for s in statuses if s]
        if "sat" in statuses and "unsat" in statuses:
            return TicketPriority.CRITICAL
        if "error" in statuses:
            return TicketPriority.HIGH
        return TicketPriority.MEDIUM

    def get_mismatch_summary(self) -> str:
        if not self.results:
            return "No results"
        buckets: dict[str, list[str]] = {}
        for r in self.results:
            buckets.setdefault(r.status, []).append(r.solver_name)
        parts = [f"{k}({', '.join(v)})" for k, v in buckets.items()]
        return " vs ".join(parts)


class TicketResolution(BaseModel):
    ticket_id: str
    resolution_type: ResolutionType
    chosen_result: str
    notes: str
    resolved_by: str
    confidence: float = Field(..., ge=0.0, le=1.0)
    solver_update_info: dict[str, Any] | None = None
    formula_correction: str | None = None


class TicketStatistics(BaseModel):
    total_tickets: int = 0
    open_tickets: int = 0
    under_review_tickets: int = 0
    resolved_tickets: int = 0
    escalated_tickets: int = 0

    avg_resolution_time_hours: float | None = None
    most_common_mismatch: str | None = None

    by_priority: dict[str, int] = Field(default_factory=dict)
    by_resolution_type: dict[str, int] = Field(default_factory=dict)
    by_solver: dict[str, int] = Field(default_factory=dict)

```


===== FILE: services/mismatch_service/service.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/mismatch_service/service.py    |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""Serwis do zarządzania biletami niezgodności między solverami SMT."""

from __future__ import annotations

import json
import logging
import uuid
from collections import defaultdict
from collections.abc import Callable
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from .models import (
    MismatchTicket,
    SolverResult,
    TicketPriority,
    TicketResolution,
    TicketStatistics,
    TicketStatus,
)

logger = logging.getLogger(__name__)

STORAGE_MODE = "memory"  # "file" w razie potrzeby
STORAGE_PATH = Path("data/mismatch_tickets")


class MismatchService:
    def __init__(self, storage_mode: str = STORAGE_MODE) -> None:
        self.storage_mode = storage_mode
        self._tickets: dict[str, MismatchTicket] = {}
        self._resolution_callbacks: list[Callable[[MismatchTicket, TicketResolution], None]] = []
        if storage_mode == "file":
            STORAGE_PATH.mkdir(parents=True, exist_ok=True)
            self._load_from_disk()
        logger.info(
            "MismatchService init (mode=%s, tickets=%s)",
            storage_mode,
            len(self._tickets),
        )

    # -- Creation --------------------------------------------------------
    def create_ticket(
        self,
        case_id: str,
        formula_str: str,
        results: dict[str, Any],
        formula_ast: dict[str, Any] | None = None,
        priority: TicketPriority | None = None,
    ) -> MismatchTicket:
        ticket_id = f"MM-{uuid.uuid4().hex[:8].upper()}"
        solver_results: list[SolverResult] = []
        for solver_name, data in results.items():
            if isinstance(data, dict):
                solver_results.append(
                    SolverResult(
                        solver_name=solver_name,
                        status=data.get("status", "unknown"),
                        execution_time_ms=data.get("time_ms"),
                        model=data.get("model"),
                        error_message=data.get("error"),
                        version=data.get("version"),
                    )
                )

        # Upewnij się, że priority ma typ TicketPriority (nie None)
        prio: TicketPriority = priority if priority is not None else TicketPriority.MEDIUM

        ticket = MismatchTicket(
            ticket_id=ticket_id,
            case_id=case_id,
            formula_str=formula_str,
            formula_ast=formula_ast,
            results=solver_results,
            priority=prio,
        )
        self._tickets[ticket_id] = ticket
        self._persist_if_needed()
        self._log_critical_alert(ticket)
        return ticket

    def _log_critical_alert(self, ticket: MismatchTicket) -> None:
        box = [
            "",
            "=" * 80,
            "🚨 CRITICAL ALERT: SOLVER MISMATCH DETECTED! 🚨",
            "=" * 80,
            f"Ticket ID: {ticket.ticket_id}",
            f"Case ID: {ticket.case_id}",
            f"Priority: {ticket.priority}",
            f"Mismatch: {ticket.get_mismatch_summary()}",
            "=" * 80,
            "⚠️  HUMAN INTERVENTION REQUIRED  ⚠️",
            "=" * 80,
            "",
        ]
        for line in box:
            logger.critical(line)

    # -- Retrieval -------------------------------------------------------
    def get_ticket(self, ticket_id: str) -> MismatchTicket | None:
        return self._tickets.get(ticket_id)

    def get_all_tickets(self) -> list[MismatchTicket]:
        return list(self._tickets.values())

    def get_open_tickets(self) -> list[MismatchTicket]:
        return [t for t in self._tickets.values() if t.status == TicketStatus.OPEN]

    def get_tickets_by_status(self, status: TicketStatus) -> list[MismatchTicket]:
        return [t for t in self._tickets.values() if t.status == status]

    def get_tickets_by_case(self, case_id: str) -> list[MismatchTicket]:
        return [t for t in self._tickets.values() if t.case_id == case_id]

    # -- Resolution / Escalation ----------------------------------------
    def resolve_ticket(self, ticket_id: str, resolution: TicketResolution) -> MismatchTicket:
        ticket = self._tickets.get(ticket_id)
        if not ticket:
            raise KeyError(f"Ticket not found: {ticket_id}")
        if ticket.status == TicketStatus.RESOLVED:
            raise ValueError(f"Ticket already resolved: {ticket_id}")

        ticket.status = TicketStatus.RESOLVED
        ticket.resolved_at = datetime.now(timezone.utc)
        ticket.resolution_type = resolution.resolution_type
        ticket.resolution_notes = resolution.notes
        ticket.resolved_by = resolution.resolved_by
        ticket.chosen_result = resolution.chosen_result
        ticket.updated_at = datetime.now(timezone.utc)

        self._persist_if_needed()

        for cb in self._resolution_callbacks:
            try:
                cb(ticket, resolution)
            except Exception as e:
                logger.error("Resolution callback failed: %s", e)

        logger.info(
            "Ticket %s resolved (type=%s, by=%s)",
            ticket_id,
            resolution.resolution_type,
            resolution.resolved_by,
        )
        return ticket

    def escalate_ticket(self, ticket_id: str, reason: str, escalated_by: str) -> MismatchTicket:
        ticket = self._tickets.get(ticket_id)
        if not ticket:
            raise KeyError(f"Ticket not found: {ticket_id}")
        ticket.status = TicketStatus.ESCALATED
        ticket.priority = TicketPriority.CRITICAL
        ticket.updated_at = datetime.now(timezone.utc)
        note = f"[{datetime.now(timezone.utc).isoformat()}] Escalated by {escalated_by}: {reason}"
        ticket.resolution_notes = ((ticket.resolution_notes + "\n") if ticket.resolution_notes else "") + note
        self._persist_if_needed()
        logger.warning("Ticket %s escalated by %s: %s", ticket_id, escalated_by, reason)
        return ticket

    # -- Stats -----------------------------------------------------------
    def get_statistics(self) -> TicketStatistics:
        stats = TicketStatistics()
        if not self._tickets:
            return stats

        status_counts = defaultdict(int)
        priority_counts = defaultdict(int)
        resolution_counts = defaultdict(int)
        solver_counts = defaultdict(int)
        resolution_times: list[float] = []

        for t in self._tickets.values():
            status_counts[t.status] += 1
            priority_counts[t.priority] += 1
            if t.resolution_type:
                resolution_counts[t.resolution_type] += 1
            for r in t.results:
                solver_counts[r.solver_name] += 1
            if t.resolved_at and t.created_at:
                resolution_times.append((t.resolved_at - t.created_at).total_seconds() / 3600.0)

        stats.total_tickets = len(self._tickets)
        stats.open_tickets = status_counts.get(TicketStatus.OPEN, 0)
        stats.under_review_tickets = status_counts.get(TicketStatus.UNDER_REVIEW, 0)
        stats.resolved_tickets = status_counts.get(TicketStatus.RESOLVED, 0)
        stats.escalated_tickets = status_counts.get(TicketStatus.ESCALATED, 0)
        stats.by_priority = dict(priority_counts)
        stats.by_resolution_type = dict(resolution_counts)
        stats.by_solver = dict(solver_counts)
        if resolution_times:
            stats.avg_resolution_time_hours = sum(resolution_times) / len(resolution_times)
        return stats

    # -- Persistence -----------------------------------------------------
    def _persist_if_needed(self) -> None:
        if self.storage_mode == "file":
            self._save_to_disk()

    def _save_to_disk(self) -> None:
        for ticket_id, ticket in self._tickets.items():
            p = STORAGE_PATH / f"{ticket_id}.json"
            with open(p, "w", encoding="utf-8") as f:
                json.dump(ticket.model_dump(), f, indent=2, default=str)

    def _load_from_disk(self) -> None:
        if not STORAGE_PATH.exists():
            return
        for p in STORAGE_PATH.glob("MM-*.json"):
            try:
                with open(p, encoding="utf-8") as f:
                    data = json.load(f)
                t = MismatchTicket(**data)
                self._tickets[t.ticket_id] = t
            except Exception as e:
                logger.error("Failed to load ticket %s: %s", p, e)

    # -- Callbacks -------------------------------------------------------
    def register_resolution_callback(self, callback: Callable[[MismatchTicket, TicketResolution], None]) -> None:
        self._resolution_callbacks.append(callback)
        logger.debug(
            "Registered resolution callback: %s",
            getattr(callback, "__name__", str(callback)),
        )


# Singleton
mismatch_service = MismatchService()

```


===== FILE: services/raas_service/main.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/raas_service/main.py           |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+

# |                          CERTEUS                            |

# +-------------------------------------------------------------+

# | FILE: services/raas_service/main.py                       |

# | ROLE: Project module.                                       |

# | PLIK: services/raas_service/main.py                       |

# | ROLA: Moduł projektu.                                       |

# +-------------------------------------------------------------+


"""



PL: Moduł CERTEUS – uzupełnij opis funkcjonalny.



EN: CERTEUS module – please complete the functional description.



"""


# +-------------------------------------------------------------+


# |                          CERTEUS                            |


# +-------------------------------------------------------------+


# | FILE: services/raas_service/main.py                       |


# | ROLE: Project module.                                       |


# | PLIK: services/raas_service/main.py                       |


# | ROLA: Moduł projektu.                                       |


# +-------------------------------------------------------------+

```


===== FILE: services/sipp_indexer_service/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/sipp_indexer_service/__init__.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/sipp_indexer_service/__init__.py             |
# | ROLE: Package marker for SIPP Indexer Service.              |
# +-------------------------------------------------------------+

"""
PL: Pakiet serwisu indeksującego akty prawne (SIPP Indexer).
EN: Package for the legal acts indexing service (SIPP Indexer).
"""

```


===== FILE: services/sipp_indexer_service/index_isap.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/sipp_indexer_service/index_isap.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
PL: Generator migawek ISAP. Buduje pojedynczy plik JSON `<act_id>.json`
    z polami `snapshot_timestamp` i `_certeus.snapshot_timestamp_utc`.
EN: ISAP snapshot generator. Produces a single JSON `<act_id>.json` with
    `snapshot_timestamp` and `_certeus.snapshot_timestamp_utc` fields.
"""

from __future__ import annotations

import json
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from hashlib import sha256
from pathlib import Path
from typing import Any


@dataclass
class ActSnapshot:
    act_id: str
    version_id: str
    text_sha256: str
    source_url: str
    title: str | None
    text: str
    snapshot_timestamp: str
    at: str | None = None
    _certeus: dict[str, Any] = field(default_factory=dict)


def _snapshot_for(act_id: str) -> ActSnapshot:
    text = (
        "Art. 286 k.k.: Kto, w celu osiągnięcia korzyści majątkowej, "
        "doprowadza inną osobę do niekorzystnego rozporządzenia mieniem "
        "za pomocą wprowadzenia w błąd..."
    )
    digest = "sha256:" + sha256(text.encode("utf-8")).hexdigest()
    now = datetime.now(timezone.utc).isoformat(timespec="seconds")

    return ActSnapshot(
        act_id=act_id,
        version_id="2023-10-01",
        text_sha256=digest,
        source_url="https://isap.sejm.gov.pl/isap.nsf/DocDetails.xsp?id=WDU19970880553",
        title="Kodeks karny – art. 286",
        text=text,
        snapshot_timestamp=now,
        at=None,
        _certeus={"snapshot_timestamp_utc": now},
    )


def index_act(act_id: str, out_dir: Path | None = None) -> Path:
    """Create a single JSON snapshot file and return its path."""
    snap = _snapshot_for(act_id)
    out_dir = Path(out_dir or Path("snapshots"))
    out_dir.mkdir(parents=True, exist_ok=True)
    path = out_dir / f"{act_id}.json"
    path.write_text(json.dumps(asdict(snap), ensure_ascii=False, indent=2), encoding="utf-8")
    return path

```


===== FILE: services/sipp_indexer_service/isap_adapter.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/sipp_indexer_service/isap_adapter.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/sipp_indexer_service/isap_adapter.py         |
# | ROLE: Stub adapter for ISAP. Creates LegalActSnapshot.      |
# +-------------------------------------------------------------+

"""
PL: Stub adaptera do ISAP. Wersja MVP: zwraca stałą migawkę dla
    podanego act_id, z poprawnym SHA256 i timestampem UTC.
EN: ISAP adapter stub. MVP version: returns a constant snapshot
    for a given act_id with proper SHA256 and UTC timestamp.
"""

from __future__ import annotations

import hashlib
from datetime import date, datetime, timezone

from .models import LegalActSnapshot


def _ascii_info(msg: str) -> None:
    try:
        from utils.console import info as _info  # type: ignore

        s = msg.encode("ascii", "ignore").decode("ascii")
        _info(s)
    except Exception:
        s = msg.encode("ascii", "ignore").decode("ascii")
        print(f"[INFO] {s}")


class IsapAdapter:
    """
    PL: Symuluje pobranie aktu prawnego i tworzy migawkę.
    EN: Simulates fetching a legal act and creating its snapshot.
    """

    def fetch_act_snapshot(self, act_id: str, at: date | None = None) -> LegalActSnapshot:
        """
        PL: Zwraca LegalActSnapshot dla zadanego act_id. Parametr 'at' to
            data, na ktora prosimy o stan prawa (stub ignoruje).
        EN: Returns LegalActSnapshot for given act_id. 'at' asks for state
            at given date (ignored by stub).
        """
        _ascii_info(f"ISAP Stub fetching act_id={act_id} at={at}")

        mock_text = (
            "Art. 286. § 1. Kto, w celu osiagniecia korzysci majatkowej, doprowadza "
            "inna osobe do niekorzystnego rozporzadzenia mieniem za pomoca wprowadzenia "
            "jej w blad albo wyzyskania bledu lub niezdolnosci do nalezytego pojmowania "
            "przedsiewzietego dzialania, podlega karze pozbawienia wolnosci od 6 miesiecy do lat 8."
        )
        text_hash = f"sha256:{hashlib.sha256(mock_text.encode('utf-8')).hexdigest()}"

        snapshot = LegalActSnapshot(
            act_id=act_id,
            version_id="2023-10-01",
            text_sha256=text_hash,
            source_url="https://isap.sejm.gov.pl/isap.nsf/DocDetails.xsp?id=WDU19970880553",
            valid_from=date(2023, 10, 1),
            snapshot_timestamp=datetime.now(timezone.utc),
        )
        return snapshot

```


===== FILE: services/sipp_indexer_service/models.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/sipp_indexer_service/models.py |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/sipp_indexer_service/models.py               |
# | ROLE: Pydantic DTOs for SIPP Indexer (LegalActSnapshot).    |
# +-------------------------------------------------------------+

"""
PL: Modele danych Pydantic v2 dla SIPP Indexer.
EN: Pydantic v2 data models for the SIPP Indexer.
"""

from __future__ import annotations

from datetime import date, datetime

from pydantic import BaseModel, Field


class LegalActSnapshot(BaseModel):
    """
    PL: Jedna, zwersjonowana migawka aktu prawnego.
    EN: A single, versioned snapshot of a legal act.
    """

    act_id: str = Field(
        ...,
        description="PL: Id aktu (np. 'kk-art-286'). EN: Act identifier (e.g., 'kk-art-286').",
    )
    version_id: str = Field(
        ...,
        description="PL: Id wersji (np. data publikacji). EN: Version id (e.g., publication date).",
    )
    text_sha256: str = Field(
        ...,
        description="PL: SHA256 tekstu wersji aktu. EN: SHA256 of act text for this version.",
    )
    source_url: str = Field(
        ...,
        description="PL: Oficjalny URL (np. ISAP). EN: Official source URL (e.g., ISAP).",
    )
    valid_from: date = Field(
        ...,
        description="PL: Data obowiązywania wersji. EN: Date from which this version is valid.",
    )
    snapshot_timestamp: datetime = Field(
        ...,
        description="PL: Znacznik czasu wykonania migawki. EN: Timestamp of snapshot creation.",
    )

```


===== FILE: services/zkp_service/stub.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/zkp_service/stub.py            |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+

# |                          CERTEUS                            |

# +-------------------------------------------------------------+

# | FILE: services/zkp_service/stub.py                        |

# | ROLE: Project module.                                       |

# | PLIK: services/zkp_service/stub.py                        |

# | ROLA: Moduł projektu.                                       |

# +-------------------------------------------------------------+


"""



PL: Moduł CERTEUS – uzupełnij opis funkcjonalny.



EN: CERTEUS module – please complete the functional description.



"""


# +-------------------------------------------------------------+


# |                          CERTEUS                            |


# +-------------------------------------------------------------+


# | FILE: services/zkp_service/stub.py                        |


# | ROLE: Project module.                                       |


# | PLIK: services/zkp_service/stub.py                        |


# | ROLA: Moduł projektu.                                       |


# +-------------------------------------------------------------+


def prove(data):
    return b"zkp"

```


===== FILE: storage/proof_cache/cache.py =====
```text
# +=====================================================================+
# |                          CERTEUS — HEART                            |
# +=====================================================================+
# | FILE: storage/proof_cache/cache.py                                  |
# | ROLE:                                                               |
# |  PL: Klucze cache PCO (czasowe kubełki 6h).                         |
# |  EN: PCO cache keys (6h time buckets).                              |
# +=====================================================================+

"""PL: cache_key = ruleset|query|ctx|jurisdiction|pack|bucket. EN: see above."""

from __future__ import annotations

import hashlib
import math
import time


def time_bucket(now: float, seconds: int = 21600) -> str:
    """PL: Wiadro czasowe 6h. EN: 6-hour time bucket."""
    return str(int(math.floor(now / seconds)))


def cache_key(
    ruleset_hash: str, query_hash: str, ctx_hash: str, jurisdiction: str, norm_pack_id: str, now: float | None = None
) -> str:
    """PL: Oblicz klucz cache. EN: Compute cache key."""
    if now is None:
        now = time.time()
    tb = time_bucket(now)
    raw = "|".join([ruleset_hash, query_hash, ctx_hash, jurisdiction, norm_pack_id, tb])
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()

```


===== FILE: templates/answer_contract.docx.placeholder =====
```text
placeholder

```


===== FILE: templates/answer_contract.txt.placeholder =====
```text
# +-------------------------------------------------------------+
# |              CERTEUS - RAPORT ANALIZY SPRAWY                |
# +-------------------------------------------------------------+

===============================================================
PODSUMOWANIE ANALIZY
===============================================================

ID SPRAWY: ${CASE_ID}
DATA ANALIZY: ${ANALYSIS_DATE}

GŁÓWNA TEZA:
${THESIS}

===============================================================
WYNIK WERYFIKACJI FORMALNEJ
===============================================================

STATUS: ${VERIFICATION_STATUS}

MODEL ROZWIĄZANIA (jeśli dotyczy):
${SOLUTION_MODEL}

===============================================================
DOWÓD POCHODZENIA (PROVENANCE HASH)
===============================================================

${PROVENANCE_HASH}

---
Raport wygenerowany automatycznie przez system CERTEUS v0.1.
Dowód. Nie opinia.

```


===== FILE: templates/truth_bill.docx.placeholder =====
```text
placeholder

```


===== FILE: tests/conftest.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/conftest.py                       |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: tests/conftest.py                                     |
# | ROLE: Shared pytest fixtures & test helpers.                |
# | PLIK: tests/conftest.py                                     |
# | ROLA: Wspólne fikstury pytest i pomocniki testowe.          |
# +-------------------------------------------------------------+
"""
PL: Zbiór współdzielonych fikstur i pomocników testowych dla całego pakietu testów.
EN: Shared pytest fixtures and helpers used across the test suite.
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

```


===== FILE: tests/e2e/test_e2e_export_endpoint.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/e2e/test_e2e_export_endpoint.py   |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                CERTEUS - Export API E2E Test                |
# +-------------------------------------------------------------+
# | FILE: tests/e2e/test_e2e_export_endpoint.py                 |
# | ROLE: Validates /v1/export endpoint end-to-end.             |
# | PLIK: tests/e2e/test_e2e_export_endpoint.py                 |
# | ROLA: Weryfikuje endpoint /v1/export E2E.                   |
# +-------------------------------------------------------------+
"""
PL: Test E2E endpointu /v1/export – oczekuje ścieżki do wygenerowanego raportu.
EN: E2E test for /v1/export endpoint – expects path to generated report.
"""

# [BLOCK: IMPORTS]
from __future__ import annotations

from pathlib import Path
from typing import Any, TypedDict

from fastapi.testclient import TestClient

from services.api_gateway.main import app


# [BLOCK: TYPES]
class ExportPayload(TypedDict):
    """PL: Struktura żądania dla /v1/export.
    EN: Request shape for /v1/export."""

    case_id: str
    analysis_result: dict[str, Any]


# [BLOCK: CLIENT]
client = TestClient(app)


# +-------------------------------------------------------------+
# | TEST: /v1/export returns generated report path              |
# +-------------------------------------------------------------+
def test_export_endpoint_returns_path() -> None:
    # [BLOCK: ARRANGE]
    payload: ExportPayload = {
        "case_id": "pl-286kk-0001",
        "analysis_result": {"status": "sat", "model": "[x=True]"},
    }

    # [BLOCK: ACT]
    response = client.post("/v1/export", json=payload)

    # [BLOCK: ASSERT]
    assert response.status_code == 200
    body: dict[str, Any] = response.json()

    # podstawowe oczekiwania
    assert "path" in body and isinstance(body["path"], str)
    assert "message" in body and isinstance(body["message"], str)
    assert body["message"].lower().startswith("report generated")

    # konwencja nazwy pliku (zgodna z ExporterService)
    assert Path(body["path"]).name == f"raport_{payload['case_id']}.txt"

```


===== FILE: tests/e2e/test_e2e_pl_286kk_0001.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/e2e/test_e2e_pl_286kk_0001.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: tests/e2e/test_e2e_pl_286kk_0001.py                   |
# | ROLE: End-to-end test for Art. 286 orchestration            |
# | PLIK: tests/e2e/test_e2e_pl_286kk_0001.py                   |
# | ROLA: Test E2E orkiestracji dla art. 286 k.k.               |
# +-------------------------------------------------------------+

"""
CERTEUS — E2E Test: Art. 286 k.k.
PL: Test end-to-end kanonicznego przypadku oszustwa dla /v1/analyze.
EN: End-to-end test of canonical fraud case via /v1/analyze.
"""

import io

from fastapi.testclient import TestClient

from services.api_gateway.main import app

client = TestClient(app)


def test_full_analysis_returns_sat():
    payload = ("dowody.pdf", io.BytesIO(b"fake bytes"), "application/pdf")
    r = client.post("/v1/analyze?case_id=pl-286kk-0001", files={"file": payload})
    assert r.status_code == 200
    data = r.json()
    assert data["case_id"] == "pl-286kk-0001"
    assert data["analysis_result"]["status"] == "sat"
    assert "model" in data["analysis_result"]

```
