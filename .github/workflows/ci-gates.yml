# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: .github/workflows/ci-gates.yml                      |
# | ROLE: Project YAML manifest.                                |
# | PLIK: .github/workflows/ci-gates.yml                      |
# | ROLA: Manifest YAML projektu.                               |
# +-------------------------------------------------------------+

name: ci-gates

on:
  push:
  pull_request:

jobs:
  ci-gates:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install tools
        run: |
          python -m pip install -U pip wheel setuptools
          # Install project runtime deps
          python -m pip install -e .
          # Install test/dev helpers explicitly used by the suite
          python -m pip install ruff pytest pytest-asyncio httpx z3-solver hypothesis

      - name: Load CI defaults (non-secrets)
        run: |
          python scripts/ci/load_env_defaults.py
      - name: Start mock OTLP receiver (background)
        run: |
          nohup python scripts/otel/mock_otlp.py >/dev/null 2>&1 &
      - name: Decorator split scan (report-only)
        run: |
          python scripts/fix_decorator_split.py --check || true
      - name: Premium Style Gate (sec.21)
        run: |
          python scripts/check_premium_style.py
      - name: Mark Style OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/style_ok.txt
      - name: Ruff Lint (no fix)
        run: |
          python -m ruff check .
      - name: Mark Lint OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/lint_ok.txt
      - name: A11y Smoke (report-only)
        continue-on-error: true
        run: |
          python scripts/smokes/a11y_smoke.py || true
      - name: Mark A11y OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/a11y_ok.txt
      - name: GameDay DR (Boundary) — dry-run (report-only)
        continue-on-error: true
        run: |
          python scripts/dr/drill_boundary_failure.py --max-rto-sec 120 --max-rpo-sec 300 || true
          test -f out/drill_boundary.json && echo ok > out/gameday_ok.txt || true
      - name: P2P Turbulence Smoke (report-only)
        continue-on-error: true
        run: |
          python scripts/dr/turbulence_p2p_smoke.py --iters 30 --out out/turbulence_p2p.json || true
          test -f out/turbulence_p2p.json && echo ok > out/p2p_turbulence_ok.txt || true
      - name: Tenant SLO Smoke (report-only)
        continue-on-error: true
        run: |
          python scripts/slo_gate/tenant_slo_sanity.py --tenants tenant-a tenant-b --iters 10 --out out/tenant_slo.json || true
          test -f out/tenant_slo.json && echo ok > out/tenant_slo_ok.txt || true
      - name: Tenant SLO Trend Gate (report-only)
        continue-on-error: true
        run: |
          set -e
          git fetch origin ci-status || true
          if git rev-parse --verify origin/ci-status >/dev/null 2>&1; then
            git show origin/ci-status:ci/tenant_slo.json > out/prev_tenant_slo.json 2>/dev/null || true
          fi
          python scripts/slo_gate/tenant_slo_trend_gate.py || true
      - name: Mark Tenant SLO Trend OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/tenant_slo_trend_ok.txt
      - name: Marketplace Policy Gate (report-only)
        continue-on-error: true
        run: |
          python scripts/gates/marketplace_policy_gate.py || true
      - name: Mark Marketplace OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/marketplace_ok.txt
      - name: Compliance Mapping (report-only)
        continue-on-error: true
        run: |
          python scripts/gates/compliance_mapping_gate.py || true
      - name: Mark Compliance OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/compliance_ok.txt
      - name: Bandit security scan (report-only)
        continue-on-error: true
        run: |
          python -m pip install bandit
          # Report-only in ci-gates; enforcement lives in dedicated workflow
          bandit -q -r . -x .venv,venv,clients/web,clients/cli,dist,build || true
      - name: Gitleaks (secrets scan)
        continue-on-error: true
        uses: zricethezav/gitleaks-action@v2.3.7
        with:
          # Report-only for ci-gates; blocking in security-scan.yml
          args: detect --verbose --redact --no-banner --exit-code 1
      - name: Tests
        env:
          PYTEST_ADDOPTS: "-k 'not test_generate_proofs_cli_smoke_test'"
          OTEL_ENABLED: "1"
          OTEL_EXPORTER_OTLP_ENDPOINT: "http://127.0.0.1:4318"
          FINE_GRAINED_ROLES: "0"
        run: |
          python -m pytest -q
      - name: Mark Tests OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/tests_ok.txt

      - name: Boundary Snapshot (artifact)
        run: |
          python scripts/boundary_snapshot.py --out out/boundary_snapshot.json
      - name: Upload boundary snapshot
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: boundary-snapshot-${{ github.sha }}
          path: out/boundary_snapshot.json
      - name: Perf Smoke (W11)
        run: |
          python scripts/perf/quick_bench.py --iters 5 --p95-max-ms 250 --out out/perf_bench.json
      - name: Perf Regression Gate (<= +5% vs prev)
        continue-on-error: true
        run: |
          set -e
          # Try to fetch previous perf benchmark from ci-status branch
          git fetch origin ci-status || true
          if git rev-parse --verify origin/ci-status >/dev/null 2>&1; then
            git show origin/ci-status:ci/perf_bench.json > out/prev_perf_bench.json 2>/dev/null || true
          fi
          python - << 'PY'
          import json, sys, pathlib
          repo = pathlib.Path('.').resolve()
          cur_p = repo/'out'/'perf_bench.json'
          prev_p = repo/'out'/'prev_perf_bench.json'
          if not cur_p.exists():
            print('No current perf report found; skipping regression gate')
            sys.exit(0)
          cur = json.loads(cur_p.read_text())
          worst = float(cur.get('worst_p95_ms', 0.0))
          thr_pct = 0.05
          if prev_p.exists():
            prev = json.loads(prev_p.read_text())
            base = float(prev.get('worst_p95_ms', 0.0)) or 1.0
            allowed = base * (1.0 + thr_pct)
            if worst > allowed:
              print(f'Regression: worst p95 {worst:.2f} ms > allowed {allowed:.2f} ms (base {base:.2f} +20%)')
              sys.exit(1)
          print('Perf regression gate: OK')
          PY
      - name: Mark Perf OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/perf_ok.txt

      - name: Upload perf report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-bench-${{ github.sha }}
          path: out/perf_bench.json

      - name: SLO Smoke (in-proc)
        env:
          SLO_MAX_P95_MS: "200"
          SLO_MAX_ERROR_RATE: "0.01"
        run: |
          python scripts/slo_gate/measure_api.py
          python scripts/slo_gate/check_slo.py
      - name: Mark SLO OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/slo_ok.txt

      - name: Setup OPA CLI
        uses: open-policy-agent/setup-opa@v2
        with:
          version: latest
      - name: OPA policy tests (roles)
        continue-on-error: true
        run: |
          opa test policies/security -v
      - name: Governance consistency (smoke)
        run: |
          python scripts/validate_governance_consistency.py && echo ok > out/gov_ok.txt

      - name: Marketplace Policy Gate (report-only)
        continue-on-error: true
        run: |
          python scripts/gates/marketplace_policy_gate.py || true

      - name: Pack ABI/SemVer Gate (report-only)
        continue-on-error: true
        run: |
          python scripts/gates/pack_abi_semver_gate.py || true

      - name: Redaction Gate (informational)
        env:
          STRICT_REDACTION: "0"
        run: |
          echo '{"subject":{"name":"Jan KOWALSKI"}, "content":"Brak wrażliwych danych."}' | python scripts/gates/redaction_gate.py

      - name: Metrics smoke (/metrics)
        run: |
          python scripts/smokes/metrics_smoke.py
      - name: Mark metrics smoke OK
        if: success()
        run: |
          echo ok > out/metrics_ok.txt

      - name: OpenAPI smoke (/openapi.json)
        run: |
          python scripts/smokes/openapi_smoke.py
      - name: QTMP smoke (in-proc)
        run: |
          python scripts/smokes/qtm_smoke.py
      - name: QTMP smoke (sequence + expectation)
        run: |
          python scripts/smokes/qtm_seq_expect_smoke.py
      - name: LexQFT coverage smoke
        run: |
          python scripts/smokes/lexqft_smoke.py
      - name: Mark OpenAPI smoke OK
        if: success()
        run: |
          echo ok > out/openapi_ok.txt

      - name: OpenAPI smoke (/openapi.json)
        run: |
          python scripts/smokes/openapi_smoke.py

      - name: Comment PR with perf bench
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let body = '';
            try {
              const data = JSON.parse(fs.readFileSync('out/perf_bench.json', 'utf8'));
              const lines = [];
              lines.push(`### Perf Bench (p95)`);
              lines.push(`Worst p95: ${data.worst_p95_ms.toFixed(2)} ms (threshold ${data.threshold_ms} ms)`);
              lines.push('');
              lines.push('Endpoints:');
              for (const e of data.endpoints) {
                lines.push(`- ${e.method} ${e.path}: p95=${e.p95_ms.toFixed(2)} ms (min=${e.min_ms.toFixed(2)}, max=${e.max_ms.toFixed(2)})`);
              }
              body = lines.join('\n');
            } catch (e) {
              body = 'Perf bench: report not available';
            }
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body,
            });

      - name: Build PR comment summary
        if: github.event_name == 'pull_request' && always()
        run: |
          set -e
          # Try to fetch previous perf benchmark from ci-status branch
          git fetch origin ci-status || true
          if git rev-parse --verify origin/ci-status >/dev/null 2>&1; then
            git show origin/ci-status:ci/perf_bench.json > out/prev_perf_bench.json 2>/dev/null || true
          fi
          python - << 'PY'
          import json, os, pathlib, urllib.request
          repo = pathlib.Path('.').resolve()
          lines = []
          # Perf current
          perf = None
          try:
              perf = json.loads((repo/'out'/'perf_bench.json').read_text())
          except Exception:
              pass
          # Perf previous
          prev = None
          try:
              prev = json.loads((repo/'out'/'prev_perf_bench.json').read_text())
          except Exception:
              pass
          lines.append('### Perf Bench (p95)')
          if perf:
              worst = float(perf.get('worst_p95_ms', 0.0))
              thr = perf.get('threshold_ms')
              lines.append(f"Worst p95: {worst:.2f} ms (threshold {thr} ms)")
              if prev and 'worst_p95_ms' in prev:
                  delta = worst - float(prev.get('worst_p95_ms', 0.0))
                  sign = '+' if delta >= 0 else ''
                  lines.append(f"Δ vs prev: {sign}{delta:.2f} ms")
              lines.append('')
              lines.append('Endpoints:')
              for e in perf.get('endpoints', []):
                  lines.append(f"- {e['method']} {e['path']}: p95={e['p95_ms']:.2f} ms (min={e['min_ms']:.2f}, max={e['max_ms']:.2f})")
              lines.append('')
          else:
              lines.append('Perf bench: report not available')
          # SLO summary with pass/fail (defaults align with step env)
          try:
              slo = json.loads((repo/'out'/'slo.json').read_text())
              p95 = float(slo.get('p95_ms', 0.0))
              er = float(slo.get('error_rate', 0.0))
              max_p95 = float(os.getenv('SLO_MAX_P95_MS', '300'))
              max_er = float(os.getenv('SLO_MAX_ERROR_RATE', '0.01'))
              ok = (p95 <= max_p95) and (er <= max_er)
              lines.append(f"SLO: p95={p95:.2f} ms (<= {max_p95}), error_rate={er:.4f} (<= {max_er}) => {'✅' if ok else '❌'}")
          except Exception:
              lines.append('SLO: report not available')
          # Smokes
          metrics_ok = (repo/'out'/'metrics_ok.txt').exists()
          openapi_ok = (repo/'out'/'openapi_ok.txt').exists()
          gov_ok = (repo/'out'/'gov_ok.txt').exists()
          lines.append(f"Smokes: metrics={'✅' if metrics_ok else '❌'} openapi={'✅' if openapi_ok else '❌'} governance={'✅' if gov_ok else '❌'}")
          # Local gate ticks
          def _tick(p: str) -> str:
              return '✅' if (repo/'out'/p).exists() else '❌'
          lines.append(f"Gates: style={_tick('style_ok.txt')} lint={_tick('lint_ok.txt')} tests={_tick('tests_ok.txt')} perf={_tick('perf_ok.txt')} slo={_tick('slo_ok.txt')}")
          lines.append(f"Marketplace: policy={_tick('marketplace_ok.txt')} • A11y={_tick('a11y_ok.txt')} • Compliance={_tick('compliance_ok.txt')} • GameDay={_tick('gameday_ok.txt')} • P2P={_tick('p2p_turbulence_ok.txt')} • TenantSLO={_tick('tenant_slo_ok.txt')}")
      - name: Generate OpenAPI (reports/openapi.json)
        run: |
          python - << 'PY'
          import json
          from pathlib import Path
          from fastapi.testclient import TestClient
          from services.api_gateway.main import app
          c = TestClient(app)
          r = c.get('/openapi.json')
          r.raise_for_status()
          Path('reports').mkdir(parents=True, exist_ok=True)
          Path('reports/openapi.json').write_text(json.dumps(r.json(), indent=2), encoding='utf-8')
          print('Generated reports/openapi.json')
          PY
      - name: Spectral Lint OpenAPI (report-only)
        continue-on-error: true
        run: |
          npx -y @stoplight/spectral-cli lint -r .spectral.yaml -f json -o reports/spectral.json reports/openapi.json || true
      - name: Mark Spectral OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/openapi_spectral_ok.txt
      - name: Install OpenAPI validator
        run: |
          python -m pip install openapi-spec-validator
      - name: OpenAPI Spec Validate (report-only)
        continue-on-error: true
        run: |
          python scripts/contracts/openapi_spec_validate.py || true
      - name: Mark OpenAPI Spec Validate OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/openapi_spec_ok.txt
      - name: OpenAPI Contract (report-only)
        continue-on-error: true
        run: |
          python scripts/gates/openapi_contract_gate.py || true
      - name: Mark OpenAPI Contract OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/openapi_contract_ok.txt
      - name: OpenAPI GET Sanity (report-only)
        continue-on-error: true
        run: |
          python scripts/contracts/openapi_get_sanity.py || true
      - name: Mark OpenAPI GET Sanity OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/openapi_get_sanity_ok.txt

      - name: Comment PR with SLO/Smokes/Perf
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let body = 'CI Summary';
            try { body = fs.readFileSync('out/ci_pr_comment.md','utf8'); } catch(e) {}
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body,
            });

      - name: Publish CI status to branch
        if: always()
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

          attempt=0
          max_attempts=5
          until [ $attempt -ge $max_attempts ]; do
            attempt=$((attempt+1))
            echo "[ci-status] Attempt $attempt/$max_attempts"
            # Always refetch latest remote tip
            git fetch origin ci-status || true
            if git rev-parse --verify origin/ci-status >/dev/null 2>&1; then
              git checkout -B ci-status origin/ci-status
            else
              git checkout --orphan ci-status
              git rm -rf . || true
            fi
            mkdir -p ci
            ts="$(date -u +%FT%TZ)"
            printf '%s\n' '{' \
              "  \"repo\": \"${{ github.repository }}\"," \
              "  \"workflow\": \"${{ github.workflow }}\"," \
              "  \"run_id\": \"${{ github.run_id }}\"," \
              "  \"run_number\": \"${{ github.run_number }}\"," \
              "  \"event\": \"${{ github.event_name }}\"," \
              "  \"branch\": \"${{ github.ref_name }}\"," \
              "  \"sha\": \"${{ github.sha }}\"," \
              "  \"status\": \"${{ job.status }}\"," \
              "  \"updated_at\": \"${ts}\"" \
            '}' > ci/status.json
            # Persist latest perf bench for trend comparisons
          if [ -f out/perf_bench.json ]; then
            cp -f out/perf_bench.json ci/perf_bench.json || true
          fi
          if [ -f out/tenant_slo.json ]; then
            cp -f out/tenant_slo.json ci/tenant_slo.json || true
          fi
          git add ci/status.json
          git add ci/perf_bench.json || true
          git add ci/tenant_slo.json || true
            git commit -m "ci: update status to ${{ job.status }} for ${{ github.sha }}" || echo "No changes"
            if git push origin ci-status; then
              echo "[ci-status] Push OK"
              break
            else
              echo "[ci-status] Push failed (likely race). Retrying..."
              sleep 2
            fi
          done
          if [ $attempt -ge $max_attempts ]; then
            echo "[ci-status] Failed to push after $max_attempts attempts"
            exit 1
          fi
