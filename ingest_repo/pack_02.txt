+=============================================================+
|                       CERTEUS — HEART                        |
+=============================================================+
CERTEUS PACK — Context for AI assistants (Claude/GPT/Gemini).
PL: Pliki repo są oddzielone '===== FILE: … ====='.
EN: Files are delimited by '===== FILE: … ====='.
Guidance: read sequentially; do not assume missing files exist; respect file boundaries.



===== FILE: scripts/lexlog_eval_smoke.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/lexlog_eval_smoke.py            |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |               CERTEUS - LEXLOG Smoke Evaluator              |
# +-------------------------------------------------------------+
# | PLIK: lexlog_eval_smoke.py                                  |
# | ROLA: Szybki test spełnienia art. 286 k.k. na podstawie     |
# |       reguł z kk.lex i mapowania kk.mapping.json.          |
# +-------------------------------------------------------------+
#
# PL: Jeśli nie podasz pliku z flagami (--flags), skrypt użyje
#     domyślnych trzech ról faktów, wymaganych przez art. 286:
#       - intent_financial_gain
#       - act_deception
#       - detrimental_property_disposal
#
# EN: If you don’t pass a flags file (--flags), the script uses
#     the default three fact roles required by Art. 286:
#       - intent_financial_gain
#       - act_deception
#       - detrimental_property_disposal
"""
PL: Smoke-test Jądra Prawdy: wczytuje flags JSON i sprawdza spełnialność.
EN: Truth Engine smoke test: loads flags JSON and checks satisfiability.
"""

from __future__ import annotations

import argparse
import json
from collections.abc import Mapping
from pathlib import Path

from services.lexlog_parser.evaluator import evaluate_rule
from services.lexlog_parser.mapping import load_mapping
from services.lexlog_parser.parser import parse_lexlog

RULE_ID = "R_286_OSZUSTWO"
RULES_PATH = Path("packs/jurisdictions/PL/rules/kk.lex")
MAP_PATH = Path("packs/jurisdictions/PL/rules/kk.mapping.json")

DEFAULT_FLAGS: Mapping[str, bool] = {
    "intent_financial_gain": True,
    "act_deception": True,
    "detrimental_property_disposal": True,
}


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--flags", type=str, default="", help="Ścieżka do pliku JSON z kluczem 'flags'")
    args = ap.parse_args()

    if not RULES_PATH.exists():
        raise SystemExit(f"[FATAL] Missing rules file: {RULES_PATH}")
    if not MAP_PATH.exists():
        raise SystemExit(f"[FATAL] Missing mapping file: {MAP_PATH}")

    ast = parse_lexlog(RULES_PATH.read_text(encoding="utf-8"))
    ctx = load_mapping(MAP_PATH)

    if args.flags:
        flags_path = Path(args.flags)
        if not flags_path.exists():
            raise SystemExit(f"[ERROR] Missing flags file: {flags_path}")
        data = json.loads(flags_path.read_text(encoding="utf-8"))
        flags = data.get("flags", {})
    else:
        flags = dict(DEFAULT_FLAGS)

    # Now evaluate explicit rule id (4-arg signature).
    res = evaluate_rule(ast, RULE_ID, flags, ctx)

    if getattr(res, "satisfied", False):
        print("[SUCCESS] art. 286 satisfied")
    else:
        missing = getattr(res, "missing_premises", []) or []
        failing = getattr(res, "failing_excludes", []) or []
        print("[ERROR] art. 286 NOT satisfied")
        if missing:
            print("  missing_premises:", sorted(missing))
        if failing:
            print("  failing_excludes:", sorted(failing))


if __name__ == "__main__":
    main()

```


===== FILE: scripts/lexlog_eval_smoke_fallback.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/lexlog_eval_smoke_fallback.py   |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: scripts/lexlog_eval_smoke_fallback.py                 |
# | ROLE: Fallback smoke runner for LEXLOG evaluation           |
# | PLIK: scripts/lexlog_eval_smoke_fallback.py                 |
# | ROLA: Skrypt awaryjny: buduje flagi i uruchamia smoke eval  |
# +-------------------------------------------------------------+

"""
CERTEUS — Lexlog Smoke Eval (Fallback)
PL: Skrypt pomocniczy budujący plik flag (na podstawie mappingu) i
    uruchamiający test dymny oceny LEXLOG (art. 286 k.k.).
EN: Auxiliary script that builds the flags file (from mapping) and
    runs the LEXLOG smoke evaluation (Art. 286).
"""

import os
import subprocess
import sys

FLAGS_PATH = os.path.join("packs", "jurisdictions", "PL", "flags", "lexenith_results_latest.json")


def main():
    print("[INFO] Building flags from mapping for R_286_OSZUSTWO...")
    subprocess.run(
        [
            sys.executable,
            "scripts/build_flags_from_mapping.py",
            "--rule-id",
            "R_286_OSZUSTWO",
        ],
        check=True,
    )
    print("[INFO] Running smoke with generated flags...")
    subprocess.run(
        [sys.executable, "scripts/lexlog_eval_smoke.py", "--flags", FLAGS_PATH],
        check=True,
    )


if __name__ == "__main__":
    main()

```


===== FILE: scripts/make_merkle_sample.py =====
```text
#!/usr/bin/env python3
# +======================================================================+
# |                               CERTEUS                                |
# +======================================================================+
# | FILE / PLIK: scripts/make_merkle_sample.py                           |
# | ROLE / ROLA:                                                          |
# |  EN: Build demo public bundle (non-empty Merkle path) and sign it.   |
# |  PL: Buduje przykładowy publiczny bundle (niepusta ścieżka Merkle)   |
# |      i podpisuje go.                                                 |
# +======================================================================+
# Założenia:
# - czyta klucz prywatny Ed25519 z PEM: ED25519_PRIVKEY_PEM
# - zapisuje do $PROOF_BUNDLE_DIR (fallback: ./data/public_pco)
# - materiał demo: LFSC/SMT2 w razie braku plików podawanych flagami
# ----Bloki----- IMPORTY (PEP 8/PEP 585)
#   - max 120 znaków/linia

from __future__ import annotations

import base64
import hashlib
import json
import os
import sys
from argparse import ArgumentParser
from collections.abc import Iterable
from pathlib import Path
from typing import Any, TypedDict

from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey, Ed25519PublicKey


# ----Bloki----- TYPY
class MerkleStep(TypedDict):
    sibling: str  # hex
    dir: str  # 'L' | 'R'


# ----Bloki----- POMOCNICZE
DEFAULT_DIR = os.getenv("PROOF_BUNDLE_DIR", "./data/public_pco")


def _hx_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _is_hex64(x: str) -> bool:
    return isinstance(x, str) and len(x) == 64 and all(c in "0123456789abcdef" for c in x.lower())


def sha256_hex_utf8(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def compute_bundle_hash_hex(pub: dict[str, Any]) -> str:
    payload = {"smt2_hash": pub["smt2_hash"], "lfsc_sha256": sha256_hex_utf8(pub["lfsc"])}
    if pub.get("drat") is not None:
        payload["drat_sha256"] = sha256_hex_utf8(pub["drat"])
    blob = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
    return hashlib.sha256(blob).hexdigest()


def merkle_root_from_path(leaf_hex: str, path: Iterable[MerkleStep]) -> str:
    cur = bytes.fromhex(leaf_hex)
    for step in path:
        sib = bytes.fromhex(step["sibling"])
        if step["dir"] == "L":
            cur = hashlib.sha256(sib + cur).digest()
        elif step["dir"] == "R":
            cur = hashlib.sha256(cur + sib).digest()
        else:
            raise ValueError(f"Invalid dir: {step['dir']!r}")
    return cur.hex()


def canonical_digest_hex(pub: dict[str, Any], merkle_root_hex: str) -> str:
    parts = [
        hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest(),
        pub["smt2_hash"].lower(),
        sha256_hex_utf8(pub["lfsc"]),
    ]
    if pub.get("drat") is not None:
        parts.append(sha256_hex_utf8(pub["drat"]))
    parts.append(merkle_root_hex.lower())
    msg = b"".join(bytes.fromhex(x) for x in parts)
    return hashlib.sha256(msg).hexdigest()


def ed25519_from_b64url(x_b64u: str) -> Ed25519PublicKey:
    pad = "=" * (-len(x_b64u) % 4)
    raw = base64.urlsafe_b64decode(x_b64u + pad)
    return Ed25519PublicKey.from_public_bytes(raw)


# ----Bloki----- BUNDLE (demo)
def make_bundle(
    outdir: Path,
    rid: str,
    lfsc_path: str | None,
    smt2_path: str | None,
    signature_b64u: str | None,
    *,
    allow_missing: bool = True,
    lfsc_text: str | None = None,
    smt2_text: str | None = None,
) -> Path:
    outdir.mkdir(parents=True, exist_ok=True)

    # Materiał: LFSC/SMT2 – z pliku lub inline albo fallback demo
    lfsc = lfsc_text or (
        Path(lfsc_path).read_text(encoding="utf-8") if lfsc_path and Path(lfsc_path).exists() else None
    )
    smt2 = smt2_text or (
        Path(smt2_path).read_text(encoding="utf-8") if smt2_path and Path(smt2_path).exists() else None
    )
    if not allow_missing and (lfsc is None or smt2 is None):
        print("Missing LFSC/SMT2 and --allow-missing not set", file=sys.stderr)
        raise SystemExit(2)
    if lfsc is None:
        lfsc = "(lfsc proof for demo-002)"
    if smt2 is None:
        smt2 = "(set-logic ALL)\n(check-sat)"

    pub = {
        "rid": rid,
        "smt2_hash": hashlib.sha256(smt2.encode("utf-8")).hexdigest(),
        "lfsc": lfsc,
        "drat": None,
    }

    # Ścieżka Merkle – przykład niepusty
    path: list[MerkleStep] = [
        {"sibling": "a" * 64, "dir": "L"},
        {"sibling": "b" * 64, "dir": "R"},
    ]

    rid_hash_hex = hashlib.sha256(rid.encode("utf-8")).hexdigest()
    bundle_hash_hex = compute_bundle_hash_hex(pub)
    leaf_hex = hashlib.sha256(bytes.fromhex(rid_hash_hex) + bytes.fromhex(bundle_hash_hex)).hexdigest()
    merkle_root_hex = merkle_root_from_path(leaf_hex, path)

    digest_hex = canonical_digest_hex(pub | {"merkle_proof": path}, merkle_root_hex)

    # Podpis – jeśli nie podano detached signature, generujemy z PEM (ENV)
    if signature_b64u is None:
        pem_path = os.getenv("ED25519_PRIVKEY_PEM")
        if not pem_path or not Path(pem_path).exists():
            print("Missing ED25519_PRIVKEY_PEM", file=sys.stderr)
            raise SystemExit(2)
        sk_any = serialization.load_pem_private_key(Path(pem_path).read_bytes(), password=None)
        if not isinstance(sk_any, Ed25519PrivateKey):
            print("PEM is not Ed25519 private key", file=sys.stderr)
            raise SystemExit(2)
        sig = sk_any.sign(bytes.fromhex(digest_hex))
        signature_b64u = base64.urlsafe_b64encode(sig).rstrip(b"=").decode()

    out = {
        **pub,
        "merkle_proof": path,
        "signature": signature_b64u,
        "issued_at": "2025-08-19T12:00:00Z",
    }

    out_path = outdir / f"{rid}.json"
    out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
    return out_path


# --- blok --- CLI --------------------------------------------------------------
def _parse_args() -> Any:
    p = ArgumentParser(description="Build demo public bundle (non-empty Merkle path) and sign it.")
    p.add_argument("--rid", help="Resource ID (64-hex). If absent, derived from seed.")
    p.add_argument("--seed", help="Seed text for RID (SHA-256)", default=None)
    p.add_argument("--lfsc", dest="lfsc_path", help="Path to LFSC file", default=None)
    p.add_argument("--smt2", dest="smt2_path", help="Path to SMT2 file", default=None)
    p.add_argument("--signature", help="Detached signature (>= 40 chars)", default=None)
    p.add_argument("--outdir", help="Output dir (defaults to PROOF_BUNDLE_DIR)", default=DEFAULT_DIR)
    p.add_argument("--echo-url", action="store_true", help="Print GET URL for API")
    p.add_argument("--allow-missing", action="store_true", help="Use default text if --lfsc/--smt2 files are missing")
    p.add_argument("--lfsc-text", dest="lfsc_text", default=None, help="Inline LFSC text (overrides --lfsc file)")
    p.add_argument("--smt2-text", dest="smt2_text", default=None, help="Inline SMT2 text (overrides --smt2 file)")
    return p.parse_args()


def main() -> int:
    args = _parse_args()

    # RID resolve
    if args.rid:
        rid = args.rid.strip().lower()
        if not _is_hex64(rid):
            print("[RID] must be 64-hex (lowercase).", flush=True)
            return 2
    else:
        seed = args.seed or "rid-demo"
        rid = _hx_text(seed)

    outdir = Path(args.outdir)
    out_path = make_bundle(
        outdir,
        rid,
        args.lfsc_path,
        args.smt2_path,
        args.signature,
        allow_missing=args.allow_missing,
        lfsc_text=args.lfsc_text,
        smt2_text=args.smt2_text,
    )

    print(f"[OK] bundle written: {out_path}")
    if args.echo_url:
        print(f"[GET] http://127.0.0.1:8000/pco/public/{rid}")
    return 0


# --- blok --- Entrypoint -------------------------------------------------------
if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: scripts/make_pco_bundle.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: scripts/make_pco_bundle.py                          |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: Create a minimal public PCO bundle JSON under PROOF_BUNDLE_DIR |
# |      (0 PII). Lets you choose RID (seed/hex), LFSC/SMT2 sources and |
# |      signature.                                                     |
# |  PL: Tworzy minimalny publiczny bundle PCO (0 PII) w PROOF_BUNDLE_DIR|
# |      z możliwością wyboru RID (seed/hex), źródeł LFSC/SMT2 i podpisu.|
# +=====================================================================+

r"""
Użycie / Usage (PowerShell):
  # 1) na podstawie "seed" (RID = sha256(seed))
  uv run python scripts/make_pco_bundle.py --seed rid-demo

  # 2) z własnym RID (64-hex), LFSC i SMT2 z plików
  uv run python scripts/make_pco_bundle.py `
    --rid 0123...cafe `
    --lfsc F:\dowody\proof.lfsc `
    --smt2 F:\modele\model.smt2 `
    --signature 0f0f...

  # 3) od razu pokaż URL GET dla API
  uv run python scripts/make_pco_bundle.py --seed case-42 --echo-url

Domyślny katalog wyjściowy:
  PROOF_BUNDLE_DIR lub ./data/public_pco (utworzy się automatycznie)

Zawartość bundla (minimal):
  {
    "rid": <64-hex>,
    "smt2_hash": <sha256 hexdigest pliku/tekstu SMT2>,
    "lfsc": "(proof ...)" lub zawartość pliku LFSC,
    "merkle_proof": { "root": <leaf>, "leaf": <leaf>, "path": [] },
    "signature": "0"*64 (lub podany)
  }

Weryfikacja:
  curl http://127.0.0.1:8000/pco/public/<rid>
"""

# --- blok --- Importy ----------------------------------------------------------
from __future__ import annotations

import argparse
import json
import os
from hashlib import sha256
from pathlib import Path
from typing import Final

# --- blok --- Stałe ------------------------------------------------------------

DEFAULT_DIR: Final[str] = os.getenv("PROOF_BUNDLE_DIR", "./data/public_pco")


# --- blok --- Hash utils -------------------------------------------------------
def _hx_bytes(data: bytes) -> str:
    """sha256 -> hex."""
    return sha256(data).hexdigest()


def _hx_text(text: str) -> str:
    """sha256(utf-8) -> hex."""
    return _hx_bytes(text.encode("utf-8"))


def _is_hex64(s: str) -> bool:
    """Czy ciąg jest 64-znakowym heksadecymalnym ID (lowercase)."""
    return len(s) == 64 and all(c in "0123456789abcdef" for c in s)


def _compute_leaf(rid_hex: str, smt2_hex: str) -> str:
    """
    Minimalny wariant z testów E2E:
    leaf = sha256( (rid_hex + smt2_hex).encode("utf-8") )
    """
    return _hx_text(rid_hex + smt2_hex)


# --- blok --- Bundle build -----------------------------------------------------
def _read_or_default(
    path: str | None,
    default_text: str,
    *,
    allow_missing: bool = False,
    inline_text: str | None = None,
) -> tuple[str, str]:
    """
    Zwraca (content_text, sha256_hex).
    Priorytet: inline_text > plik(path) > default_text (gdy allow_missing lub brak path).
    """
    if inline_text is not None:
        return inline_text, _hx_text(inline_text)

    if path:
        p = Path(path)
        try:
            text = p.read_text(encoding="utf-8")
            return text, _hx_text(text)
        except FileNotFoundError:
            if allow_missing:
                text = default_text
                return text, _hx_text(text)
            raise

    text = default_text
    return text, _hx_text(text)


def make_bundle(
    outdir: Path,
    rid: str,
    lfsc_path: str | None,
    smt2_path: str | None,
    signature: str | None,
    *,
    allow_missing: bool = False,
    lfsc_text: str | None = None,
    smt2_text: str | None = None,
) -> Path:
    outdir.mkdir(parents=True, exist_ok=True)

    # LFSC & SMT2 (treść + hash SMT2)
    lfsc_text_val, _ = _read_or_default(
        lfsc_path,
        "(proof ...)",
        allow_missing=allow_missing,
        inline_text=lfsc_text,
    )
    _, smt2_hex = _read_or_default(
        smt2_path,
        "(set-logic QF_UF) ...",
        allow_missing=allow_missing,
        inline_text=smt2_text,
    )

    # Merkle minimal: root == leaf, brak ścieżki
    leaf = _compute_leaf(rid, smt2_hex)
    merkle_proof = {"root": leaf, "leaf": leaf, "path": []}

    # Podpis: >= 40 znaków (domyślnie 64 zera)
    sig = signature if (signature and len(signature) >= 40) else ("0" * 64)

    bundle = {
        "rid": rid,
        "smt2_hash": smt2_hex,
        "lfsc": lfsc_text_val,
        "merkle_proof": merkle_proof,
        "signature": sig,
    }

    out_path = outdir / f"{rid}.json"
    out_path.write_text(
        json.dumps(bundle, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )
    return out_path


# --- blok --- CLI --------------------------------------------------------------


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        prog="make_pco_bundle",
        description="Create a minimal public PCO bundle JSON (0 PII).",
    )
    rid_grp = p.add_mutually_exclusive_group(required=False)
    rid_grp.add_argument("--rid", help="RID (64-hex). If not provided, use --seed")
    rid_grp.add_argument("--seed", help="Seed string -> RID = sha256(seed)")

    p.add_argument("--lfsc", dest="lfsc_path", help="Path to LFSC file", default=None)
    p.add_argument("--smt2", dest="smt2_path", help="Path to SMT2 file", default=None)
    p.add_argument("--signature", help="Detached signature (>= 40 chars)", default=None)
    p.add_argument("--outdir", help="Output dir (defaults to PROOF_BUNDLE_DIR)", default=DEFAULT_DIR)
    p.add_argument("--echo-url", action="store_true", help="Print GET URL for API")
    p.add_argument("--allow-missing", action="store_true", help="Use default text if --lfsc/--smt2 files are missing")
    p.add_argument("--lfsc-text", dest="lfsc_text", default=None, help="Inline LFSC text (overrides --lfsc file)")
    p.add_argument("--smt2-text", dest="smt2_text", default=None, help="Inline SMT2 text (overrides --smt2 file)")
    return p.parse_args()


def main() -> int:
    args = _parse_args()

    # RID resolve
    if args.rid:
        rid = args.rid.strip().lower()
        if not _is_hex64(rid):
            print("[RID] must be 64-hex (lowercase).", flush=True)
            return 2
    else:
        seed = args.seed or "rid-demo"
        rid = _hx_text(seed)

    outdir = Path(args.outdir)
    out_path = make_bundle(
        outdir,
        rid,
        args.lfsc_path,
        args.smt2_path,
        args.signature,
        allow_missing=args.allow_missing,
        lfsc_text=args.lfsc_text,
        smt2_text=args.smt2_text,
    )

    print(f"[OK] bundle written: {out_path}")
    if args.echo_url:
        print(f"[GET] http://127.0.0.1:8000/pco/public/{rid}")
    return 0


# --- blok --- Entrypoint -------------------------------------------------------

if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: scripts/pem_to_b64url.py =====
```text
# +======================================================================+
# |                               CERTEUS                                |
# +======================================================================+
# | FILE / PLIK: scripts/pem_to_b64url.py                                |
# | ROLA / ROLE:                                                          |
# |  PL: Konwersja PEM Ed25519 (public) -> Base64URL (pole 'x' do JWKS).  |
# |  EN: Convert Ed25519 public PEM -> Base64URL (JWKS 'x' value).        |
# +======================================================================+
# Opis:
# - Wczytuje klucz publiczny Ed25519 w PEM i wypisuje Base64URL bez '='.
# Użycie:
#   uv run python scripts/pem_to_b64url.py --in ed25519-public.pem
# Wymagania:
#   - Python 3.11+, cryptography
# ----Bloki----- IMPORTY
from __future__ import annotations

import argparse
import base64
from pathlib import Path

from cryptography.hazmat.primitives import serialization


# ----Bloki----- MAIN
def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--in", dest="inp", required=True)
    args = ap.parse_args()

    raw = Path(args.inp).read_bytes()
    pk = serialization.load_pem_public_key(raw)
    # Ed25519 only:
    b = pk.public_bytes(
        encoding=serialization.Encoding.Raw,
        format=serialization.PublicFormat.Raw,
    )
    print(base64.urlsafe_b64encode(b).rstrip(b"=").decode())


if __name__ == "__main__":
    main()

```


===== FILE: scripts/sign_bundle.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE: scripts/sign_bundle.py                                        |
# | ROLE: Sign public PCO bundle (Ed25519 detached signature)           |
# +=====================================================================+
# ----Bloki----- IMPORTY
from __future__ import annotations

import base64
import hashlib
import json
import os
from argparse import ArgumentParser
from pathlib import Path
from typing import Any, cast

from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey


# ----Bloki----- FUNKCJE
def sha256_hex_utf8(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def compute_bundle_hash_hex(pub: dict[str, Any]) -> str:
    payload = {"smt2_hash": pub["smt2_hash"], "lfsc_sha256": sha256_hex_utf8(pub["lfsc"])}
    if pub.get("drat") is not None:
        payload["drat_sha256"] = sha256_hex_utf8(pub["drat"])
    blob = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
    return hashlib.sha256(blob).hexdigest()


def canonical_digest_hex(pub: dict[str, Any], merkle_root_hex: str) -> str:
    parts = [
        hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest(),
        pub["smt2_hash"].lower(),
        sha256_hex_utf8(pub["lfsc"]),
    ]
    if pub.get("drat") is not None:
        parts.append(sha256_hex_utf8(pub["drat"]))
    parts.append(merkle_root_hex.lower())
    msg = b"".join(bytes.fromhex(x) for x in parts)
    return hashlib.sha256(msg).hexdigest()


# ----Bloki----- MAIN
def main() -> None:
    ap = ArgumentParser(description="Sign public PCO bundle (Ed25519 detached signature).")
    ap.add_argument("--rid", required=True)
    ap.add_argument("--bundle-dir", default=os.getenv("PROOF_BUNDLE_DIR", "./data/public_pco"))
    ap.add_argument("--key", default="ed25519-private.pem", help="PEM private key (PKCS8, no password)")
    args = ap.parse_args()

    bundle_path = Path(args.bundle_dir) / f"{args.rid}.json"
    # Czytaj 'utf-8-sig' (leczy BOM); zapisz bez BOM
    pub = json.loads(bundle_path.read_text(encoding="utf-8-sig"))

    # Merkle (MVP): path=[] → root = leaf
    bundle_hash_hex = compute_bundle_hash_hex(pub)
    rid_hash_hex = hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest()
    leaf_hex = hashlib.sha256(bytes.fromhex(rid_hash_hex) + bytes.fromhex(bundle_hash_hex)).hexdigest()
    merkle_root_hex = leaf_hex

    digest_hex = canonical_digest_hex(pub, merkle_root_hex)

    sk_any = serialization.load_pem_private_key(Path(args.key).read_bytes(), password=None)
    if not isinstance(sk_any, Ed25519PrivateKey):
        raise TypeError("Loaded key is not Ed25519 (expected Ed25519PrivateKey).")
    sk = cast(Ed25519PrivateKey, sk_any)

    sig = sk.sign(bytes.fromhex(digest_hex))
    pub["signature"] = base64.urlsafe_b64encode(sig).rstrip(b"=").decode()

    bundle_path.write_text(json.dumps(pub, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"OK: signed {bundle_path}")


if __name__ == "__main__":
    main()

```


===== FILE: scripts/smoke286.ps1 =====
```text
# +-------------------------------------------------------------+
# |                 CERTEUS - smoke286 helper                   |
# +-------------------------------------------------------------+
# | FILE: scripts/smoke286.ps1                                  |
# | ROLE: One-liner to run build->smoke->tests                  |
# +-------------------------------------------------------------+

param()

uv run python scripts/lexlog_eval_smoke_fallback.py
uv run pytest -q tests/services/test_lexlog_parser.py

```


===== FILE: scripts/smoke_ingest.ps1 =====
```text
# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |                 Smoke test for /v1/ingest                   |
# +-------------------------------------------------------------+

Set-StrictMode -Version Latest
$ErrorActionPreference = "Stop"

# Files live only in working dir; we do not git-add them.
Set-Content -NoNewline -Encoding ASCII .\sample.txt 'hello CERTEUS'
Set-Content -NoNewline -Encoding ASCII .\sample.pdf '%PDF-1.4 dummy'

Write-Host "[INFO] Health"
curl.exe -sS http://127.0.0.1:8000/health

Write-Host "[INFO] Ingest TXT"
curl.exe -sS -i --form "file=@sample.txt;type=text/plain" http://127.0.0.1:8000/v1/ingest | Select-String -Pattern "HTTP/1.1 200"

Write-Host "[INFO] Ingest PDF"
curl.exe -sS -i --form "file=@sample.pdf;type=application/pdf" http://127.0.0.1:8000/v1/ingest | Select-String -Pattern "HTTP/1.1 200"

Write-Host "[INFO] Done"

```


===== FILE: scripts/smoke_ingest.sh =====
```text
# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |                 Smoke test for /v1/ingest                   |
# +-------------------------------------------------------------+

set -euo pipefail

printf '%s' 'hello CERTEUS' > sample.txt
printf '%s' '%PDF-1.4 dummy' > sample.pdf

echo "[INFO] Health"
curl -sS http://127.0.0.1:8000/health

echo "[INFO] Ingest TXT"
curl -sS -i -F "file=@sample.txt;type=text/plain" http://127.0.0.1:8000/v1/ingest | grep "HTTP/1.1 200"

echo "[INFO] Ingest PDF"
curl -sS -i -F "file=@sample.pdf;type=application/pdf" http://127.0.0.1:8000/v1/ingest | grep "HTTP/1.1 200"

echo "[INFO] Done"

```


===== FILE: scripts/temporal_fortress.py =====
```text
# +=====================================================================+
# |                          CERTEUS — HEART                            |
# +=====================================================================+
# | FILE: scripts/temporal_fortress.py                                  |
# | ROLE:                                                               |
# |  PL: Re-proof po TTL (TTDE) per domena.                             |
# |  EN: TTDE re-proof per domain TTL.                                  |
# +=====================================================================+

"""PL: Harmonogram odświeżania dowodów (prawo=365, med=90, fin=7).
EN: Re-proof scheduler (law=365, med=90, fin=7).
"""

from __future__ import annotations

from datetime import datetime, timedelta, timezone

TTL_DAYS: dict[str, int] = {"prawo": 365, "med": 90, "fin": 7}


def due(domain: str, last_proof_at: datetime) -> bool:
    """PL: Czy wygasło TTL? EN: TTL expired?"""
    days = TTL_DAYS.get(domain, 365)
    return datetime.now(timezone.utc) - last_proof_at > timedelta(days=days)


def run() -> None:
    """PL: Znajdź kapsuły do re-proof i zleć zadania. EN: Enqueue due re-proofs."""
    # Integracja z magazynem kapsuł (do uzupełnienia zgodnie z repo).
    # Keeping architecture unchanged.
    return

```


===== FILE: scripts/validate_policy_pack.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: scripts/validate_policy_pack.py                     |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: Validate PCO Policy Pack (YAML) vs JSON Schema;                |
# |      enforce invariants (no-PII, required fields, endpoint pattern, |
# |      drat_required) and provide CLI/report.                         |
# |  PL: Walidacja Policy Pack (YAML) względem Schema; egzekwowanie     |
# |      inwariantów (brak PII, wymagane pola, wzorzec endpointu,       |
# |      drat_required) + CLI/raport.                                   |
# +=====================================================================+
from __future__ import annotations

# stdlib
import argparse
import json
import os
import re
import sys
from pathlib import Path
from typing import Any

# third-party
import yaml  # type: ignore
from jsonschema import Draft7Validator, Draft201909Validator, Draft202012Validator

# ----Bloki----- STAŁE
DEFAULT_SCHEMA = Path("policies/pco/policy_pack.schema.v0.1.json")
DEFAULT_PACK = Path("policies/pco/policy_pack.v0.1.yaml")
ENV_SCHEMA = "PCO_POLICY_PACK_SCHEMA"
ENV_PACK = "PCO_POLICY_PACK_PATH"

# Denylista PII (klucze)
PII_FIELD_NAMES: set[str] = {
    "name",
    "first_name",
    "last_name",
    "pesel",
    "email",
    "phone",
    "address",
    "dob",
    "ssn",
    "patient_id",
    "person_id",
    "user_id",
}

# Dozwolone klucze w publicznym payload
ALLOWED_PUBLIC_FIELDS: set[str] = {
    "rid",
    "smt2_hash",
    "lfsc",
    "drat",
    "merkle_proof",
    "signature",
}

# Minimalny zestaw wymaganych pól
REQUIRED_PUBLIC_FIELDS: set[str] = {
    "rid",
    "smt2_hash",
    "lfsc",
    "merkle_proof",
    "signature",
}

ENDPOINT_PATTERN = re.compile(r"^/pco/public/\{case_id\}$")


# ----Bloki----- I/O
def _read_json(path: Path) -> dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def _read_yaml(path: Path) -> dict[str, Any]:
    data = yaml.safe_load(path.read_text(encoding="utf-8"))
    if not isinstance(data, dict):
        raise TypeError("YAML must decode to an object (mapping)")
    return data


# ----Bloki----- Dobór walidatora
def _pick_validator(schema: dict[str, Any]):
    ident = str(schema.get("$schema", "")).lower()
    if "draft-07" in ident:
        return Draft7Validator
    if "2019-09" in ident:
        return Draft201909Validator
    return Draft202012Validator


# ----Bloki----- Inwarianty
def _ensure_no_pii(fields: list[str], ctx: str, messages: list[dict[str, Any]]) -> None:
    lowered = {f.replace("?", "").lower() for f in fields}
    forbidden = sorted(lowered.intersection(PII_FIELD_NAMES))
    if forbidden:
        messages.append(
            {
                "level": "error",
                "code": "PII_FORBIDDEN",
                "where": ctx,
                "detail": f"PII fields not allowed: {', '.join(forbidden)}",
            }
        )


def _check_required_fields(fields: list[str], ctx: str, messages: list[dict[str, Any]]) -> None:
    s = set(x.replace("?", "") for x in fields)
    missing = sorted(REQUIRED_PUBLIC_FIELDS - s)
    if missing:
        messages.append(
            {
                "level": "error",
                "code": "REQUIRED_MISSING",
                "where": ctx,
                "detail": f"Missing required fields: {', '.join(missing)}",
            }
        )


def _check_unknown_fields(fields: list[str], ctx: str, messages: list[dict[str, Any]]) -> None:
    s = set(x.replace("?", "") for x in fields)
    unknown = sorted(s - ALLOWED_PUBLIC_FIELDS)
    if unknown:
        messages.append(
            {
                "level": "warning",
                "code": "UNKNOWN_FIELD",
                "where": ctx,
                "detail": f"Unknown fields present: {', '.join(unknown)}",
            }
        )


def _check_endpoint_pattern(endpoint: str, ctx: str, messages: list[dict[str, Any]]) -> None:
    if not ENDPOINT_PATTERN.fullmatch(endpoint):
        messages.append(
            {
                "level": "error",
                "code": "ENDPOINT_PATTERN",
                "where": ctx,
                "detail": f"Endpoint must match '^/pco/public/{{case_id}}$', got: {endpoint}",
            }
        )


def run_invariants(pack: dict[str, Any]) -> list[dict[str, Any]]:
    msgs: list[dict[str, Any]] = []

    use_cases = pack.get("use_cases", {})
    if not isinstance(use_cases, dict):
        msgs.append({"level": "error", "code": "USE_CASES_TYPE", "where": "use_cases", "detail": "must be object"})
        return msgs

    for uc_name, uc in use_cases.items():
        if not isinstance(uc, dict):
            msgs.append(
                {"level": "error", "code": "UC_TYPE", "where": f"use_cases.{uc_name}", "detail": "must be object"}
            )
            continue

        publish = uc.get("publish", {})
        if not isinstance(publish, dict):
            msgs.append(
                {
                    "level": "error",
                    "code": "PUBLISH_TYPE",
                    "where": f"use_cases.{uc_name}.publish",
                    "detail": "must be object",
                }
            )
            continue

        endpoint = str(publish.get("endpoint", ""))
        _check_endpoint_pattern(endpoint, f"use_cases.{uc_name}.publish.endpoint", msgs)

        fields = publish.get("fields", [])
        if not isinstance(fields, list) or not all(isinstance(x, str) for x in fields):
            msgs.append(
                {
                    "level": "error",
                    "code": "FIELDS_TYPE",
                    "where": f"use_cases.{uc_name}.publish.fields",
                    "detail": "must be array of strings",
                }
            )
            continue

        _ensure_no_pii(fields, f"use_cases.{uc_name}.publish.fields", msgs)
        _check_required_fields(fields, f"use_cases.{uc_name}.publish.fields", msgs)
        _check_unknown_fields(fields, f"use_cases.{uc_name}.publish.fields", msgs)

        # NEW: drat_required => wymagamy 'drat' w polach publikacji
        drat_required = bool(uc.get("drat_required", False))
        if drat_required and "drat" not in {f.replace("?", "") for f in fields}:
            msgs.append(
                {
                    "level": "error",
                    "code": "DRAT_REQUIRED",
                    "where": f"use_cases.{uc_name}.publish.fields",
                    "detail": "drat_required=true but 'drat' field not present",
                }
            )

    return msgs


# ----Bloki----- CLI
def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        prog="validate_policy_pack",
        description="Validate PCO Policy Pack (schema + invariants).",
    )
    p.add_argument("--schema", type=Path, default=Path(os.getenv(ENV_SCHEMA) or DEFAULT_SCHEMA))
    p.add_argument("--pack", type=Path, default=Path(os.getenv(ENV_PACK) or DEFAULT_PACK))
    p.add_argument("--format", choices=["text", "json"], default="text")
    p.add_argument("--strict", action="store_true")
    p.add_argument("--list-use-cases", action="store_true")
    return p.parse_args()


def _emit_text(schema_errors: list[str], messages: list[dict[str, Any]], use_cases: list[str]) -> None:
    if use_cases:
        print("[use_cases] " + ", ".join(use_cases))
    for e in schema_errors:
        print(f"[SCHEMA] {e}", file=sys.stderr)
    for m in messages:
        lvl = str(m.get("level", "info")).upper()
        code = str(m.get("code", "MSG"))
        where = str(m.get("where", "-"))
        detail = str(m.get("detail", ""))
        print(f"[{lvl}] {code} @ {where}: {detail}")


def _emit_json(schema_errors: list[str], messages: list[dict[str, Any]], use_cases: list[str]) -> None:
    out = {"use_cases": use_cases, "schema_errors": schema_errors, "messages": messages}
    print(json.dumps(out, ensure_ascii=False, indent=2))


# ----Bloki----- MAIN
def main() -> int:
    args = _parse_args()
    try:
        schema = _read_json(args.schema)
        pack = _read_yaml(args.pack)
    except Exception:
        return 4

    Validator = _pick_validator(schema)
    schema_errs = [
        f"{'/'.join(map(str, e.path))}: {e.message}"
        for e in sorted(Validator(schema).iter_errors(pack), key=lambda e: e.path)
    ]  # type: ignore[arg-type]

    messages = run_invariants(pack)

    uc_names: list[str] = []
    if args.list_use_cases and isinstance(pack.get("use_cases"), dict):
        uc_names = list(pack["use_cases"].keys())  # type: ignore[index]

    if args.format == "json":
        _emit_json(schema_errs, messages, uc_names)
    else:
        _emit_text(schema_errs, messages, uc_names)

    has_schema_errors = bool(schema_errs)
    has_errors = has_schema_errors or any(m.get("level") == "error" for m in messages)
    has_warnings = any(m.get("level") == "warning" for m in messages)

    if has_errors:
        return 2
    if has_warnings and args.strict:
        return 2
    if has_warnings:
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: scripts/validate_schemas.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/scripts/validate_schemas.py             |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                    CERTEUS - Schema Validator               |
# +-------------------------------------------------------------+
# | PLIK / FILE: scripts/validate_schemas.py                    |
# | ROLA / ROLE: Waliduje składnię i jakość wszystkich          |
# |             schematów JSON w projekcie.                     |
# +-------------------------------------------------------------+
# | STYLE: PL-first, headers & notes dual-language (PL/EN).     |
# +-------------------------------------------------------------+

"""
PL: Ten moduł zapewnia, że wszystkie kontrakty danych (schematy) są
    zgodne ze standardem JSON Schema Draft 7 oraz spełniają minimalne
    wymogi jakości (title/description, spójność required/properties).

EN: Ensures all data contracts (schemas) comply with JSON Schema Draft 7
    and pass minimal quality gates (title/description presence,
    required/properties consistency).
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

from jsonschema import Draft7Validator  # Only what we truly use

SCHEMA_DIR = Path("schemas")


def _check_quality(name: str, schema: dict[str, Any]) -> list[str]:
    """
    PL: Dodatkowe bramki jakości dla schematów.
    EN: Additional quality gates for schemas.
    """
    errs: list[str] = []

    # Wymagamy tytułu i opisu
    if not schema.get("title"):
        errs.append("Missing 'title'")
    if not schema.get("description"):
        errs.append("Missing 'description'")

    # Spójność required/properties na poziomie root
    props = schema.get("properties", {})
    required = schema.get("required", [])
    for key in required:
        if key not in props:
            errs.append(f"'required' contains '{key}' not present in 'properties'")

    # Restrykcyjność: preferujemy additionalProperties:false na root
    if schema.get("additionalProperties", True) is not False:
        errs.append("Root 'additionalProperties' should be false for stricter contracts")

    return errs


def main() -> None:
    """
    PL: Główna funkcja — weryfikuje syntaksę schematów i bramki jakości.
    EN: Main routine — verifies schema syntax and quality gates.
    """
    print(f"🔎 Validating schemas in: {SCHEMA_DIR.absolute()}")
    has_errors = False

    schema_files = sorted(SCHEMA_DIR.glob("*.json"))
    if not schema_files:
        print("⚠️ No schemas found to validate.")
        raise SystemExit(0)

    for schema_path in schema_files:
        print(f"--- Checking: {schema_path.name} ---")
        try:
            schema_instance = json.loads(schema_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError as e:
            print(f"❌ ERROR: Invalid JSON in {schema_path.name}: {e}")
            has_errors = True
            continue

        # Walidacja składniowa samego schematu
        try:
            Draft7Validator.check_schema(schema_instance)
            print("✅ Schema syntax is valid.")
        except Exception as e:  # noqa: BLE001
            print(f"❌ ERROR: Schema is invalid in {schema_path.name}: {e}")
            has_errors = True
            continue

        # Bramka jakości
        q_errs = _check_quality(schema_path.name, schema_instance)
        if q_errs:
            has_errors = True
            for q in q_errs:
                print(f"❌ QUALITY: {q}")
        else:
            print("✨ Quality gate passed.")

    if has_errors:
        print("\n💥 Validation failed for one or more schemas.")
        raise SystemExit(1)

    print("\n🎉 All schemas are syntactically valid and passed quality checks!")


if __name__ == "__main__":
    main()

```


===== FILE: scripts/verify_bundle.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | FILE: scripts/verify_bundle.py                                      |
# | ROLE: Verify public PCO bundle (Merkle + canonical digest + Ed25519)|
# +=====================================================================+
# Opis:
# - Weryfikuje publiczny bundle PCO (0 PII): struktura, Merkle, podpis.
# - Obsługa Merkle path: MVP [] oraz pełna ścieżka [{sibling, dir:'L'|'R'}].
# - Kanoniczny digest: sha256( rid_hash || smt2_hash || lfsc_sha256 || [drat_sha256] || merkle_root ).
# - Podpis: Ed25519 (Base64URL, bez "="); klucz publiczny z --pub-b64url lub ENV ED25519_PUBKEY_B64URL.
# - Zwraca kod wyjścia: 0 OK, 2 błąd weryfikacji/IO.
#
# Użycie (CLI):
#   python scripts/verify_bundle.py --rid demo-001 \
#       --bundle-dir ./data/public_pco \
#       --pub-b64url $env:ED25519_PUBKEY_B64URL
#
# Wymagania:
#   - Python 3.11+, pakiet 'cryptography'
#   - JSON bez BOM (lub czytany 'utf-8-sig')
#
# Konwencje:
#   - future → stdlib (import, from) → third-party (import, from)
#   - typy: dict[str, Any], list[str] itd. (PEP 585)
#   - linia: max 120
# ----Bloki----- IMPORTY
from __future__ import annotations

import base64
import hashlib
import json
import os
import sys
from argparse import ArgumentParser
from collections.abc import Iterable
from pathlib import Path
from typing import Any, TypedDict, cast

from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PublicKey


# ----Bloki----- TYPY
class MerkleStep(TypedDict):
    sibling: str  # hex
    dir: str  # 'L' | 'R'


# ----Bloki----- POMOCNICZE
def sha256_hex_utf8(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def is_hex64(x: str) -> bool:
    return isinstance(x, str) and len(x) == 64 and all(c in "0123456789abcdef" for c in x.lower())


def compute_bundle_hash_hex(pub: dict[str, Any]) -> str:
    payload = {"smt2_hash": pub["smt2_hash"], "lfsc_sha256": sha256_hex_utf8(pub["lfsc"])}
    if pub.get("drat") is not None:
        payload["drat_sha256"] = sha256_hex_utf8(pub["drat"])
    blob = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
    return hashlib.sha256(blob).hexdigest()


def merkle_root_from_path(leaf_hex: str, path: Iterable[MerkleStep]) -> str:
    cur = bytes.fromhex(leaf_hex)
    for step in path:
        sib = bytes.fromhex(step["sibling"])
        if step["dir"] == "L":
            cur = hashlib.sha256(sib + cur).digest()
        elif step["dir"] == "R":
            cur = hashlib.sha256(cur + sib).digest()
        else:
            raise ValueError(f"Invalid dir: {step['dir']!r}")
    return cur.hex()


def canonical_digest_hex(pub: dict[str, Any], merkle_root_hex: str) -> str:
    parts = [
        hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest(),
        pub["smt2_hash"].lower(),
        sha256_hex_utf8(pub["lfsc"]),
    ]
    if pub.get("drat") is not None:
        parts.append(sha256_hex_utf8(pub["drat"]))
    parts.append(merkle_root_hex.lower())
    msg = b"".join(bytes.fromhex(x) for x in parts)
    return hashlib.sha256(msg).hexdigest()


def ed25519_from_b64url(x_b64u: str) -> Ed25519PublicKey:
    pad = "=" * (-len(x_b64u) % 4)
    raw = base64.urlsafe_b64decode(x_b64u + pad)
    return Ed25519PublicKey.from_public_bytes(raw)


# ----Bloki----- MAIN
def main() -> int:
    ap = ArgumentParser(description="Verify public PCO bundle (Merkle + Ed25519 signature).")
    ap.add_argument("--rid", required=True)
    ap.add_argument("--bundle-dir", default=os.getenv("PROOF_BUNDLE_DIR", "./data/public_pco"))
    ap.add_argument("--pub-b64url", default=os.getenv("ED25519_PUBKEY_B64URL"))
    args = ap.parse_args()

    if not args.pub_b64url:
        print("ERR: missing public key (use --pub-b64url or ED25519_PUBKEY_B64URL)", file=sys.stderr)
        return 2

    bundle_path = Path(args.bundle_dir) / f"{args.rid}.json"
    try:
        pub = json.loads(bundle_path.read_text(encoding="utf-8-sig"))
    except Exception as e:  # noqa: BLE001
        print(f"ERR: cannot read bundle: {e}", file=sys.stderr)
        return 2

    # Sanity checks
    if not is_hex64(pub.get("smt2_hash", "")):
        print("ERR: smt2_hash must be 64 hex chars", file=sys.stderr)
        return 2

    # 1) leaf := sha256( rid_hash || bundle_hash )
    rid_hash_hex = hashlib.sha256(pub["rid"].encode("utf-8")).hexdigest()
    bundle_hash_hex = compute_bundle_hash_hex(pub)
    leaf_hex = hashlib.sha256(bytes.fromhex(rid_hash_hex) + bytes.fromhex(bundle_hash_hex)).hexdigest()

    # 2) merkle_root (MVP: path może być [] lub L/R ścieżka)
    path = pub.get("merkle_proof") or []
    merkle_root_hex = merkle_root_from_path(leaf_hex, cast(Iterable[MerkleStep], path))

    # 3) canonical digest
    digest_hex = canonical_digest_hex(pub, merkle_root_hex)

    # 4) verify signature
    try:
        pk = ed25519_from_b64url(cast(str, args.pub_b64url))
        sig_b64u = pub.get("signature", "")
        pad = "=" * (-len(sig_b64u) % 4)
        sig = base64.urlsafe_b64decode(sig_b64u + pad)
        pk.verify(sig, bytes.fromhex(digest_hex))
    except Exception as e:  # noqa: BLE001
        print(f"ERR: signature invalid: {e}", file=sys.stderr)
        return 2

    print(f"OK: {bundle_path} verified")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```


===== FILE: security/proofs.md =====
```text
External Proof Checking (DRAT/LFSC) – zasady publikacji.

```


===== FILE: security/rbac_policies.md =====
```text
Role: Architect, Proof‑Engineer, LKEW, Ops.

```


===== FILE: services/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/__init__.py                    |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
PL: Pakiet inicjalizacyjny modułu.
EN: Package initializer.
"""

```


===== FILE: services/api_gateway/app_e2e.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/app_e2e.py         |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Moduł systemu CERTEUS.
EN: CERTEUS system module.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

from fastapi import FastAPI
from pydantic import BaseModel, Field

from kernel.truth_engine import DualCoreVerifier
from services.exporter_service.exporter import ExporterService

__all__ = ["app"]

# ──────────────────────────────────────────────────────────────────────
# App & services
# ──────────────────────────────────────────────────────────────────────

app = FastAPI(title="CERTEUS E2E", version="1.1.0")

# Provide explicit constructor args (fixes: missing template_dir/output_dir)
_exporter = ExporterService(template_dir="templates", output_dir="exports/e2e")
_verifier = DualCoreVerifier()

# ──────────────────────────────────────────────────────────────────────
# Schemas
# ──────────────────────────────────────────────────────────────────────


class SimpleFact(BaseModel):
    """Minimalny model wejściowy do E2E solve."""

    case_id: str = Field(..., description="Case identifier")
    smt2: str = Field(..., description="SMT-LIB2 formula")
    export: bool = Field(False, description="Export report file after solve")
    force_mismatch: bool = Field(
        False, description="Flip Core-2 to trigger mismatch protocol (testing)"
    )


class SolveResponse(BaseModel):
    status: str
    time_ms: float | None = None
    model: dict[str, Any] | None = None
    error: str | None = None
    report_path: str | None = None
    version: str | None = None


# ──────────────────────────────────────────────────────────────────────
# Routes
# ──────────────────────────────────────────────────────────────────────


@app.get("/health")
def health() -> dict[str, Any]:
    return {"status": "ok", "services": ["verifier", "exporter"]}


@app.post("/e2e/solve", response_model=SolveResponse)
def e2e_solve(payload: SimpleFact) -> SolveResponse:
    # 1) verify with DualCore
    result = _verifier.verify(
        payload.smt2,
        lang="smt2",
        case_id=payload.case_id,
        force_mismatch=payload.force_mismatch,
    )
    # 2) optional export
    report_path: str | None = None
    if payload.export:
        out_path = _exporter.export_report(payload.case_id, result)
        report_path = str(Path(out_path))

    return SolveResponse(
        status=str(result.get("status", "unknown")),
        time_ms=result.get("time_ms"),
        model=result.get("model"),
        error=result.get("error"),
        report_path=report_path,
        version=result.get("version"),
    )

```


===== FILE: services/api_gateway/main.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/api_gateway/main.py                        |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: API Gateway bootstrap. Mounts static, registers routers,       |
# |      enables DEV CORS, exposes health and root redirect.            |
# |  PL: Bootstrap bramy API. Montuje statyczne zasoby, rejestruje      |
# |      routery, włącza CORS (DEV), wystawia health i redirect root.   |
# +=====================================================================+

"""
PL: Główna aplikacja FastAPI dla CERTEUS: statyki, routery, CORS (DEV), health.
EN: Main FastAPI app for CERTEUS: statics, routers, CORS (DEV), health.
"""

# --- blok --- Importy ----------------------------------------------------------
from __future__ import annotations

# stdlib
from contextlib import asynccontextmanager
from pathlib import Path

# third-party
from fastapi import FastAPI, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse
from fastapi.staticfiles import StaticFiles

# local (rozbite na pojedyncze linie — łatwiej sortować i Ruff nie marudzi)
from services.api_gateway.routers import (
    export,
    ledger,
    mismatch,
    pco_public,
    preview,
    system,  # /v1/ingest, /v1/analyze, /v1/sipp
    verify,
)
from services.api_gateway.routers.well_known_jwks import router as jwks_router
from services.ingest_service.adapters.contracts import Blob
from services.ingest_service.adapters.registry import get_llm, get_preview

# --- blok --- Ścieżki i katalogi ----------------------------------------------

ROOT = Path(__file__).resolve().parents[2]
STATIC_DIR = ROOT / "static"
STATIC_PREVIEWS = STATIC_DIR / "previews"
CLIENTS_WEB = ROOT / "clients" / "web"  # expects /app/proof_visualizer/index.html

STATIC_PREVIEWS.mkdir(parents=True, exist_ok=True)
CLIENTS_WEB.mkdir(parents=True, exist_ok=True)

APP_TITLE = "CERTEUS API Gateway"
APP_VERSION = "1.1.5"

# --- blok --- Lifespan (inicjalizacja adapterów) -------------------------------


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    PL: Leniwe utworzenie singletonów adapterów przy starcie.
    EN: Lazily create adapter singletons on startup.
    """
    _ = (get_preview(), get_llm())
    yield


# --- blok --- Aplikacja i middleware -------------------------------------------

app = FastAPI(
    title=APP_TITLE,
    version=APP_VERSION,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan,
)


# statyki
app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")
app.mount("/app", StaticFiles(directory=str(CLIENTS_WEB)), name="app")

# CORS (DEV) – szeroko, bez credentials
DEV_ORIGINS: list[str] = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=DEV_ORIGINS,
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- blok --- Rejestr routerów -------------------------------------------------

app.include_router(system.router)
app.include_router(preview.router)
app.include_router(pco_public.router)
app.include_router(export.router)
app.include_router(ledger.router)
app.include_router(mismatch.router)
app.include_router(verify.router)
app.include_router(jwks_router)

# --- blok --- Health i root redirect -------------------------------------------


@app.get("/health")
def health() -> dict[str, object]:
    """PL: Liveness; EN: Liveness."""
    return {"status": "ok", "version": APP_VERSION}


@app.get("/")
def root_redirect() -> RedirectResponse:
    """
    PL: W DEV kierujemy na UI wizualizatora.
    EN: In DEV, redirect to the proof visualizer UI.
    """
    return RedirectResponse(url="/app/proof_visualizer/index.html", status_code=307)


# --- blok --- Pomocnicze -------------------------------------------------------


def _make_blob(upload: UploadFile, data: bytes) -> Blob:
    """
    PL: Buduje Blob z UploadFile.
    EN: Build Blob from UploadFile.
    """
    return Blob(
        filename=upload.filename or "file",
        content_type=upload.content_type or "application/octet-stream",
        data=data,
    )

```


===== FILE: services/api_gateway/routers/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/__init__.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: services/api_gateway/routers/__init__.py              |
# | ROLE: Make 'routers' a package (no implicit re-exports).    |
# | PLIK: services/api_gateway/routers/__init__.py              |
# | ROLA: Czyni 'routers' pakietem (bez niejawnych re-eksportów).|
# +-------------------------------------------------------------+
"""
PL: Minimalne __init__, by uniknąć ostrzeżeń Pylance/Ruff o „unused import”.
EN: Minimal __init__ to avoid Pylance/Ruff 'unused import' warnings.
"""

# [UWAGA]
# Nie re-eksportujemy tutaj verify/system/export/ledger, żeby nie generować
# F401/unused-import. Moduły importujemy bezpośrednio w main.py.

```


===== FILE: services/api_gateway/routers/export.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/api_gateway/routers/export.py              |
# | DATE / DATA: 2025-08-17                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: /v1/export – validate payload and write a TXT report           |
# |      named `raport_{case_id}.txt`. Enrich response with provenance  |
# |      (sha256, timestamp_utc, artifacts).                            |
# |  PL: /v1/export – walidacja ładunku i zapis raportu TXT             |
# |      `raport_{case_id}.txt`. Odpowiedź wzbogacona o provenance      |
# |      (sha256, timestamp_utc, artifacts).                            |
# +=====================================================================+

"""
PL: Endpoint eksportu. Przyjmuje `case_id` i `analysis_result`, zapisuje raport
    tekstowy do katalogu `exports/`, po czym zwraca ścieżkę oraz provenance
    zawierające hash SHA-256 pliku raportu, znacznik czasu UTC i listę artefaktów.
EN: Export endpoint. Accepts `case_id` and `analysis_result`, writes a text
    report under `exports/`, then returns the path and provenance with the
    report file's SHA-256, UTC timestamp, and artifacts list.
"""

from __future__ import annotations

import hashlib
import json
from collections.abc import Mapping
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

router = APIRouter(prefix="", tags=["export"])


# === MODELS / MODELE ===
class ExportPayload(BaseModel):
    case_id: str = Field(..., description="Public case id, e.g. 'pl-286kk-0001'")
    analysis_result: Mapping[str, Any] = Field(default_factory=dict)
    fmt: str = Field("report", description="Output format (tests use 'report').")


class ExportResponse(BaseModel):
    path: str
    message: str
    provenance: dict[str, Any] | None = None  # optional to keep tests happy


# === HELPERS ===
def _hash_file_sha256(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def _now_iso_utc() -> str:
    return datetime.now(timezone.utc).isoformat()


def _write_report(case_id: str, analysis_result: Mapping[str, Any], out_dir: Path) -> Path:
    """
    PL: Zapis raportu jako .txt z podstawowym podsumowaniem i pretty JSON.
    EN: Write .txt report with a tiny header + pretty JSON of the analysis.
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    filename = f"raport_{case_id}.txt"
    path = out_dir / filename

    lines: list[str] = []
    lines.append("# CERTEUS – Raport analityczny / Analytical Report")
    lines.append(f"Case: {case_id}")
    try:
        pretty = json.dumps(analysis_result, ensure_ascii=False, indent=2, sort_keys=True)
    except Exception:
        pretty = str(analysis_result)
    lines.append("")
    lines.append("=== ANALIZA / ANALYSIS ===")
    lines.append(pretty)

    path.write_text("\n".join(lines), encoding="utf-8")
    return path


# === ENDPOINT ===
@router.post("/v1/export", response_model=ExportResponse)
def export_endpoint(payload: ExportPayload) -> ExportResponse:
    """
    PL: Generuje raport i zwraca ścieżkę + provenance (hash, timestamp, artifacts).
    EN: Generate report and return path + provenance (hash, timestamp, artifacts).
    """
    case_id = payload.case_id.strip()
    if not case_id:
        raise HTTPException(status_code=400, detail="case_id required")

    out_dir = Path("exports")
    path = _write_report(case_id, payload.analysis_result, out_dir)

    # Build provenance / Budowa provenance
    prov: dict[str, Any] = {
        "hash_sha256": _hash_file_sha256(path),
        "timestamp_utc": _now_iso_utc(),
        "artifacts": {
            "report": str(path),
        },
    }

    return ExportResponse(
        path=str(path),
        message=f"Report generated at {path}",
        provenance=prov,
    )

```


===== FILE: services/api_gateway/routers/ledger.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/ledger.py  |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: services/api_gateway/routers/ledger.py                |
# | ROLE: Public API for the ledger (record/list/prove)         |
# | PLIK: services/api_gateway/routers/ledger.py                |
# | ROLA: Publiczne API dla księgi (record/list/prove)          |
# +-------------------------------------------------------------+

"""
PL: Router FastAPI dla księgi pochodzenia (ledger):
    - /record-input       : rejestruje dokument wejściowy,
    - /{case_id}/records  : pobiera wpisy dla sprawy,
    - /{case_id}/prove    : buduje i (opcjonalnie) waliduje paragon pochodzenia.
EN: FastAPI router for the provenance ledger:
    - /record-input       : record an input document,
    - /{case_id}/records  : list entries for a case,
    - /{case_id}/prove    : build and (optionally) validate a provenance receipt.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Protocol, cast

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

# Import the module, then obtain a working singleton/instance.
import services.ledger_service.ledger as ledger_mod  # noqa: F401


# --- Protocol to satisfy type checker (methods used by this router) ---
class LedgerLike(Protocol):
    def record_input(self, *, case_id: str, document_hash: str) -> dict[str, Any]: ...
    def get_records_for_case(self, *, case_id: str) -> list[dict[str, Any]]: ...
    def build_provenance_receipt(self, *, case_id: str) -> dict[str, Any]: ...


# Prefer existing singleton; else instantiate a fresh Ledger from the module.
ledger_service: LedgerLike = cast(
    LedgerLike,
    getattr(ledger_mod, "ledger_service", None) or ledger_mod.Ledger(),  # type: ignore[attr-defined, call-arg]
)

# Optional JSON Schema validation (soft dependency)
try:
    from jsonschema import Draft7Validator  # type: ignore
except Exception:  # pragma: no cover
    Draft7Validator = None  # type: ignore[assignment]

router = APIRouter()

# Repo paths
REPO_ROOT = Path(__file__).resolve().parents[3]
SCHEMAS_DIR = REPO_ROOT / "schemas"

# Lazy schema/validator (not hard constants)
_provenance_schema: dict[str, Any] | None = None
_provenance_validator: Any | None = None
if Draft7Validator is not None:
    schema_path = SCHEMAS_DIR / "provenance_receipt_v1.json"
    if schema_path.exists():
        try:
            _provenance_schema = json.loads(schema_path.read_text(encoding="utf-8"))
            _provenance_validator = Draft7Validator(_provenance_schema)  # type: ignore[call-arg]
        except Exception:
            _provenance_schema = None
            _provenance_validator = None


# === MODELS ===
class RecordInputRequest(BaseModel):
    """
    PL: Wejście do zarejestrowania dokumentu.
    EN: Input to record a document ingestion.
    """

    case_id: str = Field(..., min_length=1, description="PL: Id sprawy. / EN: Case identifier.")
    document_hash: str = Field(
        ...,
        min_length=7,
        description="PL: Np. 'sha256:<hex>'. / EN: e.g., 'sha256:<hex>'.",
    )


class RecordInputResponse(BaseModel):
    """
    PL: Odpowiedź na zarejestrowanie dokumentu.
    EN: Response for recorded document ingestion.
    """

    event_id: int
    type: str
    case_id: str
    document_hash: str | None
    timestamp: str
    chain_prev: str | None
    chain_self: str


# === ENDPOINTS ===
@router.post("/record-input", response_model=RecordInputResponse, tags=["Ledger"])
def record_input(payload: RecordInputRequest) -> RecordInputResponse:
    """
    PL: Rejestruje nowy dokument w księdze (INPUT_INGESTION).
    EN: Records a new document in the ledger (INPUT_INGESTION).
    """
    result = ledger_service.record_input(
        case_id=payload.case_id, document_hash=payload.document_hash
    )
    return RecordInputResponse(**result)


@router.get("/{case_id}/records", tags=["Ledger"])
def get_records(case_id: str) -> list[RecordInputResponse]:
    """
    PL: Zwraca listę wpisów dla danego case_id.
    EN: Returns all entries for the given case_id.
    """
    items = ledger_service.get_records_for_case(case_id=case_id)
    return [RecordInputResponse(**it) for it in items]


@router.get("/{case_id}/prove", tags=["Ledger"])
def prove_case(case_id: str) -> dict[str, Any]:
    """
    PL: Generuje i (jeśli możliwe) waliduje Provenance Receipt dla sprawy.
    EN: Generates and (if available) validates the Provenance Receipt for a case.
    """
    try:
        receipt = ledger_service.build_provenance_receipt(case_id=case_id)
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e)) from e

    if _provenance_validator is not None:
        try:
            _provenance_validator.validate(receipt)  # type: ignore[union-attr]
        except Exception as e:
            # 500: service error (receipt doesn't match contract)
            raise HTTPException(
                status_code=500,
                detail=f"Provenance receipt schema validation failed: {e}",
            ) from e

    return receipt

```


===== FILE: services/api_gateway/routers/mismatch.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/mismatch.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Router FastAPI dla usług CERTEUS.
EN: FastAPI router for CERTEUS services.
"""

from __future__ import annotations

from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from services.mismatch_service.models import TicketPriority
from services.mismatch_service.service import mismatch_service

router = APIRouter(prefix="/mismatch", tags=["mismatch"])


class EngineResult(BaseModel):
    status: str
    time_ms: float | None = None
    model: dict[str, Any] | None = None
    error: str | None = None
    version: str | None = None


class MismatchCreateRequest(BaseModel):
    case_id: str
    formula_str: str
    results: dict[str, EngineResult]
    formula_ast: dict[str, Any] | None = None
    priority: TicketPriority | None = Field(default=None)


@router.post("/tickets")
def create_ticket(req: MismatchCreateRequest) -> dict[str, Any]:
    t = mismatch_service.create_ticket(
        case_id=req.case_id,
        formula_str=req.formula_str,
        results={k: v.model_dump() for k, v in req.results.items()},
        formula_ast=req.formula_ast,
        priority=req.priority,
    )
    return t.model_dump()


@router.get("/tickets/{ticket_id}")
def get_ticket(ticket_id: str) -> dict[str, Any]:
    t = mismatch_service.get_ticket(ticket_id)
    if not t:
        raise HTTPException(status_code=404, detail="Ticket not found")
    return t.model_dump()

```


===== FILE: services/api_gateway/routers/pco_public.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/api_gateway/routers/pco_public.py          |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: Public PCO (zero PII). Verifies Merkle (MVP/real) + Ed25519.   |
# |  PL: Publiczny PCO (0 PII). Weryfikuje Merkle (MVP/real) + Ed25519. |
# +=====================================================================+
"""
PL:
  Endpoint: GET /pco/public/{rid}
  Payload (publiczny, 0 PII):
    { rid, smt2_hash, lfsc, [drat?], merkle_proof, signature }
  Weryfikacje:
    1) Brak PII (denylists: klucze)
    2) Merkle: leaf = sha256( sha256(rid) || bundle_hash );
       root = apply(path) lub leaf gdy path=[]
       Akceptowane formaty `merkle_proof`:
         • [] (lista kroków) – MVP
         • [{"sibling":HEX, "dir":"L|R"}] – pełna ścieżka
         • {"path":[...]} – zgodność wstecz
         • alias 'position' ≡ 'dir'
    3) Ed25519: detached signature (base64url) nad kanonicznym skrótem
       digest_hex = sha256(
         sha256(rid) || smt2_hash || sha256(lfsc) || [sha256(drat)?] || merkle_root
       )

EN:
  Endpoint: GET /pco/public/{rid}
  Public payload (zero PII): as above.
  Validates zero-PII keys, Merkle proof (MVP or full), and Ed25519 signature.
"""

# ----Bloki----- IMPORTY
from __future__ import annotations

# stdlib
import base64
import hashlib
import json
import os
from pathlib import Path
from typing import Any, TypedDict

# third-party
from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel, Field, field_validator

# ----Bloki----- KONFIGURACJA
router = APIRouter(prefix="/pco/public", tags=["pco"])

# zero-PII: klucze ZAKAZANE (tylko nazwy kluczy; nie analizujemy treści)
FORBIDDEN_KEYS = {
    "name",
    "first_name",
    "last_name",
    "pesel",
    "email",
    "phone",
    "address",
    "dob",
    "ssn",
    "patient_id",
    "person_id",
    "user_id",
    "ip",
    "session_id",
    "headers",
}

DEFAULT_BUNDLE_DIR_FALLBACK = Path("./data/public_pco")


def _bundle_dir() -> Path:
    """Resolve bundle dir at call time, honoring current ENV."""
    return Path(os.getenv("PROOF_BUNDLE_DIR") or DEFAULT_BUNDLE_DIR_FALLBACK)


# ----Bloki----- MODELE
class MerkleStep(BaseModel):
    sibling: str  # hex
    dir: str  # "L" or "R"


class PublicPCO(BaseModel):
    rid: str = Field(..., min_length=3)
    smt2_hash: str = Field(..., min_length=64, max_length=64)  # hex sha256 of SMT2
    lfsc: str = Field(..., min_length=2)  # LFSC (plain text)
    drat: str | None = None  # DRAT (plain text, optional)
    # Akceptujemy listę kroków; parser przyjmuje też {"path":[...]} (zgodność wstecz)
    merkle_proof: list[MerkleStep] = Field(default_factory=list)
    signature: str = Field(..., min_length=40)  # detached (base64url, bez '=')

    @field_validator("smt2_hash")
    @classmethod
    def _hex64(cls, v: str) -> str:
        int(v, 16)  # raises on invalid hex
        return v.lower()


# ----Bloki----- MERKLE (narzędzia)
class _MerkleStepDict(TypedDict, total=False):
    sibling: str
    dir: str
    position: str  # alias


def _h(b: bytes) -> bytes:
    return hashlib.sha256(b).digest()


def _sha256_hex_utf8(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _compute_bundle_hash_hex(pub: dict[str, Any]) -> str:
    # Kanoniczny JSON (sort_keys, compact) → stabilny hash danych (bez sygnatury)
    payload: dict[str, str] = {
        "smt2_hash": str(pub["smt2_hash"]).lower(),
        "lfsc_sha256": _sha256_hex_utf8(str(pub["lfsc"])),
    }
    if pub.get("drat") is not None:
        payload["drat_sha256"] = _sha256_hex_utf8(str(pub["drat"]))
    blob = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
    return hashlib.sha256(blob).hexdigest()


def compute_leaf_hex(rid: str, bundle_hash_hex: str) -> str:
    """
    EN/PL: Leaf = sha256( sha256(rid) || bundle_hash ), hex-encoded.
    Użyj w testach/narzędziach, aby wyliczyć liść spójnie z endpointem.
    """
    rid_hash = hashlib.sha256(rid.encode("utf-8")).digest()
    bundle_hash = bytes.fromhex(bundle_hash_hex)
    return hashlib.sha256(rid_hash + bundle_hash).hexdigest()


def _apply_merkle_path(leaf_hex: str, path: list[MerkleStep]) -> str:
    cur = bytes.fromhex(leaf_hex)
    for step in path:
        sib = bytes.fromhex(step.sibling)
        if step.dir == "L":
            cur = _h(sib + cur)
        elif step.dir == "R":
            cur = _h(cur + sib)
        else:
            raise HTTPException(status_code=400, detail=f"Invalid merkle step.dir: {step.dir}")
    return cur.hex()


def _parse_merkle_proof(raw: object) -> list[MerkleStep]:
    """
    Akceptuj:
      • [] (MVP)
      • [{"sibling":..., "dir":"L|R"}]
      • {"path":[...]} (zgodność wstecz)
      • alias 'position' ≡ 'dir'
    """
    if raw is None:
        return []
    if isinstance(raw, dict) and "path" in raw:
        raw = raw["path"]
    if isinstance(raw, list):
        norm: list[MerkleStep] = []
        for step in raw:
            if isinstance(step, MerkleStep):  # już zparsowany
                norm.append(step)
                continue
            s: _MerkleStepDict = step  # type: ignore[assignment]
            d = s.get("dir") or s.get("position")
            sib = s.get("sibling")
            if not isinstance(d, str) or d not in ("L", "R"):
                raise HTTPException(status_code=400, detail="Invalid merkle step.dir/position")
            if not isinstance(sib, str) or not sib:
                raise HTTPException(status_code=400, detail="Invalid merkle step: missing 'sibling'")
            # pydantic zweryfikuje format heksadecymalny
            norm.append(MerkleStep(sibling=sib, dir=d))
        return norm
    raise HTTPException(status_code=400, detail="merkle_proof must be list or {path:[...] }")


# ----Bloki----- PODPIS Ed25519
def _b64u_decode(s: str) -> bytes:
    pad = "=" * (-len(s) % 4)
    return base64.urlsafe_b64decode(s + pad)


def _load_pubkey_bytes() -> bytes:
    b64u = os.getenv("ED25519_PUBKEY_B64URL")
    if b64u:
        return _b64u_decode(b64u)
    hexv = os.getenv("ED25519_PUBKEY_HEX")
    if hexv:
        return bytes.fromhex(hexv)
    raise HTTPException(status_code=500, detail="Missing ED25519_PUBKEY_B64URL or ED25519_PUBKEY_HEX")


def _verify_signature(signature_b64u: str, msg_hex: str) -> None:
    try:
        from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PublicKey
    except Exception as e:  # pragma: no cover
        raise HTTPException(status_code=500, detail="Missing 'cryptography' for Ed25519") from e
    pub = Ed25519PublicKey.from_public_bytes(_load_pubkey_bytes())
    try:
        pub.verify(_b64u_decode(signature_b64u), bytes.fromhex(msg_hex))
    except Exception as e:
        raise HTTPException(status_code=400, detail="Invalid signature") from e


def _canonical_digest_hex(pub: dict[str, Any], merkle_root_hex: str) -> str:
    parts = [
        hashlib.sha256(str(pub["rid"]).encode("utf-8")).hexdigest(),
        str(pub["smt2_hash"]).lower(),
        _sha256_hex_utf8(str(pub["lfsc"])),
    ]
    if pub.get("drat") is not None:
        parts.append(_sha256_hex_utf8(str(pub["drat"])))
    parts.append(merkle_root_hex.lower())
    data = b"".join(bytes.fromhex(p) for p in parts)
    return hashlib.sha256(data).hexdigest()


# ----Bloki----- STORAGE
def _bundle_path(rid: str) -> Path:
    return _bundle_dir() / f"{rid}.json"


def _load_public_bundle_from_fs(rid: str) -> dict[str, Any]:
    p = _bundle_path(rid)
    if not p.exists():
        raise HTTPException(status_code=404, detail="PCO bundle not found")
    try:
        raw = p.read_text(encoding="utf-8")
        data = json.loads(raw)
        if not isinstance(data, dict):
            raise ValueError("Bundle is not an object")
        return data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cannot read bundle: {e}") from e


# ----Bloki----- PII
def _assert_no_pii(bundle: dict[str, Any]) -> None:
    bad = sorted(set(bundle.keys()) & FORBIDDEN_KEYS)
    if bad:
        raise HTTPException(status_code=422, detail=f"PII field(s) present: {bad}")


# ----Bloki----- ENTRYPOINT
@router.get("/{rid}", response_model=PublicPCO)
def get_public_pco(rid: str, request: Request) -> PublicPCO:
    """
    EN/PL: Returns public PCO; validates zero-PII, Merkle proof, and Ed25519 signature.
    """
    pub = _load_public_bundle_from_fs(rid)
    _assert_no_pii(pub)

    bundle_hash_hex = _compute_bundle_hash_hex(pub)
    leaf_hex = compute_leaf_hex(pub["rid"], bundle_hash_hex)
    path = _parse_merkle_proof(pub.get("merkle_proof"))
    merkle_root_hex = _apply_merkle_path(leaf_hex, path)

    digest_hex = _canonical_digest_hex(pub, merkle_root_hex)
    _verify_signature(pub["signature"], digest_hex)

    # Pydantic dodatkowo sanityzuje typy/formaty
    return PublicPCO(**{**pub, "merkle_proof": path})

```


===== FILE: services/api_gateway/routers/preview.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/api_gateway/routers/preview.py             |
# | ROLE / ROLA:                                                         |
# |  EN: Stub preview endpoint for uploaded files (PDF/DOCX/TXT/IMG).   |
# |  PL: Endpoint podglądu wgranych plików (PDF/DOCX/TXT/IMG).          |
# +=====================================================================+

"""PL: Router podglądu plików. EN: Preview router."""

from __future__ import annotations

import shutil
import uuid
from pathlib import Path
from typing import Annotated

from fastapi import APIRouter, File, UploadFile
from fastapi.responses import JSONResponse

router = APIRouter()

STATIC_PREV = Path("static/previews")
STATIC_PREV.mkdir(parents=True, exist_ok=True)


@router.post("/v1/preview")
async def preview(file: Annotated[UploadFile, File(...)]) -> JSONResponse:
    """
    PL: Zwraca URL do podglądu pliku. Na razie zapisuje pod /static/previews/.
    EN: Returns a URL for preview. For now writes into /static/previews/.
    """
    raw_name: str = file.filename or "upload.bin"  # UploadFile.filename can be None
    ext = Path(raw_name).suffix.lower()

    safe_name = f"{uuid.uuid4().hex}{ext}"
    dst = STATIC_PREV / safe_name

    try:
        with dst.open("wb") as out:
            shutil.copyfileobj(file.file, out)
    finally:
        await file.close()

    return JSONResponse({"url": f"/static/previews/{safe_name}"})

```


===== FILE: services/api_gateway/routers/publish.py =====
```text
# +=====================================================================+
# |                          CERTEUS — HEART                            |
# +=====================================================================+
# | FILE: services/api_gateway/routers/publish.py                       |
# | ROLE:                                                               |
# |  PL: Publiczny endpoint publikacji (kontrakt Publication V1).       |
# |  EN: Public publication endpoint (Publication V1 contract).         |
# +=====================================================================+

"""PL: Mapuje wynik rdzenia na kontrakt publikacji. EN: Map core to publication contract."""

from __future__ import annotations

from typing import Any

from fastapi import APIRouter, Header

from core.truthops.engine import post_solve, pre_solve
from runtime.proof_queue import PROOF_QUEUE

router = APIRouter()


@router.post("/defx/reason")
def reason(
    body: dict[str, Any],
    x_norm_pack_id: str = Header(..., alias="X-Norm-Pack-ID"),
    x_jurisdiction: str = Header(..., alias="X-Jurisdiction"),
) -> dict[str, Any]:
    """PL: Zwraca status + PCO/plan/eta_hint. EN: Returns status + PCO/plan/eta_hint."""
    pre = pre_solve(body, policy_profile="default")
    if pre.heat != "HOT":
        task = PROOF_QUEUE.enqueue(
            tenant=body.get("tenant", "anon"), heat=pre.heat, payload=body, sla=body.get("sla", "basic")
        )
        return {
            "status": "PENDING",
            "proof_task_id": task.id,
            "eta_hint": task.eta_hint,
            "pco.plan": pre.plan,
            "headers": {"X-Norm-Pack-ID": x_norm_pack_id, "X-Jurisdiction": x_jurisdiction},
        }

    artifacts: dict[str, Any] = {}  # plug: wyniki z szybkich solverów
    decision, meta = post_solve(artifacts, policy_profile="default")
    resp: dict[str, Any] = {
        "status": decision,
        "headers": {"X-Norm-Pack-ID": x_norm_pack_id, "X-Jurisdiction": x_jurisdiction},
    }
    if decision == "PUBLISH":
        resp["pco"] = meta.get("pco", {})
    else:
        resp["pco.plan"] = meta.get("plan", {})
    return resp

```


===== FILE: services/api_gateway/routers/system.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/system.py |
# | DATE:    2025-08-17                                                 |
# +=====================================================================+

"""
PL: Router narzędziowy (systemowy). Udostępnia:
    • /v1/ingest  – lekki stub OCR → FACTLOG (z nagłówkiem łańcucha, limitami, MIME),
    • /v1/analyze – minimalny stub analizy E2E (SAT),
    • /v1/sipp/snapshot/{act_id} – migawka aktu prawnego (z `_certeus`).

EN: System/utility router exposing:
    • /v1/ingest  – light OCR → FACTLOG stub (with chain header, limits, MIME),
    • /v1/analyze – minimal E2E analysis stub (SAT),
    • /v1/sipp/snapshot/{act_id} – legal act snapshot (with `_certeus`).
"""

from __future__ import annotations

import json
from datetime import datetime, timezone
from hashlib import sha256
from typing import Annotated, Any

from fastapi import APIRouter, File, HTTPException, Response, UploadFile
from pydantic import BaseModel, Field

from services.ingest_service.adapters.contracts import Blob
from services.ingest_service.adapters.ocr_injector import build_ocr_preview

router = APIRouter(tags=["system"])

# ---------------------------------------------------------------------
# /v1/ingest
# ---------------------------------------------------------------------


class IngestResult(BaseModel):
    kind: str = Field(default="fact")
    role: str
    source: str
    value: str
    fact_id: str


@router.post("/v1/ingest")
async def ingest_document(
    response: Response,
    file: Annotated[UploadFile, File(...)],
) -> list[dict[str, Any]]:
    """
    PL: Zwraca dwa deterministyczne fakty (z `fact_id`, `thesis`, `confidence_score`),
        waliduje MIME/rozmiar i ustawia nagłówek łańcucha `X-CERTEUS-Ledger-Chain`.
        Dodatkowo (nieinwazyjnie) umieszcza skrót OCR w nagłówku
        `X-CERTEUS-OCR-Preview` dla PDF/obrazów.
    EN: Returns two deterministic facts (with `fact_id`, `thesis`, `confidence_score`),
        validates MIME/size, and sets `X-CERTEUS-Ledger-Chain` header. Also (non-
        invasive) puts OCR snippet into `X-CERTEUS-OCR-Preview` header for PDF/images.
    """
    MAX_BYTES = 10 * 1024 * 1024  # 10 MiB
    allowed_mime = "application/pdf"

    if (file.content_type or "") != allowed_mime:
        raise HTTPException(status_code=415, detail="Only application/pdf is supported")

    content = await file.read()
    if len(content) > MAX_BYTES:
        raise HTTPException(status_code=413, detail="File too large")

    src = file.filename or "document.pdf"

    def make_fact(role: str, value: str) -> dict[str, Any]:
        base = f"{src}|{role}|{value}".encode()
        fid = "fact-" + sha256(base).hexdigest()[:12]
        return {
            "kind": "fact",
            "role": role,
            "source": src,
            "value": value,
            "fact_id": fid,
        }

    facts = [
        make_fact("claim_contract_date", "2023-10-01"),
        make_fact("evidence_payment", "TAK"),
    ]

    # Uzupełnij wymagane pola (thesis + confidence)
    for f in facts:
        if f["role"] == "claim_contract_date":
            f["thesis"] = "Umowa została zawarta 2023-10-01."
            f["confidence_score"] = 0.98
        elif f["role"] == "evidence_payment":
            f["thesis"] = "Istnieje dowód wpłaty."
            f["confidence_score"] = 0.99

    # Nagłówek łańcucha: "sha256:<...>;sha256:<...>"
    chain_parts: list[str] = []
    for f in facts:
        payload = json.dumps(f, ensure_ascii=False, sort_keys=True).encode("utf-8")
        chain_parts.append("sha256:" + sha256(payload).hexdigest())
    response.headers["X-CERTEUS-Ledger-Chain"] = ";".join(chain_parts)

    # DODATEK: OCR preview jako nagłówek (bez zmiany body).
    try:
        blob = Blob(
            filename=src,
            content_type=file.content_type or "application/octet-stream",
            data=content,
        )
        ocr = await build_ocr_preview(blob, case_id=None, max_chars=160)
        preview = ocr.get("ocr_preview")
        if preview:
            # Krótki, jednowierszowy nagłówek (bez znaków nowych linii).
            response.headers["X-CERTEUS-OCR-Preview"] = " ".join(preview.split())
    except Exception:
        # Bezpieczne pominięcie OCR w razie błędu stubu.
        pass

    return facts


# ---------------------------------------------------------------------
# /v1/analyze
# ---------------------------------------------------------------------


@router.post("/v1/analyze")
async def analyze(case_id: str, file: Annotated[UploadFile, File(...)]) -> dict[str, Any]:
    """
    Minimalny stub E2E: przyjmuje PDF i zwraca wynik SAT z prostym modelem.
    Zgodne z tests/e2e/test_e2e_pl_286kk_0001.py.
    """
    await file.read()  # nieużywane w stubie
    return {
        "case_id": case_id,
        "analysis_result": {"status": "sat", "model": "[x=True]"},
    }


# ---------------------------------------------------------------------
# /v1/sipp/snapshot/{act_id}
# ---------------------------------------------------------------------


@router.get("/v1/sipp/snapshot/{act_id}")
async def get_snapshot(act_id: str) -> dict[str, Any]:
    """Zwraca minimalny snapshot (dict), aby dozwolić klucz `_certeus`."""
    text = (
        "Art. 286 k.k.: Kto, w celu osiągnięcia korzyści majątkowej, doprowadza inną osobę "
        "do niekorzystnego rozporządzenia mieniem za pomocą wprowadzenia w błąd..."
    ).strip()
    digest = "sha256:" + sha256(text.encode("utf-8")).hexdigest()
    snap_ts = datetime.now(timezone.utc).isoformat(timespec="seconds")

    return {
        "act_id": act_id,
        "version_id": "2023-10-01",
        "title": "Kodeks karny – art. 286",
        "source_url": ("https://isap.sejm.gov.pl/isap.nsf/DocDetails.xsp?id=WDU19970880553"),
        "text": text,
        "text_sha256": digest,
        "at": None,
        "snapshot_timestamp": snap_ts,
        "_certeus": {"snapshot_timestamp_utc": snap_ts},
    }

```


===== FILE: services/api_gateway/routers/verify.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/api_gateway/routers/verify.py  |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: services/api_gateway/routers/verify.py                |
# | ROLE: Expose /v1/verify endpoint (Truth Engine).            |
# | PLIK: services/api_gateway/routers/verify.py                |
# | ROLA: Udostępnia endpoint /v1/verify (Silnik Prawdy).       |
# +-------------------------------------------------------------+
"""
PL: Publiczny endpoint do weryfikacji formuł SMT-LIB2 przez Silnik Prawdy.
EN: Public endpoint to verify SMT-LIB2 formulas via the Truth Engine.
"""

from __future__ import annotations

# === IMPORTY / IMPORTS ======================================== #
from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from kernel.mismatch_protocol import MismatchError
from kernel.truth_engine import DualCoreVerifier

# === ROUTER / ROUTER ========================================== #
router = APIRouter(prefix="/v1", tags=["Truth Engine"])
_verifier = DualCoreVerifier()


# === DTO / MODELE DANYCH ====================================== #
class VerificationRequest(BaseModel):
    """
    PL: Wejściowy DTO do weryfikacji (MVP: tylko 'smt2').
    EN: Input DTO for verification (MVP: 'smt2' only).
    """

    formula: str
    lang: str = "smt2"


# === ENDPOINTY / ENDPOINTS ==================================== #
@router.post("/verify")
def verify_formula(req: VerificationRequest) -> dict[str, Any]:
    """
    PL: Weryfikuje formułę. Zwraca sat/unsat/unknown oraz artefakty (model/proof).
    EN: Verifies the formula. Returns sat/unsat/unknown and artifacts (model/proof).
    """
    try:
        # lang w DualCoreVerifier.verify jest parametrem *keyword-only*.
        return _verifier.verify(req.formula, lang=req.lang)
    except MismatchError as e:
        raise HTTPException(
            status_code=409,
            detail={"requires_human": True, "message": str(e)},
        ) from e
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e)) from e
    except Exception as e:  # pragma: no cover
        raise HTTPException(status_code=500, detail=f"Unexpected error: {e}") from e


# === KONIEC / END ============================================= #

```


===== FILE: services/api_gateway/routers/well_known_jwks.py =====
```text
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE: services/api_gateway/routers/.well_known_jwks.py            |
# | DATE:   2025-08-19                                                  |
# +=====================================================================+
"""
PL: Endpoint publikujący klucz publiczny Ed25519 w formacie JWKS (/.well-known/jwks.json).
EN: Endpoint exposing Ed25519 public key via JWKS (/.well-known/jwks.json).
"""

# ----Bloki----- IMPORTY
from __future__ import annotations

import base64
import hashlib
import os

from fastapi import APIRouter, HTTPException

# ----Bloki----- KONFIGURACJA
router = APIRouter(prefix="", tags=["well-known"])


def _b64u(data: bytes) -> str:
    return base64.urlsafe_b64encode(data).rstrip(b"=").decode("ascii")


def _load_pubkey_bytes() -> bytes:
    b64u = os.getenv("ED25519_PUBKEY_B64URL")
    if b64u:
        pad = "=" * (-len(b64u) % 4)
        return base64.urlsafe_b64decode(b64u + pad)
    hexv = os.getenv("ED25519_PUBKEY_HEX")
    if hexv:
        return bytes.fromhex(hexv)
    raise RuntimeError("Brak klucza publicznego: ustaw ED25519_PUBKEY_B64URL lub ED25519_PUBKEY_HEX")


def _kid_from_key(pub: bytes) -> str:
    return hashlib.sha256(pub).hexdigest()[:16]


# ----Bloki----- ENTRYPOINT
@router.get("/.well-known/jwks.json")
def jwks():
    try:
        pub = _load_pubkey_bytes()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e)) from e
    kid = _kid_from_key(pub)
    jwk = {
        "kty": "OKP",
        "crv": "Ed25519",
        "x": _b64u(pub),
        "kid": kid,
        "use": "sig",
        "alg": "EdDSA",
    }
    return {"keys": [jwk]}

```


===== FILE: services/exporter_service/__init__.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/exporter_service/__init__.py   |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Pakiet inicjalizacyjny modułu.
EN: Package initializer.
"""

from __future__ import annotations

from .exporter import ExporterService, export_answer, export_answer_to_txt

__all__ = ["ExporterService", "export_answer_to_txt", "export_answer"]

```


===== FILE: services/exporter_service/exporter.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/exporter_service/exporter.py   |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Eksport raportów i artefaktów procesu.
EN: Report/artefact exporter.
"""

from __future__ import annotations

import json
from collections.abc import Mapping
from pathlib import Path
from typing import Any

# (hash helpers są w ledger_service, ale nie są tu potrzebne do samego eksportu)

TEMPL_REPORT = """# CERTEUS Report
Case: {case_id}
Status: {status}
Model: {model}
"""


class ExporterService:
    def __init__(self, template_dir: str, output_dir: str) -> None:
        self.template_dir = Path(template_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def export_report(self, case_id: str, analysis: dict[str, Any]) -> Path:
        status = str(analysis.get("status", "")).upper()
        model = analysis.get("model", "")
        content = TEMPL_REPORT.format(case_id=case_id, status=status, model=model)
        out = self.output_dir / f"{case_id}.txt"
        out.write_text(content, encoding="utf-8")
        return out


def export_answer_to_txt(
    answer: Mapping[str, Any], *, out_path: str, create_ledger_entry: bool = False
) -> str:
    p = Path(out_path)
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(answer, indent=2, sort_keys=True), encoding="utf-8")
    return str(p)


def export_answer(answer: Mapping[str, Any], *, fmt: str, output_dir: Path | None = None):
    """
    - fmt="json": return pretty json string
    - fmt="file": write <case_id>.json to output_dir, return Path
    - fmt="docx": write placeholder .docx (text), return Path
    """
    if fmt == "json":
        return json.dumps(answer, indent=2, sort_keys=True)

    outdir = output_dir or Path("build/exports")
    outdir.mkdir(parents=True, exist_ok=True)
    case_id = str(answer.get("case_id", "case"))

    if fmt == "file":
        p = outdir / f"{case_id}.json"
        p.write_text(json.dumps(answer, indent=2, sort_keys=True), encoding="utf-8")
        return p

    if fmt == "docx":
        p = outdir / f"{case_id}.docx"
        p.write_text(json.dumps(answer, indent=2, sort_keys=True), encoding="utf-8")
        return p

    raise ValueError(f"Unsupported fmt: {fmt}")

```


===== FILE: services/ingest_service/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ingest_service/__init__.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: services/ingest_service/__init__.py
# License: Apache-2.0
# Description (PL): Pakiet serwisu ingestii (FACTLOG): eksportuje główne typy.
# Description (EN): Ingestion service package (FACTLOG): exports main types.
# Style Guide: ASCII header, PL/EN docs, labeled code blocks. (See README)
# ────────────────────────────────────────────────────────────────────────────────

"""
PL: Pakiet serwisu ingestii. Ten moduł oznacza pakiet Pythona oraz
    udostępnia publiczny interfejs (eksport) najczęściej używanych typów.
EN: Ingestion service package. This module marks the Python package and
    exposes a public interface (exports) of the most commonly used types.
"""

# [BLOCK: PUBLIC API EXPORTS / EKSPORT INTERFEJSU PUBLICZNEGO]
from .models import Fact, FactRole

__all__ = ["Fact", "FactRole"]

```


===== FILE: services/ingest_service/adapters/__init__.py =====
```text
# =============================================================================
#  CERTEUS — adapters package
#  PL: Adaptery (Preview/OCR/Drive/LLM).
#  EN: Adapters (Preview/OCR/Drive/LLM).
# =============================================================================
"""
PL: Pakiet adapterów (Preview/OCR/Drive/LLM).
EN: Adapters package (Preview/OCR/Drive/LLM).
"""

```


===== FILE: services/ingest_service/adapters/contracts.py =====
```text
# =============================================================================
#  CERTEUS — Adapters Contracts
#  Preview/OCR/Drive/LLM interfaces (Protocols) + lightweight DTOs
# =============================================================================
#  PL
#  Ten moduł definiuje kontrakty (interfejsy) adapterów używanych przez API:
#  - PreviewAdapter: DOCX→PDF (i ogólny preview), zwraca PreviewResult
#  - OCRAdapter: ekstrakcja tekstu z PDF/obrazów (stub na start)
#  - DriveAdapter: prosty storage (np. lokalny /static albo chmura)
#  - LLMAdapter: analiza/inspekcja treści (stub ALI pod /v1/analyze)
#
#  Zasady:
#  - Proste DTO z dataclasses (bez zależności pydantic).
#  - Asynchroniczne metody (spójne z FastAPI).
#  - Linie do 100 znaków, LF, brak trailing whitespace.
#  - Wyjątki dziedziczą po AdapterError.
#
#  EN
#  This module defines adapter contracts used by the API layer:
#  - PreviewAdapter: DOCX→PDF (generic preview), returns PreviewResult
#  - OCRAdapter: text extraction from PDFs/images (stub initially)
#  - DriveAdapter: storage abstraction (local /static or cloud)
#  - LLMAdapter: analysis/inspection (ALI stub used by /v1/analyze)
#
#  Rules:
#  - Lightweight DTOs via dataclasses (no pydantic dependency).
#  - Async methods (compatible with FastAPI).
#  - 100-char lines, LF endings, no trailing whitespace.
#  - Exceptions derive from AdapterError.
# =============================================================================
"""
PL: Kontrakty adapterów i lekkie DTO (Preview/OCR/Drive/LLM) używane przez API.
EN: Adapter contracts and lightweight DTOs (Preview/OCR/Drive/LLM) used by the API.
"""

from __future__ import annotations

from collections.abc import Iterable, Mapping, Sequence
from dataclasses import dataclass, field
from typing import Any, Literal, Protocol

# ----------------------------- Common DTOs ----------------------------------


@dataclass(slots=True)
class Blob:
    """
    PL: Surowe dane pliku.
    EN: Raw file payload.
    """

    filename: str
    content_type: str
    data: bytes


@dataclass(slots=True)
class PreviewRequest:
    """
    PL: Wejście do adaptera preview. 'target_format' np. 'application/pdf'.
    EN: Input for preview adapter. 'target_format' e.g. 'application/pdf'.
    """

    blob: Blob
    case_id: str
    target_format: str = "application/pdf"
    deterministic: bool = True


@dataclass(slots=True)
class PreviewResult:
    """
    PL: Wynik generowania podglądu. 'url' wskazuje zasób serwowany przez API.
    EN: Preview generation result. 'url' points to API-served resource.
    """

    url: str
    content_type: str
    pages: int | None = None
    size_bytes: int | None = None
    meta: Mapping[str, Any] = field(default_factory=dict)


@dataclass(slots=True)
class OCRPage:
    """
    PL: Tekst i proste metadane strony po OCR.
    EN: OCR'ed page text and basic metadata.
    """

    index: int
    text: str
    width_px: int | None = None
    height_px: int | None = None
    meta: Mapping[str, Any] = field(default_factory=dict)


@dataclass(slots=True)
class OCRRequest:
    """
    PL: Wejście do OCR; jeśli PDF → stronicowanie wewnątrz adaptera.
    EN: OCR input; if PDF → paging handled inside the adapter.
    """

    blob: Blob
    lang_hint: str = "pl+en"
    dpi: int = 300
    max_pages: int | None = None
    case_id: str | None = None


@dataclass(slots=True)
class DriveSaveResult:
    """
    PL: Wynik zapisu do storage (lokalny/chmura).
    EN: Storage save result (local/cloud).
    """

    file_id: str
    url: str | None = None
    size_bytes: int | None = None
    content_type: str | None = None
    meta: Mapping[str, Any] = field(default_factory=dict)


@dataclass(slots=True)
class Attachment:
    """
    PL: Załącznik do analizy LLM (np. tekst OCR).
    EN: Attachment for LLM analysis (e.g., OCR text).
    """

    name: str
    kind: Literal["text", "preview_url", "binary"]
    content: str | bytes
    content_type: str | None = None


@dataclass(slots=True)
class LLMRequest:
    """
    PL: Wejście do adaptera LLM.
    EN: Input for LLM adapter.
    """

    prompt: str
    attachments: Sequence[Attachment] = ()
    case_id: str | None = None
    model_hint: str | None = None
    temperature: float = 0.0
    max_tokens: int | None = None


@dataclass(slots=True)
class LLMResponse:
    """
    PL: Zunifikowany wynik LLM do /v1/analyze (stub ALI).
    EN: Unified LLM result for /v1/analyze (ALI stub).
    """

    status: Literal["ok", "error"]
    model: str
    answer: Mapping[str, Any]
    trace: Sequence[str] = ()
    provenance: Mapping[str, Any] = field(default_factory=dict)
    error: str | None = None


# ------------------------------- Errors -------------------------------------


class AdapterError(RuntimeError):
    """PL: Błąd ogólny adapterów. EN: Generic adapters error."""


class PreviewError(AdapterError):
    """PL/EN: Preview adapter error."""


class OCRError(AdapterError):
    """PL/EN: OCR adapter error."""


class DriveError(AdapterError):
    """PL/EN: Drive/storage adapter error."""


class LLMError(AdapterError):
    """PL/EN: LLM adapter error."""


# ------------------------------ Interfaces ----------------------------------


class PreviewAdapter(Protocol):
    """
    PL:
      Interfejs generowania podglądu (np. DOCX→PDF).
      Kontrakt:
        - Wejście: PreviewRequest
        - Wyjście: PreviewResult (URL serwowany przez API, np. /static/previews/..)
        - Determinizm: dla tych samych danych i case_id ścieżka może być stała.
    EN:
      Preview generation interface (e.g., DOCX→PDF).
      Contract:
        - Input: PreviewRequest
        - Output: PreviewResult (API-served URL, e.g., /static/previews/..)
        - Determinism: stable path for same input + case_id (optional).
    """

    async def generate(self, request: PreviewRequest) -> PreviewResult:
        """PL/EN: Produce preview for given blob."""
        ...


class OCRAdapter(Protocol):
    """
    PL:
      Interfejs OCR dla PDF/obrazów.
      Kontrakt:
        - Wejście: OCRRequest
        - Wyjście: Iterable[OCRPage] (kolejność stron gwarantowana)
        - Minimalna gwarancja stubu: zwróć 1 stronę z prostym 'text' (echo/heurystyka)
    EN:
      OCR interface for PDF/images.
      Contract:
        - Input: OCRRequest
        - Output: Iterable[OCRPage] (page order guaranteed)
        - Stub minimum guarantee: return 1 page with basic 'text' (echo/heuristic)
    """

    async def extract(self, request: OCRRequest) -> Iterable[OCRPage]:
        """PL/EN: Extract text pages from the blob (PDF/image)."""
        ...


class DriveAdapter(Protocol):
    """
    PL:
      Abstrakcja storage (lokalny katalog /static, S3, GDrive, itd.).
      Kontrakt:
        - save_bytes: zapisuje dane i zwraca logiczne ID oraz (opcjonalnie) URL
        - read_bytes: pobiera blob po file_id
        - url_for: zwraca publiczny URL jeśli dostępny
    EN:
      Storage abstraction (local /static, S3, GDrive, etc.).
      Contract:
        - save_bytes: persists payload and returns logical ID and optional URL
        - read_bytes: fetches payload by file_id
        - url_for: returns public URL if available
    """

    async def save_bytes(
        self,
        data: bytes,
        *,
        filename: str,
        content_type: str,
        case_id: str | None = None,
        deterministic: bool = True,
    ) -> DriveSaveResult:
        """PL/EN: Persist a payload and return its handle."""
        ...

    async def read_bytes(self, file_id: str) -> bytes:
        """PL/EN: Retrieve raw bytes by logical file id."""
        ...

    async def url_for(self, file_id: str) -> str | None:
        """PL/EN: Optional public URL for given file id."""
        ...


class LLMAdapter(Protocol):
    """
    PL:
      Interfejs do warstwy analitycznej LLM (stub ALI).
      Kontrakt:
        - analyze: zwraca LLMResponse zgodny z oczekiwaniem /v1/analyze
        - Implementacja stub: deterministyczny 'model', krótki 'trace', 'answer'
    EN:
      Interface to LLM analytical layer (ALI stub).
      Contract:
        - analyze: returns LLMResponse aligned with /v1/analyze expectations
        - Stub impl: deterministic 'model', short 'trace', 'answer'
    """

    async def analyze(self, request: LLMRequest) -> LLMResponse:
        """PL/EN: Perform analysis over prompt + attachments."""
        ...


# ----------------------------- Helper Contracts -----------------------------


def infer_extension(content_type: str, fallback: str = ".bin") -> str:
    """
    PL: Prosty mapping MIME→rozszerzenie dla ścieżek deterministycznych.
    EN: Simple MIME→extension mapping for deterministic paths.
    """
    mapping = {
        "application/pdf": ".pdf",
        "application/" "vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
        "text/plain": ".txt",
        "image/png": ".png",
        "image/jpeg": ".jpg",
        "image/jpg": ".jpg",
    }
    return mapping.get(content_type.lower(), fallback)


__all__ = [
    "Blob",
    "PreviewRequest",
    "PreviewResult",
    "OCRPage",
    "OCRRequest",
    "DriveSaveResult",
    "Attachment",
    "LLMRequest",
    "LLMResponse",
    "AdapterError",
    "PreviewError",
    "OCRError",
    "DriveError",
    "LLMError",
    "PreviewAdapter",
    "OCRAdapter",
    "DriveAdapter",
    "LLMAdapter",
    "infer_extension",
]

```


===== FILE: services/ingest_service/adapters/local_impl.py =====
```text
# =============================================================================
#  CERTEUS — Local Adapters (stubs)
#  Preview/OCR/Drive/LLM — deterministic, no-cloud implementations
# =============================================================================
#  PL
#  Proste, lokalne implementacje interfejsów adapterów:
#   - LocalDriveAdapter: zapis/odczyt pod ./static
#   - StubPreviewAdapter: "DOCX→PDF" (1-str. PDF placeholder)
#   - StubOCRAdapter: zwraca 1 stronę z heurystycznym tekstem
#   - StubLLMAdapter: deterministyczny ALI-stub do /v1/analyze
#
#  EN
#  Simple local implementations of adapter interfaces:
#   - LocalDriveAdapter: save/read under ./static
#   - StubPreviewAdapter: "DOCX→PDF" (1-page PDF placeholder)
#   - StubOCRAdapter: returns 1 page with heuristic text
#   - StubLLMAdapter: deterministic ALI stub for /v1/analyze
#
#  Zasady/Rules:
#  - Deterministyczne ID: sha256(data, filename, content_type, case_id, salt)
#  - Tylko stdlib. Linie ≤ 100 znaków, LF, brak trailing spaces.
# =============================================================================
"""
PL: Lokalne implementacje stubów adapterów (bez chmury).
EN: Local stub implementations of adapters (no cloud).
"""

from __future__ import annotations

import hashlib
from collections.abc import Iterable, Sequence
from dataclasses import asdict
from pathlib import Path

from .contracts import (
    DriveAdapter,
    DriveError,
    DriveSaveResult,
    LLMAdapter,
    LLMRequest,
    LLMResponse,
    OCRAdapter,
    OCRError,
    OCRPage,
    OCRRequest,
    PreviewAdapter,
    PreviewError,
    PreviewRequest,
    PreviewResult,
    infer_extension,
)

# ------------------------------ Helpers -------------------------------------


def _stable_hexdigest(*parts: str | bytes) -> str:
    """PL/EN: Deterministic short sha256 hex."""
    h = hashlib.sha256()
    for p in parts:
        if isinstance(p, str):
            p = p.encode("utf-8", "ignore")
        h.update(p)
    return h.hexdigest()[:24]


def _ensure_dir(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


# --------------------------- LocalDriveAdapter ------------------------------


class LocalDriveAdapter(DriveAdapter):
    """
    PL: Lokalny storage zapisujący do ./static; zwraca URL pod /static.
    EN: Local storage saving to ./static; returns URLs under /static.
    """

    def __init__(self, base_dir: str | Path = "static", base_url_prefix: str = "/static"):
        self.base_dir = Path(base_dir)
        self.base_url_prefix = base_url_prefix.rstrip("/")

    async def save_bytes(
        self,
        data: bytes,
        *,
        filename: str,
        content_type: str,
        case_id: str | None = None,
        deterministic: bool = True,
    ) -> DriveSaveResult:
        try:
            salt = "det" if deterministic else _stable_hexdigest(filename)
            file_id = _stable_hexdigest(data, filename, content_type, case_id or "", salt)
            ext = infer_extension(content_type)
            rel = Path("uploads") / (case_id or "general") / f"{file_id}{ext}"
            dst = self.base_dir / rel
            _ensure_dir(dst)
            dst.write_bytes(data)
            url = f"{self.base_url_prefix}/{rel.as_posix()}"
            return DriveSaveResult(
                file_id=str(rel),
                url=url,
                size_bytes=len(data),
                content_type=content_type,
                meta={"deterministic": deterministic},
            )
        except Exception as e:
            raise DriveError("LocalDriveAdapter.save_bytes failed") from e

    async def read_bytes(self, file_id: str) -> bytes:
        try:
            path = self.base_dir / Path(file_id)
            return path.read_bytes()
        except Exception as e:
            raise DriveError(f"LocalDriveAdapter.read_bytes failed: {file_id}") from e

    async def url_for(self, file_id: str) -> str | None:
        # PL: Dla lokalnego storage URL jest deterministyczny.
        # EN: Deterministic URL for local storage.
        rel = Path(file_id)
        return f"{self.base_url_prefix}/{rel.as_posix()}"


# --------------------------- StubPreviewAdapter -----------------------------


class StubPreviewAdapter(PreviewAdapter):
    """
    PL:
      Tworzy minimalny PDF (1 strona) jako placeholder podglądu.
      Nie wykonuje realnej konwersji DOCX→PDF; to most do UI (/v1/preview).
    EN:
      Produces a minimal PDF (1 page) as preview placeholder.
      No real DOCX→PDF; just a bridge to the UI (/v1/preview).
    """

    def __init__(self, drive: LocalDriveAdapter):
        self.drive = drive

    @staticmethod
    def _minimal_pdf(title: str) -> bytes:
        # PL: Mini-PDF wystarczający do smoke-testu w przeglądarce.
        # EN: Tiny PDF good enough for a browser smoke test.
        # Uwaga: nieidealny, ale poprawny syntaktycznie.
        pdf = (
            b"%PDF-1.4\n"
            b"1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\n"
            b"2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\n"
            b"3 0 obj<</Type/Page/Parent 2 0 R/MediaBox[0 0 612 792]/Contents 4 0 R"
            b"/Resources<</Font<</F1 5 0 R>>>>>>endobj\n"
            b"4 0 obj<</Length 55>>stream\nBT\n/F1 24 Tf\n72 720 Td\n("
            + title.encode("latin-1", "ignore")
            + b") Tj\nET\nendstream\nendobj\n"
            b"5 0 obj<</Type/Font/Subtype/Type1/BaseFont/Helvetica>>endobj\n"
            b"xref\n0 6\n0000000000 65535 f \n"
            b"0000000010 00000 n \n0000000060 00000 n \n0000000116 00000 n \n"
            b"0000000276 00000 n \n0000000401 00000 n \n"
            b"trailer<</Size 6/Root 1 0 R>>\nstartxref\n480\n%%EOF\n"
        )
        return pdf

    async def generate(self, request: PreviewRequest) -> PreviewResult:
        try:
            title = f"Preview: {request.blob.filename}"
            pdf_bytes = self._minimal_pdf(title)
            saved = await self.drive.save_bytes(
                pdf_bytes,
                filename=request.blob.filename,
                content_type="application/pdf",
                case_id=request.case_id,
                deterministic=request.deterministic,
            )
            url = await self.drive.url_for(saved.file_id)
            return PreviewResult(
                url=url or "",
                content_type="application/pdf",
                pages=1,
                size_bytes=len(pdf_bytes),
                meta={
                    "source_content_type": request.blob.content_type,
                    "note": "stub preview",
                },
            )
        except Exception as e:
            raise PreviewError("StubPreviewAdapter.generate failed") from e


# ----------------------------- StubOCRAdapter -------------------------------


class StubOCRAdapter(OCRAdapter):
    """
    PL: Zwraca jedną stronę z prostym tekstem na podstawie metadanych pliku.
    EN: Returns single page with simple text based on file metadata.
    """

    async def extract(self, request: OCRRequest) -> Iterable[OCRPage]:
        try:
            text = (
                f"[STUB_OCR]\nfilename={request.blob.filename}\n"
                f"content_type={request.blob.content_type}\n"
                f"bytes={len(request.blob.data)}\nlang_hint={request.lang_hint}\n"
            )
            return [OCRPage(index=0, text=text, meta={"stub": True})]
        except Exception as e:
            raise OCRError("StubOCRAdapter.extract failed") from e


# ----------------------------- StubLLMAdapter -------------------------------


class StubLLMAdapter(LLMAdapter):
    """
    PL: Deterministyczny stub. Nie łączy się z żadnym LLM.
    EN: Deterministic stub. Does not call any LLM.
    """

    def __init__(self, model_name: str = "ALI-Stub-0"):
        self.model_name = model_name

    async def analyze(self, request: LLMRequest) -> LLMResponse:
        try:
            att_summary: list[str] = []
            for a in request.attachments:
                if isinstance(a.content, bytes | bytearray):
                    size = len(a.content)
                else:
                    size = len(str(a.content))
                att_summary.append(f"{a.kind}:{a.name}:{size}")

            answer = {
                "summary": {"prompt_len": len(request.prompt), "attachments": att_summary},
                "satisfied": [],
                "missing": [],
            }
            trace: Sequence[str] = ("stub:init", "stub:analyze", "stub:done")
            provenance = {
                "adapter": "StubLLMAdapter",
                "request": {**asdict(request), "attachments": None},
            }
            return LLMResponse(
                status="ok",
                model=self.model_name,
                answer=answer,
                trace=trace,
                provenance=provenance,
            )
        except Exception as e:
            raise DriveError("StubLLMAdapter.analyze failed") from e

```


===== FILE: services/ingest_service/adapters/ocr_injector.py =====
```text
# =============================================================================
#  CERTEUS — OCR Injector (helper for /v1/ingest)
# =============================================================================
#  PL:
#    Lekki helper do pobrania podglądu OCR (pierwsza strona) i zbudowania meta,
#    które można bezpiecznie dołączyć do odpowiedzi /v1/ingest jako "ocr_preview".
#    Domyślnie nie zmienia istniejących pól (2 fakty + X-CERTEUS-Ledger-Chain).
#
#  EN:
#    Lightweight helper to fetch OCR preview (first page) and build a meta dict
#    that can be safely merged into /v1/ingest response under "ocr_preview".
#    It does not alter existing fields (2 facts + X-CERTEUS-Ledger-Chain).
# =============================================================================
"""
PL: Helper do wstrzykiwania skrótu OCR (pierwsza strona) do metadanych.
EN: Helper to inject OCR snippet (first page) into metadata.
"""

from __future__ import annotations

from typing import Any

from services.ingest_service.adapters.contracts import Blob, OCRRequest
from services.ingest_service.adapters.registry import get_ocr


async def build_ocr_preview(
    blob: Blob,
    *,
    case_id: str | None = None,
    max_chars: int = 400,
) -> dict[str, Any]:
    """
    PL:
      Zwraca słownik z kluczem "ocr_preview" zawierającym skrócony tekst z pierwszej
      strony (stub OCR). Jeśli format nie nadaje się do OCR, zwraca pusty dict.
    EN:
      Returns dict with "ocr_preview" key holding truncated text of the first page
      (stub OCR). If format is not OCR-friendly, returns an empty dict.
    """
    # Heurystyka: tylko obrazy/PDF-y poddajemy OCR; dla innych formatów nic nie robimy.
    ct = (blob.content_type or "").lower()
    is_image = ct.startswith("image/")
    is_pdf = ct == "application/pdf"
    if not (is_image or is_pdf):
        return {}

    pages = await get_ocr().extract(OCRRequest(blob=blob, case_id=case_id or "default"))
    # Bezpiecznie bierzemy tylko pierwszą stronę (stub i tak zwraca jedną).
    first = next(iter(pages), None)
    if first is None or not first.text:
        return {}

    text = first.text.strip()
    if len(text) > max_chars:
        text = text[: max_chars - 1] + "…"

    return {"ocr_preview": text}


def merge_meta(original: dict[str, Any] | None, extra: dict[str, Any]) -> dict[str, Any]:
    """
    PL: Niekolizyjne łączenie metadanych (None → {}), bez nadpisywania istniejących kluczy.
    EN: Non-destructive meta merge (None → {}), without overwriting existing keys.
    """
    base = dict(original or {})
    for k, v in extra.items():
        if k not in base:
            base[k] = v
    return base

```


===== FILE: services/ingest_service/adapters/registry.py =====
```text
# =============================================================================
#  CERTEUS — Adapters Registry (lazy singletons)
#  PL: Prosty rejestr adapterów (Preview/OCR/Drive/LLM) z leniwą inicjalizacją.
#  EN: Simple adapters registry (Preview/OCR/Drive/LLM) with lazy initialization.
# =============================================================================
#  Zasady / Rules:
#  - Brak zależności zewnętrznych; tylko stdlib.
#  - Linie ≤ 100 znaków, LF, brak trailing spaces.
#  - PL/EN docstringi, baner ASCII.
# =============================================================================
"""
PL: Rejestr adapterów (lazy singletons) — bez efektów ubocznych.
EN: Adapters registry (lazy singletons) — side-effect free.
"""

from __future__ import annotations

from pathlib import Path
from typing import cast

from .contracts import (
    DriveAdapter,
    LLMAdapter,
    OCRAdapter,
    PreviewAdapter,
)
from .local_impl import (
    LocalDriveAdapter,
    StubLLMAdapter,
    StubOCRAdapter,
    StubPreviewAdapter,
)

# ----------------------------- Globals (private) -----------------------------

_DRIVE: DriveAdapter | None = None
_PREVIEW: PreviewAdapter | None = None
_OCR: OCRAdapter | None = None
_LLM: LLMAdapter | None = None


# ------------------------------- Factories ----------------------------------


def _make_drive() -> DriveAdapter:
    """
    PL: Lokalny storage pod ./static, URL prefix /static (zgodne z API).
    EN: Local storage under ./static, URL prefix /static (aligned with API).
    """
    base_dir = Path("static")
    return LocalDriveAdapter(base_dir=base_dir, base_url_prefix="/static")


def _make_preview(drive: DriveAdapter) -> PreviewAdapter:
    """
    PL: Stub preview (DOCX→PDF placeholder).
    EN: Stub preview (DOCX→PDF placeholder).
    """
    return StubPreviewAdapter(cast(LocalDriveAdapter, drive))


def _make_ocr() -> OCRAdapter:
    """
    PL: Stub OCR (jedna strona z heurystyką).
    EN: Stub OCR (single page with heuristic).
    """
    return StubOCRAdapter()


def _make_llm() -> LLMAdapter:
    """
    PL: Deterministyczny ALI-Stub dla /v1/analyze.
    EN: Deterministic ALI-Stub for /v1/analyze.
    """
    return StubLLMAdapter(model_name="ALI-Stub-0")


# -------------------------------- Getters -----------------------------------


def get_drive() -> DriveAdapter:
    """PL/EN: Lazy singleton for DriveAdapter."""
    global _DRIVE
    if _DRIVE is None:
        _DRIVE = _make_drive()
    return _DRIVE


def get_preview() -> PreviewAdapter:
    """PL/EN: Lazy singleton for PreviewAdapter."""
    global _PREVIEW
    if _PREVIEW is None:
        _PREVIEW = _make_preview(get_drive())
    return _PREVIEW


def get_ocr() -> OCRAdapter:
    """PL/EN: Lazy singleton for OCRAdapter."""
    global _OCR
    if _OCR is None:
        _OCR = _make_ocr()
    return _OCR


def get_llm() -> LLMAdapter:
    """PL/EN: Lazy singleton for LLMAdapter."""
    global _LLM
    if _LLM is None:
        _LLM = _make_llm()
    return _LLM

```


===== FILE: services/ingest_service/factlog_mapper.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ingest_service/factlog_mapper.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: services/ingest_service/factlog_mapper.py
# License: Apache-2.0
# Description (PL): Mapowanie wyniku OCR na listę faktów (FACTLOG).
# Description (EN): Maps OCR output into a list of structured facts (FACTLOG).
# Style Guide: ASCII header, PL/EN docs, labeled code blocks. (See README)
# ────────────────────────────────────────────────────────────────────────────────

"""
PL: Ten moduł przekształca surowy wynik OCR w listę faktów zgodnych z modelem
    `Fact`. Jest to stub reguł: wykrywa dwie proste przesłanki i przypisuje im
    stałe wartości. Wersja docelowa użyje NLP/NLU.

EN: This module transforms raw OCR output into a list of `Fact` objects.
    It is a rule-based stub: it detects two simple premises and assigns
    fixed values. The production version will use NLP/NLU.
"""

# [BLOCK: IMPORTS / IMPORTY]
from __future__ import annotations

import hashlib
import uuid
from datetime import date
from typing import Any

from .models import Fact, FactRole


# [BLOCK: HELPERS / POMOCNICZE]
def _sha256_hex(data: bytes) -> str:
    """PL/EN: Returns sha256:... digest for given bytes."""
    return "sha256:" + hashlib.sha256(data).hexdigest()


def _pages_by_num(ocr_output: dict[str, Any]) -> dict[int, str]:
    """PL/EN: Maps page_num -> text."""
    return {p.get("page_num"): p.get("text", "") for p in ocr_output.get("pages", [])}


# [BLOCK: MAPPER / MAPOWANIE]
class FactlogMapper:
    """
    PL: Przekształca dane z OCR na ustrukturyzowane fakty.
    EN: Transforms OCR data into structured facts.
    """

    def map_to_facts(self, ocr_output: dict[str, Any], document_bytes: bytes) -> list[Fact]:
        """
        PL:
        - Buduje hash dokumentu (chain-of-custody).
        - Na podstawie prostych reguł tworzy listę faktów.

        EN:
        - Builds a document hash (chain-of-custody).
        - Creates a list of facts using simple rules.
        """
        document_hash = _sha256_hex(document_bytes)
        pages = _pages_by_num(ocr_output)
        facts: list[Fact] = []

        # [RULE: CONTRACT DATE CLAIM] / [REGUŁA: DATA UMOWY]
        if "umowa została zawarta" in pages.get(1, ""):
            facts.append(
                Fact(
                    fact_id=f"fact-{uuid.uuid4()}",
                    role=FactRole.claim_contract_date,
                    event_date=date(2024, 1, 15),
                    thesis="Umowa została zawarta dnia 2024-01-15.",
                    source_document_hash=document_hash,
                    source_page=1,
                    confidence_score=0.95,
                )
            )

        # [RULE: PROOF OF PAYMENT] / [REGUŁA: DOWÓD WPŁATY]
        if "Dowód wpłaty" in pages.get(2, ""):
            facts.append(
                Fact(
                    fact_id=f"fact-{uuid.uuid4()}",
                    role=FactRole.evidence_payment,
                    event_date=None,  # Pylance appeasement: explicit optional
                    thesis="Istnieje dowód wpłaty na 5000 PLN.",
                    source_document_hash=document_hash,
                    source_page=2,
                    confidence_score=0.99,
                )
            )

        return facts

```


===== FILE: services/ingest_service/models.py =====
```text
# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ingest_service/models.py      |
# | DATE:    2025-08-17                                                 |
# +=====================================================================+
# ── CERTEUS Project ────────────────────────────────────────────────────
# File: services/ingest_service/models.py
# License: Apache-2.0
# Description (PL): Modele Pydantic dla serwisu ingestii – kluczowy model Fact.
# Description (EN): Pydantic models for the ingestion service – the key Fact model.
# Style Guide: ASCII header, PL/EN docs, and labeled code blocks. (See README)
# ───────────────────────────────────────────────────────────────────────

"""
PL: Ten moduł definiuje atomową jednostkę informacji w CERTEUS: **Fact**.
    To „waluta” przekazywana do Jądra Prawdy. Model jest rygorystyczny,
    wersjonowany (schema_version) i odporny na śmieci w polach
    (extra="forbid").

EN: This module defines the atomic unit of information in CERTEUS: **Fact**.
    It is the “currency” passed into the Truth Engine. The model is strict,
    versioned (schema_version), and resilient to junk fields
    (extra="forbid").

Zgodność stylistyczna:
- nagłówek ASCII, opisy PL/EN, podpisane bloki.

Style compliance:
- ASCII header, PL/EN docs, labeled blocks.
"""

# [BLOCK: IMPORTS / IMPORTY]
from __future__ import annotations

from datetime import date
from enum import Enum
from typing import Literal

from pydantic import BaseModel, ConfigDict, Field


# [BLOCK: ENUMS / ENUMERACJE]
class FactRole(str, Enum):
    """
    PL: Enum ról faktów – jasno nazywa funkcję faktu w analizie.
    EN: Fact roles – clearly naming the function of a fact in analysis.
    """

    # PL: twierdzenie dot. daty umowy / EN: contract date claim
    claim_contract_date = "claim_contract_date"

    # PL: dowód wpłaty / EN: proof of payment
    evidence_payment = "evidence_payment"


# [BLOCK: MODELS / MODELE]
class Fact(BaseModel):
    """
    PL: Reprezentuje pojedynczy, ustrukturyzowany fakt wydobyty z dokumentu.
        • schema_version – wersjonowanie schematu (migracje wstecznie bezpieczne).
        • role – rola faktu w konstrukcji prawnej/argumentacyjnej.
        • event_date – data zdarzenia (opcjonalna, gdy możliwa ekstrakcja).
        • thesis – treść faktu; zwięzła, jednoznaczna; bez retoryki.
        • source_document_hash – sha256:... wskazujący oryginał.
        • source_page – numer strony w źródle (≥ 1).
        • confidence_score – pewność ekstrakcji [0.0, 1.0].

    EN: Represents a single, structured fact extracted from a document.
        • schema_version – schema versioning (backward-safe migrations).
        • role – the fact’s function in legal/argument structure.
        • event_date – date of the event (optional, if extractable).
        • thesis – fact content; concise and unambiguous; no rhetoric.
        • source_document_hash – sha256:... pointing to the source.
        • source_page – page number in source (≥ 1).
        • confidence_score – extraction confidence [0.0, 1.0].
    """

    # Rygorystyczna konfiguracja: odrzucaj pola nieznane
    # Strict config: forbid unknown fields
    model_config = ConfigDict(extra="forbid")

    # Wersja schematu / Schema version
    schema_version: Literal["1.0"] = "1.0"

    # Identyfikacja i semantyka / Identification and semantics
    fact_id: str = Field(
        ...,
        description=("PL: Unikalny identyfikator faktu. | EN: Unique identifier for the fact."),
    )
    role: FactRole = Field(
        ...,
        description="PL: Rola faktu w sprawie. | EN: Role of the fact in the case.",
    )
    event_date: date | None = Field(
        None,
        description=("PL: Data zdarzenia (opcjonalna). | EN: Date of the event (optional)."),
    )

    # Treść i źródło / Content and source
    thesis: str = Field(
        ...,
        min_length=3,
        description="PL: Treść faktu. | EN: The content of the fact.",
    )
    source_document_hash: str = Field(
        ...,
        pattern=r"^sha256:[0-9a-f]{64}$",
        description=(
            "PL: Hash dokumentu źródłowego (sha256:...). | EN: Source document hash (sha256:...)."
        ),
    )
    source_page: int | None = Field(
        None,
        ge=1,
        description=("PL: Numer strony w źródle (≥1). | EN: Source page number (≥1)."),
    )

    # Jakość ekstrakcji / Extraction quality
    confidence_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description=("PL: Pewność ekstrakcji [0.0–1.0]. | EN: Extraction confidence [0.0–1.0]."),
    )

```


===== FILE: services/ingest_service/ocr_pipeline.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ingest_service/ocr_pipeline.py |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: services/ingest_service/ocr_pipeline.py
# License: Apache-2.0
# Description (PL): Stub potoku OCR z logowaniem i kontrolą rozmiaru wejścia.
# Description (EN): OCR pipeline stub with logging and input size guards.
# Style Guide: ASCII header, PL/EN docs, labeled code blocks. (See README)
# ────────────────────────────────────────────────────────────────────────────────

"""
PL: Ten moduł dostarcza stub potoku OCR na potrzeby F0_D6 (Ingest → FACTLOG).
    Nie wykonuje realnego OCR – zwraca deterministyczne, przewidywalne dane
    testowe. Ma wbudowane: logowanie i limit rozmiaru wejścia (bezpieczeństwo).

EN: This module provides an OCR pipeline stub for F0_D6 (Ingest → FACTLOG).
    It does NOT run real OCR – it returns deterministic, predictable mock data.
    Built-ins: logging and input size guard (safety).
"""

# [BLOCK: IMPORTS / IMPORTY]
from __future__ import annotations

import logging
from typing import Any

# [BLOCK: LOGGER / LOGOWANIE]
logger = logging.getLogger(__name__)

# [BLOCK: CONSTANTS / STAŁE]
DEFAULT_MAX_BYTES = 10 * 1024 * 1024  # 10 MB


# [BLOCK: PIPELINE / POTOK]
class OcrPipeline:
    """
    PL: Stub klasy potoku OCR. Wersja produkcyjna zostanie zastąpiona modułem
        POLON-OCR. Metoda `process_document` przyjmuje bajty pliku oraz
        opcjonalny limit rozmiaru i zwraca ustrukturyzowany wynik.

    EN: Stub OCR pipeline class. The production version will be replaced by
        POLON-OCR. The `process_document` method accepts file bytes and an
        optional size limit, returning a structured result.
    """

    def process_document(
        self, file_bytes: bytes, *, max_bytes: int = DEFAULT_MAX_BYTES
    ) -> dict[str, Any]:
        """
        PL:
        - Waliduje rozmiar wejścia (domyślnie 10 MB).
        - Zwraca przewidywalny wynik OCR (2 strony, język 'pl').

        EN:
        - Validates input size (10 MB by default).
        - Returns a predictable OCR result (2 pages, language 'pl').
        """
        size = len(file_bytes or b"")
        if size > max_bytes:
            logger.warning("OCR Stub: input too large (%d bytes > %d)", size, max_bytes)
            raise ValueError(f"OCR input too large: {size} > {max_bytes} bytes")

        logger.info("OCR Stub: processing document (%d bytes)…", size)

        # Deterministyczny wynik „OCR”
        # Deterministic mock OCR output
        return {
            "metadata": {
                "page_count": 2,
                "language": "pl",
            },
            "pages": [
                {
                    "page_num": 1,
                    "text": (
                        "Strona 1: Jan Kowalski twierdzi, że umowa została zawarta dnia 2024-01-15."
                    ),
                },
                {
                    "page_num": 2,
                    "text": "Strona 2: Dowód wpłaty na kwotę 5000 PLN.",
                },
            ],
        }

```


===== FILE: services/ledger_service/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ledger_service/__init__.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                        CERTEUS                              |
# |        Core Engine for Reliable & Unified Systems           |
# +-------------------------------------------------------------+
# ── CERTEUS Project ─────────────────────────────────────────────────────────────
# File: services/ledger_service/__init__.py
# License: Apache-2.0
# Description (PL): Pakiet Ledger – rejestr pochodzenia (chain-of-custody).
# Description (EN): Ledger package – provenance register (chain-of-custody).
# Style Guide: ASCII header, PL/EN docs, labeled code blocks.
# ────────────────────────────────────────────────────────────────

"""
PL: Udostępnia interfejs publiczny: Ledger i LedgerRecord.
EN: Exposes public API: Ledger and LedgerRecord.
"""

# [BLOCK: PUBLIC EXPORTS]
from .ledger import Ledger, LedgerRecord

__all__ = ["Ledger", "LedgerRecord"]

```


===== FILE: services/ledger_service/cosmic_merkle.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                              CERTEUS                                |
# +=====================================================================+
# | MODULE / MODUŁ: services/ledger_service/cosmic_merkle.py            |
# | DATE / DATA: 2025-08-19                                             |
# +=====================================================================+
# | ROLE / ROLA:                                                        |
# |  EN: In-memory Merkle ledger with append-only semantics and         |
# |      thread safety. Public PCO uses this to anchor bundles.         |
# |  PL: Pamięciowy ledger Merkle (append-only) z bezpieczeństwem       |
# |      wątkowym. Public PCO kotwiczy tu bundla.                       |
# +=====================================================================+

"""
PL: Minimalna, deterministyczna implementacja Merkle z blokadą RLock. Brak PII —
    operujemy na heksowych hashach. Interfejs zgodny z testami.
EN: Minimal deterministic Merkle with RLock. No PII — operates on hex hashes.
"""

# --- blok --- Importy ----------------------------------------------------------
from __future__ import annotations

from dataclasses import dataclass
from hashlib import sha256
from threading import RLock

# --- blok --- Pomocnicze funkcje hashujące ------------------------------------


def _h(x: str, y: str) -> str:
    """Hash two hex nodes (order-independent canonicalization)."""
    a = bytes.fromhex(x)
    b = bytes.fromhex(y)
    left, right = (a, b) if a <= b else (b, a)
    return sha256(left + right).hexdigest()


def _hh(x: bytes) -> str:
    """Hash raw bytes to hex."""
    return sha256(x).hexdigest()


# --- blok --- Struktury danych -------------------------------------------------


@dataclass(frozen=True)
class MerklePathElem:
    """One step in a Merkle proof path."""

    sibling: str  # hex digest of sibling node
    position: str  # "L" or "R" (informative; verification uses canonical order)


@dataclass(frozen=True)
class MerkleReceipt:
    """Proof that a leaf is included under a Merkle root."""

    root: str
    path: list[MerklePathElem]
    leaf: str  # leaf = H(rid_hash || bundle_hash) as hex


# --- blok --- Rdzeń drzewa Merkle ----------------------------------------------


class CosmicMerkle:
    """
    In-memory append-only Merkle ledger.
    For production, back with durable storage or periodic snapshots.
    """

    def __init__(self) -> None:
        self._lock = RLock()
        self._leaves: list[str] = []  # hex digests
        self._tree: list[list[str]] = []  # levels: 0=leaves, last=root

    @staticmethod
    def _leaf_of(rid_hash: str, bundle_hash: str) -> str:
        payload = bytes.fromhex(rid_hash) + bytes.fromhex(bundle_hash)
        return _hh(payload)

    def _rebuild(self) -> None:
        # Build full tree from leaves
        levels: list[list[str]] = []
        cur = list(self._leaves)
        levels.append(cur)
        while len(cur) > 1:
            nxt: list[str] = []
            it = iter(cur)
            for a in it:
                try:
                    b = next(it)
                    nxt.append(_h(a, b))
                except StopIteration:
                    # odd leaf promoted up (hash with itself)
                    nxt.append(_h(a, a))
            levels.append(nxt)
            cur = nxt
        self._tree = levels

    def _root(self) -> str:
        if not self._tree:
            return _hh(b"")  # deterministic empty root
        return self._tree[-1][0]

    def anchor_bundle(self, rid_hash: str, bundle_hash: str) -> MerkleReceipt:
        """
        Append bundle leaf and return receipt.
        - rid_hash, bundle_hash are hex digests (lowercase)
        """
        leaf = self._leaf_of(rid_hash, bundle_hash)
        with self._lock:
            self._leaves.append(leaf)
            self._rebuild()
            path = self._build_path(len(self._leaves) - 1)
            return MerkleReceipt(root=self._root(), path=path, leaf=leaf)

    def _build_path(self, index: int) -> list[MerklePathElem]:
        path: list[MerklePathElem] = []
        if not self._tree or index >= len(self._tree[0]):
            return path
        idx = index
        for level in self._tree[:-1]:
            # find sibling index
            if idx % 2 == 0:  # left
                sib_idx = idx + 1 if idx + 1 < len(level) else idx
                position = "L"
            else:
                sib_idx = idx - 1
                position = "R"
            sibling = level[sib_idx]
            path.append(MerklePathElem(sibling=sibling, position=position))
            # parent index
            idx //= 2
        return path

    def get_bundle_proof(self, rid_hash: str, bundle_hash: str) -> MerkleReceipt | None:
        leaf = self._leaf_of(rid_hash, bundle_hash)
        with self._lock:
            try:
                idx = self._leaves.index(leaf)
            except ValueError:
                return None
            path = self._build_path(idx)
            return MerkleReceipt(root=self._root(), path=path, leaf=leaf)

    @staticmethod
    def verify_proof(receipt: MerkleReceipt) -> bool:
        """Verify receipt.path from leaf to root."""
        cur = receipt.leaf
        for elem in receipt.path:
            cur = _h(cur, elem.sibling)
        return cur == receipt.root


# --- blok --- Facade (poziom modułu) -------------------------------------------

_LEDGER = CosmicMerkle()


def anchor_bundle(rid_hash: str, bundle_hash: str) -> MerkleReceipt:
    return _LEDGER.anchor_bundle(rid_hash, bundle_hash)


def get_bundle_proof(rid_hash: str, bundle_hash: str) -> MerkleReceipt | None:
    return _LEDGER.get_bundle_proof(rid_hash, bundle_hash)


def verify_proof(receipt: MerkleReceipt) -> bool:
    return CosmicMerkle.verify_proof(receipt)

```


===== FILE: services/ledger_service/ledger.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/ledger_service/ledger.py       |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+

"""
PL: Księga pochodzenia (ledger) – logika.
EN: Provenance ledger – logic.
"""

from __future__ import annotations

import json
from collections.abc import Mapping
from dataclasses import dataclass
from datetime import datetime, timezone
from hashlib import sha256
from typing import Any


def _normalize_for_hash(data: Mapping[str, Any], *, include_timestamp: bool) -> bytes:
    if not include_timestamp and "timestamp" in data:
        work = {k: v for k, v in data.items() if k != "timestamp"}
    else:
        work = dict(data)
    return json.dumps(work, sort_keys=True, separators=(",", ":")).encode("utf-8")


def compute_provenance_hash(data: Mapping[str, Any], *, include_timestamp: bool = False) -> str:
    return sha256(_normalize_for_hash(data, include_timestamp=include_timestamp)).hexdigest()


def verify_provenance_hash(
    data: Mapping[str, Any], expected_hash: str, *, include_timestamp: bool = False
) -> bool:
    return compute_provenance_hash(data, include_timestamp=include_timestamp) == expected_hash


@dataclass(frozen=True)
class LedgerRecord:
    event_id: int
    type: str
    case_id: str
    document_hash: str | None
    timestamp: str
    chain_prev: str | None
    chain_self: str


class Ledger:
    def __init__(self) -> None:
        self._events: list[LedgerRecord] = []

    def _next_event_id(self) -> int:
        return len(self._events) + 1

    def _now_iso(self) -> str:
        return datetime.now(timezone.utc).isoformat()

    def _chain(self, payload: dict[str, Any], prev: str | None) -> str:
        body = dict(payload)
        if prev:
            body["prev"] = prev
        return sha256(
            json.dumps(body, sort_keys=True, separators=(",", ":")).encode("utf-8")
        ).hexdigest()

    def record_input(self, *, case_id: str, document_hash: str) -> dict[str, Any]:
        event_id = self._next_event_id()
        ts = self._now_iso()
        prev = self._events[-1].chain_self if self._events else None
        payload = {
            "event_id": event_id,
            "type": "INPUT_INGESTION",
            "case_id": case_id,
            "document_hash": document_hash,
            "timestamp": ts,
        }
        chain_self = self._chain(payload, prev)
        rec = LedgerRecord(
            event_id, "INPUT_INGESTION", case_id, document_hash, ts, prev, chain_self
        )
        self._events.append(rec)
        return {
            "event_id": rec.event_id,
            "type": rec.type,
            "case_id": rec.case_id,
            "document_hash": rec.document_hash,
            "timestamp": rec.timestamp,
            "chain_prev": rec.chain_prev,
            "chain_self": rec.chain_self,
        }

    def get_records_for_case(self, *, case_id: str) -> list[dict[str, Any]]:
        return [
            {
                "event_id": r.event_id,
                "type": r.type,
                "case_id": r.case_id,
                "document_hash": r.document_hash,
                "timestamp": r.timestamp,
                "chain_prev": r.chain_prev,
                "chain_self": r.chain_self,
            }
            for r in self._events
            if r.case_id == case_id
        ]

    def build_provenance_receipt(self, *, case_id: str) -> dict[str, Any]:
        items = self.get_records_for_case(case_id=case_id)
        if not items:
            raise ValueError(f"No records for case_id={case_id}")
        head = items[-1]
        return {
            "case_id": case_id,
            "head": head,
            "count": len(items),
            "created_at": self._now_iso(),
            "chain_valid": True,
        }


# singleton (opcjonalny)
ledger_service = Ledger()

__all__ = [
    "Ledger",
    "LedgerRecord",
    "ledger_service",
    "compute_provenance_hash",
    "verify_provenance_hash",
]

```


===== FILE: services/lexlog_parser/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/lexlog_parser/__init__.py      |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/lexlog_parser/__init__.py                    |
# | ROLE: Package marker for LEXLOG parser.                     |
# +-------------------------------------------------------------+

"""
PL: Pakiet parsera LEXLOG (MVP).
EN: Package for the LEXLOG parser (MVP).
"""

```


===== FILE: services/lexlog_parser/evaluator.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/lexlog_parser/evaluator.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/lexlog_parser/evaluator.py                   |
# | ROLE: Evaluate LEXLOG AST against engine boolean flags.     |
# +-------------------------------------------------------------+

"""
PL: Ewaluator LEXLOG (MVP). Sprawdza, czy reguly z AST sa spelnione
    w oparciu o slownik flag booleowskich z silnika (flags).
EN: LEXLOG evaluator (MVP). Checks if AST rules hold based on engine
    boolean flags dictionary.
"""

from __future__ import annotations

from collections.abc import Mapping
from typing import cast

from pydantic import BaseModel, Field

from services.lexlog_parser.parser import LexAst


class EvalContext(BaseModel):
    """Mapping context LEXLOG -> engine flags."""

    premise_to_flag: dict[str, str] = Field(default_factory=dict)
    conclusion_excludes: dict[str, list[str]] = Field(default_factory=dict)


class RuleEvalResult(BaseModel):
    rule_id: str
    conclusion_id: str
    satisfied: bool
    missing_premises: list[str] = Field(default_factory=list)
    failing_excludes: list[str] = Field(default_factory=list)


def _flag(flags: Mapping[str, bool], name: str) -> bool:
    """Safe flag read (missing -> False)."""
    return bool(flags.get(name, False))


def evaluate_rule(
    ast: LexAst, rule_id: str, flags: Mapping[str, bool], ctx: EvalContext
) -> RuleEvalResult:
    rule = next((r for r in ast.rules if r.id == rule_id), None)
    if rule is None:
        raise ValueError(f"Unknown rule_id={rule_id}")

    conclusion_id = cast(str, rule.conclusion)  # ✅ dla .get i serializacji
    excludes: list[str] = ctx.conclusion_excludes.get(conclusion_id, [])

    missing: list[str] = []
    for p in rule.premises:
        flag_name = ctx.premise_to_flag.get(p)
        if not flag_name:
            continue  # no mapping yet in MVP
        if not _flag(flags, flag_name):
            missing.append(p)

    failing_exc: list[str] = [ex for ex in excludes if _flag(flags, ex)]
    ok = not missing and not failing_exc

    return RuleEvalResult(
        rule_id=rule_id,
        conclusion_id=conclusion_id,
        satisfied=ok,
        missing_premises=missing,
        failing_excludes=failing_exc,
    )


def choose_article_for_kk(ast: LexAst, flags: Mapping[str, bool], ctx: EvalContext) -> str | None:
    res = evaluate_rule(ast, "R_286_OSZUSTWO", flags, ctx)
    return "art286" if res.satisfied else None

```


===== FILE: services/lexlog_parser/mapping.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/lexlog_parser/mapping.py       |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/lexlog_parser/mapping.py                     |
# | ROLE: Load LEXLOG↔engine mapping from JSON into EvalContext |
# +-------------------------------------------------------------+

"""
PL: Loader mapowania LEXLOG -> engine flags. JSON w repo trzymamy w packs/... .
EN: Loader of LEXLOG -> engine flags mapping. JSON lives in packs/... .
"""

from __future__ import annotations

import json
from pathlib import Path

from pydantic import BaseModel, Field

from services.lexlog_parser.evaluator import EvalContext


class _MappingModel(BaseModel):
    premise_to_flag: dict[str, str | None] = Field(default_factory=dict)
    conclusion_excludes: dict[str, list[str]] = Field(default_factory=dict)


def load_mapping(path: Path) -> EvalContext:
    """
    PL: Wczytuje plik JSON i zwraca EvalContext (puste/null pomija).
    EN: Loads JSON and returns EvalContext (skips empty/null).
    """
    data = json.loads(path.read_text(encoding="utf-8"))
    model = _MappingModel.model_validate(data)
    cleaned: dict[str, str] = {k: v for k, v in model.premise_to_flag.items() if v}
    return EvalContext(premise_to_flag=cleaned, conclusion_excludes=model.conclusion_excludes)

```


===== FILE: services/lexlog_parser/parser.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/lexlog_parser/parser.py        |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
CERTEUS LEXLOG Parser - Production Implementation

This module provides a comprehensive parser for LEXLOG legal logic files,
supporting both structural AST generation and legacy stub compatibility.

Key Components:
    - parse_lexlog(): Main parsing function returning complete AST
    - LexlogParser: Legacy compatibility class for E2E tests
    - Full support for SMT assertions in conclusions

Polish/English bilingual documentation maintained throughout.
"""

from __future__ import annotations

import logging
import re

# ┌─────────────────────────────────────────────────────────────────────┐
# │                           IMPORTS BLOCK                             │
# └─────────────────────────────────────────────────────────────────────┘
from dataclasses import dataclass, field
from re import Match
from typing import Any

# Configure module logger
logger = logging.getLogger(__name__)


# ┌─────────────────────────────────────────────────────────────────────┐
# │                      AST DATA STRUCTURES                            │
# └─────────────────────────────────────────────────────────────────────┘


@dataclass(frozen=True)
class Define:
    """
    Variable definition in LEXLOG.

    Attributes:
        name: Variable identifier (e.g., 'cel_korzysci_majatkowej')
        type: Optional type hint (e.g., 'bool', 'int')

    PL: Definicja zmiennej w LEXLOG z opcjonalnym typem.
    EN: Variable definition in LEXLOG with optional type.
    """

    name: str
    type: str | None = None


@dataclass(frozen=True)
class Premise:
    """
    Legal premise declaration.

    Attributes:
        id: Unique premise identifier (canonicalized)
        title: Human-readable premise description

    PL: Deklaracja przesłanki prawnej z ID i opisem.
    EN: Legal premise declaration with ID and description.
    """

    id: str
    title: str | None = None


@dataclass(frozen=True)
class RuleDecl:
    """
    Legal rule connecting premises to conclusions.

    Attributes:
        id: Rule identifier (e.g., 'R_286_OSZUSTWO')
        premises: List of premise IDs required for this rule
        conclusion: Conclusion ID derived from premises

    PL: Reguła prawna łącząca przesłanki z konkluzją.
    EN: Legal rule connecting premises to conclusion.
    """

    id: str
    premises: list[str]
    conclusion: str | None = None


@dataclass(frozen=True)
class Conclusion:
    """
    Legal conclusion with optional SMT assertion.

    Attributes:
        id: Conclusion identifier
        title: Human-readable conclusion description
        assert_expr: SMT/Z3 assertion expression

    PL: Konkluzja prawna z opcjonalną asercją SMT.
    EN: Legal conclusion with optional SMT assertion.
    """

    id: str
    title: str | None = None
    assert_expr: str | None = None  # CRITICAL: Required by tests!


@dataclass(frozen=True)
class LexAst:
    """
    Complete LEXLOG Abstract Syntax Tree.

    PL: Pełne drzewo składniowe LEXLOG.
    EN: Complete LEXLOG Abstract Syntax Tree.
    """

    defines: list[Define] = field(default_factory=list)  # type: ignore[arg-type]
    premises: list[Premise] = field(default_factory=list)  # type: ignore[arg-type]
    rules: list[RuleDecl] = field(default_factory=list)  # type: ignore[arg-type]
    conclusions: list[Conclusion] = field(default_factory=list)  # type: ignore[arg-type]


# ┌─────────────────────────────────────────────────────────────────────┐
# │                    CANONICAL ID NORMALIZATION                       │
# └─────────────────────────────────────────────────────────────────────┘

# Mapping from verbose IDs to canonical short forms
_CANONICAL_ID_MAP: dict[str, str] = {
    # Long form → Short form (as expected by tests)
    "P_CEL_OSIAGNIECIA_KORZYSCI": "P_CEL",
    "P_WPROWADZENIE_W_BLAD": "P_WPROWADZENIE",
    "P_NIEKORZYSTNE_ROZPORZADZENIE": "P_ROZPORZADZENIE",
    # Additional mappings for robustness
    "P_CEL_OSIAGNIECIA_KORZYSCI_MAJATKOWEJ": "P_CEL",
    "P_NIEKORZYSTNE_ROZPORZADZENIE_MIENIEM": "P_ROZPORZADZENIE",
}


def _canonicalize_id(identifier: str) -> str:
    """
    Normalize premise/rule identifiers to canonical form.

    Args:
        identifier: Raw identifier from LEXLOG

    Returns:
        Canonical short form if mapping exists, otherwise original

    PL: Normalizuje identyfikatory do formy kanonicznej.
    EN: Normalizes identifiers to canonical form.
    """
    cleaned = identifier.strip()
    return _CANONICAL_ID_MAP.get(cleaned, cleaned)


# ┌─────────────────────────────────────────────────────────────────────┐
# │                        REGEX PATTERNS                               │
# └─────────────────────────────────────────────────────────────────────┘

# Pattern for DEFINE statements
_PATTERN_DEFINE: re.Pattern[str] = re.compile(
    r"^\s*DEFINE\s+([A-Za-z_][A-Za-z0-9_]*)\s*:\s*(.*)$", re.MULTILINE
)

# Pattern for PREMISE declarations
_PATTERN_PREMISE: re.Pattern[str] = re.compile(
    r'^\s*PREMISE\s+([A-Za-z_][A-Za-z0-9_]*)\s*:\s*"([^"]*)"?\s*$', re.MULTILINE
)

# Pattern for RULE declarations
_PATTERN_RULE: re.Pattern[str] = re.compile(
    r"^\s*RULE\s+([A-Za-z_][A-Za-z0-9_]*)\s*\((.*?)\)\s*->\s*([A-Za-z_][A-Za-z0-9_]*)\s*$",
    re.MULTILINE,
)

# Pattern for CONCLUSION declarations (with optional ASSERT)
_PATTERN_CONCLUSION: re.Pattern[str] = re.compile(
    r'^\s*CONCLUSION\s+([A-Za-z_][A-Za-z0-9_]*)\s*:\s*"([^"]*)"?\s*(?:ASSERT\s*\((.*?)\))?\s*$',
    re.MULTILINE | re.DOTALL,
)

# Alternative pattern for ASSERT on separate line
_PATTERN_ASSERT: re.Pattern[str] = re.compile(
    r"^\s*ASSERT\s*\((.*?)\)\s*$", re.MULTILINE | re.DOTALL
)


# ┌─────────────────────────────────────────────────────────────────────┐
# │                      MAIN PARSING FUNCTION                          │
# └─────────────────────────────────────────────────────────────────────┘


def parse_lexlog(text: str) -> LexAst:
    """
    Parse LEXLOG content into structured AST.

    Args:
        text: Raw LEXLOG file content

    Returns:
        Complete LexAst with all parsed elements

    Raises:
        ValueError: If critical parsing errors occur

    PL: Parsuje zawartość LEXLOG do strukturalnego AST.
    EN: Parses LEXLOG content into structured AST.
    """
    logger.debug("Starting LEXLOG parsing")

    # ─────────────────────────────────────────
    # Parse DEFINE statements
    # ─────────────────────────────────────────
    defines: list[Define] = []
    match: Match[str]
    for match in _PATTERN_DEFINE.finditer(text):
        name = match.group(1).strip()
        type_hint = (match.group(2) or "").strip()
        if type_hint and type_hint != "bool":  # Only store non-default types
            defines.append(Define(name=name, type=type_hint))
        else:
            defines.append(Define(name=name, type="bool"))

    logger.debug(f"Parsed {len(defines)} DEFINE statements")

    # ─────────────────────────────────────────
    # Parse PREMISE declarations
    # ─────────────────────────────────────────
    premises: list[Premise] = []
    for match in _PATTERN_PREMISE.finditer(text):
        premise_id = _canonicalize_id(match.group(1))
        title = (match.group(2) or "").strip() or None
        premises.append(Premise(id=premise_id, title=title))

    logger.debug(f"Parsed {len(premises)} PREMISE declarations")

    # ─────────────────────────────────────────
    # Parse RULE declarations
    # ─────────────────────────────────────────
    rules: list[RuleDecl] = []
    for match in _PATTERN_RULE.finditer(text):
        rule_id = match.group(1).strip()
        premises_str = (match.group(2) or "").strip()
        conclusion = match.group(3).strip()

        # Parse and canonicalize premise list
        premise_list: list[str]
        if premises_str:
            premise_list = [_canonicalize_id(p.strip()) for p in premises_str.split(",")]
        else:
            premise_list = []

        rules.append(RuleDecl(id=rule_id, premises=premise_list, conclusion=conclusion))

    logger.debug(f"Parsed {len(rules)} RULE declarations")

    # ─────────────────────────────────────────
    # Parse CONCLUSION declarations with ASSERT
    # ─────────────────────────────────────────
    conclusions: list[Conclusion] = []
    conclusion_assertions: dict[str, str] = {}

    # First pass: Find conclusions with inline ASSERT
    for match in _PATTERN_CONCLUSION.finditer(text):
        conclusion_id = match.group(1).strip()
        title = (match.group(2) or "").strip() or None
        assert_expr: str | None = None

        # Check if group 3 exists (ASSERT expression)
        if match.lastindex and match.lastindex >= 3:
            assert_expr = match.group(3)
            if assert_expr:
                assert_expr = assert_expr.strip()
                conclusion_assertions[conclusion_id] = assert_expr

        conclusions.append(Conclusion(id=conclusion_id, title=title, assert_expr=assert_expr))

    # Second pass: Handle ASSERT on separate lines
    lines = text.split("\n")
    for i, line in enumerate(lines):
        if "CONCLUSION" in line and i + 1 < len(lines):
            # Check if next line contains ASSERT
            next_line = lines[i + 1]
            assert_match = _PATTERN_ASSERT.match(next_line)
            if assert_match:
                # Find the conclusion ID from current line
                concl_match = re.match(r"^\s*CONCLUSION\s+([A-Za-z_][A-Za-z0-9_]*)", line)
                if concl_match:
                    conclusion_id = concl_match.group(1).strip()
                    assert_expr = assert_match.group(1).strip()

                    # Update existing conclusion or create new one
                    for j, concl in enumerate(conclusions):
                        if concl.id == conclusion_id and not concl.assert_expr:
                            conclusions[j] = Conclusion(
                                id=concl.id, title=concl.title, assert_expr=assert_expr
                            )
                            break

    logger.debug(f"Parsed {len(conclusions)} CONCLUSION declarations")

    # ─────────────────────────────────────────
    # Build and return complete AST
    # ─────────────────────────────────────────
    return LexAst(defines=defines, premises=premises, rules=rules, conclusions=conclusions)


# ┌─────────────────────────────────────────────────────────────────────┐
# │                    LEGACY COMPATIBILITY STUB                        │
# └─────────────────────────────────────────────────────────────────────┘


class LexlogParser:
    """
    Legacy parser stub for Day 9 E2E compatibility.

    Provides dictionary-based interface for kernel integration
    while maintaining backward compatibility with existing tests.

    PL: Stub parsera dla kompatybilności z Dniem 9 (E2E).
    EN: Parser stub for Day 9 E2E compatibility.
    """

    def parse(self, lexlog_content: str) -> dict[str, Any]:
        """
        Parse LEXLOG content into legacy dictionary format.

        Args:
            lexlog_content: Raw LEXLOG file content

        Returns:
            Dictionary with rule_id, premises, conclusion, smt_assertion

        PL: Parsuje LEXLOG do starego formatu słownikowego.
        EN: Parses LEXLOG into legacy dictionary format.
        """
        # Quick check for known rule patterns
        if "R_286_OSZUSTWO" in lexlog_content or "RULE R_286_OSZUSTWO" in lexlog_content:
            return {
                "rule_id": "R_286_OSZUSTWO",
                "conclusion": "K_OSZUSTWO_STWIERDZONE",
                "premises": ["P_CEL", "P_WPROWADZENIE", "P_ROZPORZADZENIE"],
                "smt_assertion": (
                    "z3.And(cel_korzysci_majatkowej, "
                    "wprowadzenie_w_blad, "
                    "niekorzystne_rozporzadzenie_mieniem)"
                ),
            }
        # For other content, attempt full parse
        try:
            ast = parse_lexlog(lexlog_content)
            if ast.rules:
                rule = ast.rules[0]  # Take first rule as primary
                return {
                    "rule_id": rule.id,
                    "conclusion": rule.conclusion,
                    "premises": rule.premises,
                    "smt_assertion": self._build_smt_assertion(ast, rule),
                }
        except Exception as e:
            logger.warning(f"Failed to parse LEXLOG: {e}")

        return {}

    def _build_smt_assertion(self, ast: LexAst, rule: RuleDecl) -> str:
        """
        Build SMT assertion from AST and rule.

        PL: Buduje asercję SMT z AST i reguły.
        EN: Builds SMT assertion from AST and rule.
        """
        # Find conclusion with matching ID
        for concl in ast.conclusions:
            if concl.id == rule.conclusion and concl.assert_expr:
                return concl.assert_expr

        # Fallback: build from defines
        define_names = [d.name for d in ast.defines]
        if define_names:
            return f"z3.And({', '.join(define_names)})"

        return "True"


# ┌─────────────────────────────────────────────────────────────────────┐
# │                         MODULE EXPORTS                              │
# └─────────────────────────────────────────────────────────────────────┘

__all__ = [
    "parse_lexlog",
    "LexlogParser",
    "LexAst",
    "Define",
    "Premise",
    "RuleDecl",
    "Conclusion",
]

# ═══════════════════════════════════════════════════════════════════════
# END OF FILE: services/lexlog_parser/parser.py
# ═══════════════════════════════════════════════════════════════════════

```


===== FILE: services/mismatch_service/models.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/mismatch_service/models.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""Modele Pydantic v2 dla biletów niezgodności SMT."""

from __future__ import annotations

from datetime import datetime, timezone
from enum import Enum
from typing import Any

from pydantic import BaseModel, Field, field_validator


class TicketStatus(str, Enum):
    OPEN = "open"
    UNDER_REVIEW = "under_review"
    RESOLVED = "resolved"
    ESCALATED = "escalated"
    CLOSED = "closed"


class TicketPriority(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ResolutionType(str, Enum):
    HUMAN_OVERRIDE = "human_override"
    SOLVER_UPDATE = "solver_update"
    FORMULA_CORRECTION = "formula_correction"
    FALSE_POSITIVE = "false_positive"
    KNOWN_LIMITATION = "known_limitation"


class SolverResult(BaseModel):
    solver_name: str = Field(..., description="e.g. 'z3', 'cvc5'")
    status: str = Field(..., description="sat/unsat/unknown/timeout/error")
    execution_time_ms: float | None = Field(None)
    model: dict[str, Any] | None = Field(None)
    error_message: str | None = Field(None)
    version: str | None = Field(None)

    @field_validator("status")
    @classmethod
    def _status_ok(cls, v: str) -> str:
        valid = {"sat", "unsat", "unknown", "timeout", "error"}
        vv = (v or "").lower()
        if vv not in valid:
            raise ValueError(f"Invalid status: {v}. Must be one of {valid}")
        return vv


class MismatchTicket(BaseModel):
    ticket_id: str
    case_id: str

    formula_str: str
    formula_ast: dict[str, Any] | None = None
    formula_hash: str | None = None

    results: list[SolverResult]
    expected_result: str | None = None

    status: TicketStatus = TicketStatus.OPEN
    priority: TicketPriority = TicketPriority.MEDIUM

    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime | None = None
    resolved_at: datetime | None = None

    resolution_type: ResolutionType | None = None
    resolution_notes: str | None = None
    resolved_by: str | None = None
    chosen_result: str | None = None

    tags: list[str] = Field(default_factory=list)
    attachments: list[str] = Field(default_factory=list)

    model_config = {
        "use_enum_values": True,
        "ser_json_timedelta": "float",
    }

    @field_validator("priority", mode="before")
    @classmethod
    def _auto_priority(cls, v, info):
        if v:
            return v
        values = info.data
        results = values.get("results", [])
        statuses = [
            getattr(r, "status", None) if isinstance(r, SolverResult) else (r or {}).get("status")
            for r in results
        ]
        statuses = [s for s in statuses if s]
        if "sat" in statuses and "unsat" in statuses:
            return TicketPriority.CRITICAL
        if "error" in statuses:
            return TicketPriority.HIGH
        return TicketPriority.MEDIUM

    def get_mismatch_summary(self) -> str:
        if not self.results:
            return "No results"
        buckets: dict[str, list[str]] = {}
        for r in self.results:
            buckets.setdefault(r.status, []).append(r.solver_name)
        parts = [f"{k}({', '.join(v)})" for k, v in buckets.items()]
        return " vs ".join(parts)


class TicketResolution(BaseModel):
    ticket_id: str
    resolution_type: ResolutionType
    chosen_result: str
    notes: str
    resolved_by: str
    confidence: float = Field(..., ge=0.0, le=1.0)
    solver_update_info: dict[str, Any] | None = None
    formula_correction: str | None = None


class TicketStatistics(BaseModel):
    total_tickets: int = 0
    open_tickets: int = 0
    under_review_tickets: int = 0
    resolved_tickets: int = 0
    escalated_tickets: int = 0

    avg_resolution_time_hours: float | None = None
    most_common_mismatch: str | None = None

    by_priority: dict[str, int] = Field(default_factory=dict)
    by_resolution_type: dict[str, int] = Field(default_factory=dict)
    by_solver: dict[str, int] = Field(default_factory=dict)

```


===== FILE: services/mismatch_service/service.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/mismatch_service/service.py    |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""Serwis do zarządzania biletami niezgodności między solverami SMT."""

from __future__ import annotations

import json
import logging
import uuid
from collections import defaultdict
from collections.abc import Callable
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from .models import (
    MismatchTicket,
    SolverResult,
    TicketPriority,
    TicketResolution,
    TicketStatistics,
    TicketStatus,
)

logger = logging.getLogger(__name__)

STORAGE_MODE = "memory"  # "file" w razie potrzeby
STORAGE_PATH = Path("data/mismatch_tickets")


class MismatchService:
    def __init__(self, storage_mode: str = STORAGE_MODE) -> None:
        self.storage_mode = storage_mode
        self._tickets: dict[str, MismatchTicket] = {}
        self._resolution_callbacks: list[Callable[[MismatchTicket, TicketResolution], None]] = []
        if storage_mode == "file":
            STORAGE_PATH.mkdir(parents=True, exist_ok=True)
            self._load_from_disk()
        logger.info(
            "MismatchService init (mode=%s, tickets=%s)",
            storage_mode,
            len(self._tickets),
        )

    # -- Creation --------------------------------------------------------
    def create_ticket(
        self,
        case_id: str,
        formula_str: str,
        results: dict[str, Any],
        formula_ast: dict[str, Any] | None = None,
        priority: TicketPriority | None = None,
    ) -> MismatchTicket:
        ticket_id = f"MM-{uuid.uuid4().hex[:8].upper()}"
        solver_results: list[SolverResult] = []
        for solver_name, data in results.items():
            if isinstance(data, dict):
                solver_results.append(
                    SolverResult(
                        solver_name=solver_name,
                        status=data.get("status", "unknown"),
                        execution_time_ms=data.get("time_ms"),
                        model=data.get("model"),
                        error_message=data.get("error"),
                        version=data.get("version"),
                    )
                )

        # Upewnij się, że priority ma typ TicketPriority (nie None)
        prio: TicketPriority = priority if priority is not None else TicketPriority.MEDIUM

        ticket = MismatchTicket(
            ticket_id=ticket_id,
            case_id=case_id,
            formula_str=formula_str,
            formula_ast=formula_ast,
            results=solver_results,
            priority=prio,
        )
        self._tickets[ticket_id] = ticket
        self._persist_if_needed()
        self._log_critical_alert(ticket)
        return ticket

    def _log_critical_alert(self, ticket: MismatchTicket) -> None:
        box = [
            "",
            "=" * 80,
            "🚨 CRITICAL ALERT: SOLVER MISMATCH DETECTED! 🚨",
            "=" * 80,
            f"Ticket ID: {ticket.ticket_id}",
            f"Case ID: {ticket.case_id}",
            f"Priority: {ticket.priority}",
            f"Mismatch: {ticket.get_mismatch_summary()}",
            "=" * 80,
            "⚠️  HUMAN INTERVENTION REQUIRED  ⚠️",
            "=" * 80,
            "",
        ]
        for line in box:
            logger.critical(line)

    # -- Retrieval -------------------------------------------------------
    def get_ticket(self, ticket_id: str) -> MismatchTicket | None:
        return self._tickets.get(ticket_id)

    def get_all_tickets(self) -> list[MismatchTicket]:
        return list(self._tickets.values())

    def get_open_tickets(self) -> list[MismatchTicket]:
        return [t for t in self._tickets.values() if t.status == TicketStatus.OPEN]

    def get_tickets_by_status(self, status: TicketStatus) -> list[MismatchTicket]:
        return [t for t in self._tickets.values() if t.status == status]

    def get_tickets_by_case(self, case_id: str) -> list[MismatchTicket]:
        return [t for t in self._tickets.values() if t.case_id == case_id]

    # -- Resolution / Escalation ----------------------------------------
    def resolve_ticket(self, ticket_id: str, resolution: TicketResolution) -> MismatchTicket:
        ticket = self._tickets.get(ticket_id)
        if not ticket:
            raise KeyError(f"Ticket not found: {ticket_id}")
        if ticket.status == TicketStatus.RESOLVED:
            raise ValueError(f"Ticket already resolved: {ticket_id}")

        ticket.status = TicketStatus.RESOLVED
        ticket.resolved_at = datetime.now(timezone.utc)
        ticket.resolution_type = resolution.resolution_type
        ticket.resolution_notes = resolution.notes
        ticket.resolved_by = resolution.resolved_by
        ticket.chosen_result = resolution.chosen_result
        ticket.updated_at = datetime.now(timezone.utc)

        self._persist_if_needed()

        for cb in self._resolution_callbacks:
            try:
                cb(ticket, resolution)
            except Exception as e:
                logger.error("Resolution callback failed: %s", e)

        logger.info(
            "Ticket %s resolved (type=%s, by=%s)",
            ticket_id,
            resolution.resolution_type,
            resolution.resolved_by,
        )
        return ticket

    def escalate_ticket(self, ticket_id: str, reason: str, escalated_by: str) -> MismatchTicket:
        ticket = self._tickets.get(ticket_id)
        if not ticket:
            raise KeyError(f"Ticket not found: {ticket_id}")
        ticket.status = TicketStatus.ESCALATED
        ticket.priority = TicketPriority.CRITICAL
        ticket.updated_at = datetime.now(timezone.utc)
        note = f"[{datetime.now(timezone.utc).isoformat()}] Escalated by {escalated_by}: {reason}"
        ticket.resolution_notes = (
            (ticket.resolution_notes + "\n") if ticket.resolution_notes else ""
        ) + note
        self._persist_if_needed()
        logger.warning("Ticket %s escalated by %s: %s", ticket_id, escalated_by, reason)
        return ticket

    # -- Stats -----------------------------------------------------------
    def get_statistics(self) -> TicketStatistics:
        stats = TicketStatistics()
        if not self._tickets:
            return stats

        status_counts = defaultdict(int)
        priority_counts = defaultdict(int)
        resolution_counts = defaultdict(int)
        solver_counts = defaultdict(int)
        resolution_times: list[float] = []

        for t in self._tickets.values():
            status_counts[t.status] += 1
            priority_counts[t.priority] += 1
            if t.resolution_type:
                resolution_counts[t.resolution_type] += 1
            for r in t.results:
                solver_counts[r.solver_name] += 1
            if t.resolved_at and t.created_at:
                resolution_times.append((t.resolved_at - t.created_at).total_seconds() / 3600.0)

        stats.total_tickets = len(self._tickets)
        stats.open_tickets = status_counts.get(TicketStatus.OPEN, 0)
        stats.under_review_tickets = status_counts.get(TicketStatus.UNDER_REVIEW, 0)
        stats.resolved_tickets = status_counts.get(TicketStatus.RESOLVED, 0)
        stats.escalated_tickets = status_counts.get(TicketStatus.ESCALATED, 0)
        stats.by_priority = dict(priority_counts)
        stats.by_resolution_type = dict(resolution_counts)
        stats.by_solver = dict(solver_counts)
        if resolution_times:
            stats.avg_resolution_time_hours = sum(resolution_times) / len(resolution_times)
        return stats

    # -- Persistence -----------------------------------------------------
    def _persist_if_needed(self) -> None:
        if self.storage_mode == "file":
            self._save_to_disk()

    def _save_to_disk(self) -> None:
        for ticket_id, ticket in self._tickets.items():
            p = STORAGE_PATH / f"{ticket_id}.json"
            with open(p, "w", encoding="utf-8") as f:
                json.dump(ticket.model_dump(), f, indent=2, default=str)

    def _load_from_disk(self) -> None:
        if not STORAGE_PATH.exists():
            return
        for p in STORAGE_PATH.glob("MM-*.json"):
            try:
                with open(p, encoding="utf-8") as f:
                    data = json.load(f)
                t = MismatchTicket(**data)
                self._tickets[t.ticket_id] = t
            except Exception as e:
                logger.error("Failed to load ticket %s: %s", p, e)

    # -- Callbacks -------------------------------------------------------
    def register_resolution_callback(
        self, callback: Callable[[MismatchTicket, TicketResolution], None]
    ) -> None:
        self._resolution_callbacks.append(callback)
        logger.debug(
            "Registered resolution callback: %s",
            getattr(callback, "__name__", str(callback)),
        )


# Singleton
mismatch_service = MismatchService()

```


===== FILE: services/raas_service/main.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/raas_service/main.py           |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+

# |                          CERTEUS                            |

# +-------------------------------------------------------------+

# | FILE: services/raas_service/main.py                       |

# | ROLE: Project module.                                       |

# | PLIK: services/raas_service/main.py                       |

# | ROLA: Moduł projektu.                                       |

# +-------------------------------------------------------------+


"""



PL: Moduł CERTEUS – uzupełnij opis funkcjonalny.



EN: CERTEUS module – please complete the functional description.



"""


# +-------------------------------------------------------------+


# |                          CERTEUS                            |


# +-------------------------------------------------------------+


# | FILE: services/raas_service/main.py                       |


# | ROLE: Project module.                                       |


# | PLIK: services/raas_service/main.py                       |


# | ROLA: Moduł projektu.                                       |


# +-------------------------------------------------------------+

```


===== FILE: services/sipp_indexer_service/__init__.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/sipp_indexer_service/__init__.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/sipp_indexer_service/__init__.py             |
# | ROLE: Package marker for SIPP Indexer Service.              |
# +-------------------------------------------------------------+

"""
PL: Pakiet serwisu indeksującego akty prawne (SIPP Indexer).
EN: Package for the legal acts indexing service (SIPP Indexer).
"""

```


===== FILE: services/sipp_indexer_service/index_isap.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/sipp_indexer_service/index_isap.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
PL: Generator migawek ISAP. Buduje pojedynczy plik JSON `<act_id>.json`
    z polami `snapshot_timestamp` i `_certeus.snapshot_timestamp_utc`.
EN: ISAP snapshot generator. Produces a single JSON `<act_id>.json` with
    `snapshot_timestamp` and `_certeus.snapshot_timestamp_utc` fields.
"""

from __future__ import annotations

import json
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from hashlib import sha256
from pathlib import Path
from typing import Any


@dataclass
class ActSnapshot:
    act_id: str
    version_id: str
    text_sha256: str
    source_url: str
    title: str | None
    text: str
    snapshot_timestamp: str
    at: str | None = None
    _certeus: dict[str, Any] = field(default_factory=dict)


def _snapshot_for(act_id: str) -> ActSnapshot:
    text = (
        "Art. 286 k.k.: Kto, w celu osiągnięcia korzyści majątkowej, "
        "doprowadza inną osobę do niekorzystnego rozporządzenia mieniem "
        "za pomocą wprowadzenia w błąd..."
    )
    digest = "sha256:" + sha256(text.encode("utf-8")).hexdigest()
    now = datetime.now(timezone.utc).isoformat(timespec="seconds")

    return ActSnapshot(
        act_id=act_id,
        version_id="2023-10-01",
        text_sha256=digest,
        source_url="https://isap.sejm.gov.pl/isap.nsf/DocDetails.xsp?id=WDU19970880553",
        title="Kodeks karny – art. 286",
        text=text,
        snapshot_timestamp=now,
        at=None,
        _certeus={"snapshot_timestamp_utc": now},
    )


def index_act(act_id: str, out_dir: Path | None = None) -> Path:
    """Create a single JSON snapshot file and return its path."""
    snap = _snapshot_for(act_id)
    out_dir = Path(out_dir or Path("snapshots"))
    out_dir.mkdir(parents=True, exist_ok=True)
    path = out_dir / f"{act_id}.json"
    path.write_text(json.dumps(asdict(snap), ensure_ascii=False, indent=2), encoding="utf-8")
    return path

```


===== FILE: services/sipp_indexer_service/isap_adapter.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/sipp_indexer_service/isap_adapter.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/sipp_indexer_service/isap_adapter.py         |
# | ROLE: Stub adapter for ISAP. Creates LegalActSnapshot.      |
# +-------------------------------------------------------------+

"""
PL: Stub adaptera do ISAP. Wersja MVP: zwraca stałą migawkę dla
    podanego act_id, z poprawnym SHA256 i timestampem UTC.
EN: ISAP adapter stub. MVP version: returns a constant snapshot
    for a given act_id with proper SHA256 and UTC timestamp.
"""

from __future__ import annotations

import hashlib
from datetime import date, datetime, timezone

from .models import LegalActSnapshot


def _ascii_info(msg: str) -> None:
    try:
        from utils.console import info as _info  # type: ignore

        s = msg.encode("ascii", "ignore").decode("ascii")
        _info(s)
    except Exception:
        s = msg.encode("ascii", "ignore").decode("ascii")
        print(f"[INFO] {s}")


class IsapAdapter:
    """
    PL: Symuluje pobranie aktu prawnego i tworzy migawkę.
    EN: Simulates fetching a legal act and creating its snapshot.
    """

    def fetch_act_snapshot(self, act_id: str, at: date | None = None) -> LegalActSnapshot:
        """
        PL: Zwraca LegalActSnapshot dla zadanego act_id. Parametr 'at' to
            data, na ktora prosimy o stan prawa (stub ignoruje).
        EN: Returns LegalActSnapshot for given act_id. 'at' asks for state
            at given date (ignored by stub).
        """
        _ascii_info(f"ISAP Stub fetching act_id={act_id} at={at}")

        mock_text = (
            "Art. 286. § 1. Kto, w celu osiagniecia korzysci majatkowej, doprowadza "
            "inna osobe do niekorzystnego rozporzadzenia mieniem za pomoca wprowadzenia "
            "jej w blad albo wyzyskania bledu lub niezdolnosci do nalezytego pojmowania "
            "przedsiewzietego dzialania, podlega karze pozbawienia wolnosci od 6 miesiecy do lat 8."
        )
        text_hash = f"sha256:{hashlib.sha256(mock_text.encode('utf-8')).hexdigest()}"

        snapshot = LegalActSnapshot(
            act_id=act_id,
            version_id="2023-10-01",
            text_sha256=text_hash,
            source_url="https://isap.sejm.gov.pl/isap.nsf/DocDetails.xsp?id=WDU19970880553",
            valid_from=date(2023, 10, 1),
            snapshot_timestamp=datetime.now(timezone.utc),
        )
        return snapshot

```


===== FILE: services/sipp_indexer_service/models.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/sipp_indexer_service/models.py |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: services/sipp_indexer_service/models.py               |
# | ROLE: Pydantic DTOs for SIPP Indexer (LegalActSnapshot).    |
# +-------------------------------------------------------------+

"""
PL: Modele danych Pydantic v2 dla SIPP Indexer.
EN: Pydantic v2 data models for the SIPP Indexer.
"""

from __future__ import annotations

from datetime import date, datetime

from pydantic import BaseModel, Field


class LegalActSnapshot(BaseModel):
    """
    PL: Jedna, zwersjonowana migawka aktu prawnego.
    EN: A single, versioned snapshot of a legal act.
    """

    act_id: str = Field(
        ...,
        description="PL: Id aktu (np. 'kk-art-286'). EN: Act identifier (e.g., 'kk-art-286').",
    )
    version_id: str = Field(
        ...,
        description="PL: Id wersji (np. data publikacji). EN: Version id (e.g., publication date).",
    )
    text_sha256: str = Field(
        ...,
        description="PL: SHA256 tekstu wersji aktu. EN: SHA256 of act text for this version.",
    )
    source_url: str = Field(
        ...,
        description="PL: Oficjalny URL (np. ISAP). EN: Official source URL (e.g., ISAP).",
    )
    valid_from: date = Field(
        ...,
        description="PL: Data obowiązywania wersji. EN: Date from which this version is valid.",
    )
    snapshot_timestamp: datetime = Field(
        ...,
        description="PL: Znacznik czasu wykonania migawki. EN: Timestamp of snapshot creation.",
    )

```


===== FILE: services/zkp_service/stub.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/services/zkp_service/stub.py            |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+

# |                          CERTEUS                            |

# +-------------------------------------------------------------+

# | FILE: services/zkp_service/stub.py                        |

# | ROLE: Project module.                                       |

# | PLIK: services/zkp_service/stub.py                        |

# | ROLA: Moduł projektu.                                       |

# +-------------------------------------------------------------+


"""



PL: Moduł CERTEUS – uzupełnij opis funkcjonalny.



EN: CERTEUS module – please complete the functional description.



"""


# +-------------------------------------------------------------+


# |                          CERTEUS                            |


# +-------------------------------------------------------------+


# | FILE: services/zkp_service/stub.py                        |


# | ROLE: Project module.                                       |


# | PLIK: services/zkp_service/stub.py                        |


# | ROLA: Moduł projektu.                                       |


# +-------------------------------------------------------------+


def prove(data):
    return b"zkp"

```


===== FILE: storage/proof_cache/cache.py =====
```text
# +=====================================================================+
# |                          CERTEUS — HEART                            |
# +=====================================================================+
# | FILE: storage/proof_cache/cache.py                                  |
# | ROLE:                                                               |
# |  PL: Klucze cache PCO (czasowe kubełki 6h).                         |
# |  EN: PCO cache keys (6h time buckets).                              |
# +=====================================================================+

"""PL: cache_key = ruleset|query|ctx|jurisdiction|pack|bucket. EN: see above."""

from __future__ import annotations

import hashlib
import math
import time


def time_bucket(now: float, seconds: int = 21600) -> str:
    """PL: Wiadro czasowe 6h. EN: 6-hour time bucket."""
    return str(int(math.floor(now / seconds)))


def cache_key(
    ruleset_hash: str, query_hash: str, ctx_hash: str, jurisdiction: str, norm_pack_id: str, now: float | None = None
) -> str:
    """PL: Oblicz klucz cache. EN: Compute cache key."""
    if now is None:
        now = time.time()
    tb = time_bucket(now)
    raw = "|".join([ruleset_hash, query_hash, ctx_hash, jurisdiction, norm_pack_id, tb])
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()

```


===== FILE: templates/answer_contract.docx.placeholder =====
```text
placeholder

```


===== FILE: templates/answer_contract.txt.placeholder =====
```text
# +-------------------------------------------------------------+
# |              CERTEUS - RAPORT ANALIZY SPRAWY                |
# +-------------------------------------------------------------+

===============================================================
PODSUMOWANIE ANALIZY
===============================================================

ID SPRAWY: ${CASE_ID}
DATA ANALIZY: ${ANALYSIS_DATE}

GŁÓWNA TEZA:
${THESIS}

===============================================================
WYNIK WERYFIKACJI FORMALNEJ
===============================================================

STATUS: ${VERIFICATION_STATUS}

MODEL ROZWIĄZANIA (jeśli dotyczy):
${SOLUTION_MODEL}

===============================================================
DOWÓD POCHODZENIA (PROVENANCE HASH)
===============================================================

${PROVENANCE_HASH}

---
Raport wygenerowany automatycznie przez system CERTEUS v0.1.
Dowód. Nie opinia.

```


===== FILE: templates/truth_bill.docx.placeholder =====
```text
placeholder

```


===== FILE: tests/conftest.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/conftest.py                       |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: tests/conftest.py                                     |
# | ROLE: Shared pytest fixtures & test helpers.                |
# | PLIK: tests/conftest.py                                     |
# | ROLA: Wspólne fikstury pytest i pomocniki testowe.          |
# +-------------------------------------------------------------+
"""
PL: Zbiór współdzielonych fikstur i pomocników testowych dla całego pakietu testów.
EN: Shared pytest fixtures and helpers used across the test suite.
"""

from __future__ import annotations

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

```


===== FILE: tests/e2e/test_e2e_export_endpoint.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/e2e/test_e2e_export_endpoint.py   |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                CERTEUS - Export API E2E Test                |
# +-------------------------------------------------------------+
# | FILE: tests/e2e/test_e2e_export_endpoint.py                 |
# | ROLE: Validates /v1/export endpoint end-to-end.             |
# | PLIK: tests/e2e/test_e2e_export_endpoint.py                 |
# | ROLA: Weryfikuje endpoint /v1/export E2E.                   |
# +-------------------------------------------------------------+
"""
PL: Test E2E endpointu /v1/export – oczekuje ścieżki do wygenerowanego raportu.
EN: E2E test for /v1/export endpoint – expects path to generated report.
"""

# [BLOCK: IMPORTS]
from __future__ import annotations

from pathlib import Path
from typing import Any, TypedDict

from fastapi.testclient import TestClient

from services.api_gateway.main import app


# [BLOCK: TYPES]
class ExportPayload(TypedDict):
    """PL: Struktura żądania dla /v1/export.
    EN: Request shape for /v1/export."""

    case_id: str
    analysis_result: dict[str, Any]


# [BLOCK: CLIENT]
client = TestClient(app)


# +-------------------------------------------------------------+
# | TEST: /v1/export returns generated report path              |
# +-------------------------------------------------------------+
def test_export_endpoint_returns_path() -> None:
    # [BLOCK: ARRANGE]
    payload: ExportPayload = {
        "case_id": "pl-286kk-0001",
        "analysis_result": {"status": "sat", "model": "[x=True]"},
    }

    # [BLOCK: ACT]
    response = client.post("/v1/export", json=payload)

    # [BLOCK: ASSERT]
    assert response.status_code == 200
    body: dict[str, Any] = response.json()

    # podstawowe oczekiwania
    assert "path" in body and isinstance(body["path"], str)
    assert "message" in body and isinstance(body["message"], str)
    assert body["message"].lower().startswith("report generated")

    # konwencja nazwy pliku (zgodna z ExporterService)
    assert Path(body["path"]).name == f"raport_{payload['case_id']}.txt"

```


===== FILE: tests/e2e/test_e2e_pl_286kk_0001.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/e2e/test_e2e_pl_286kk_0001.py     |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: tests/e2e/test_e2e_pl_286kk_0001.py                   |
# | ROLE: End-to-end test for Art. 286 orchestration            |
# | PLIK: tests/e2e/test_e2e_pl_286kk_0001.py                   |
# | ROLA: Test E2E orkiestracji dla art. 286 k.k.               |
# +-------------------------------------------------------------+

"""
CERTEUS — E2E Test: Art. 286 k.k.
PL: Test end-to-end kanonicznego przypadku oszustwa dla /v1/analyze.
EN: End-to-end test of canonical fraud case via /v1/analyze.
"""

import io

from fastapi.testclient import TestClient

from services.api_gateway.main import app

client = TestClient(app)


def test_full_analysis_returns_sat():
    payload = ("dowody.pdf", io.BytesIO(b"fake bytes"), "application/pdf")
    r = client.post("/v1/analyze?case_id=pl-286kk-0001", files={"file": payload})
    assert r.status_code == 200
    data = r.json()
    assert data["case_id"] == "pl-286kk-0001"
    assert data["analysis_result"]["status"] == "sat"
    assert "model" in data["analysis_result"]

```


===== FILE: tests/plugins/test_plugins_registry.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/plugins/test_plugins_registry.py  |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+

# |                          CERTEUS                            |

# +-------------------------------------------------------------+

# | FILE: tests/plugins/test_plugins_registry.py              |

# | ROLE: Project module.                                       |

# | PLIK: tests/plugins/test_plugins_registry.py              |

# | ROLA: Moduł projektu.                                       |

# +-------------------------------------------------------------+


"""



PL: Moduł CERTEUS – uzupełnij opis funkcjonalny.



EN: CERTEUS module – please complete the functional description.



"""


# +-------------------------------------------------------------+


# |                          CERTEUS                            |


# +-------------------------------------------------------------+


# | FILE: tests/plugins/test_plugins_registry.py              |


# | ROLE: Project module.                                       |


# | PLIK: tests/plugins/test_plugins_registry.py              |


# | ROLA: Moduł projektu.                                       |


# +-------------------------------------------------------------+


# +------------------------------------------------+


# |                  CERTEUS                       |


# |                Core Plugin API                 |


# +------------------------------------------------+


#


# PL: Minimalne API rdzenia do rejestracji pluginów oraz


#     rejestrów funkcjonalnych (adapters/exporters).


#


# EN: Minimal core API for plugin registration and


#     functional registries (adapters/exporters).


#


# Konwencje (README "Premium Code Style"):


# - Logo, dwujęzyczne komentarze, sekcje, type hints.


#

from __future__ import annotations

from typing import Any


class PluginAPI:
    """



    PL: Prosty rejestr pluginów i entrypointów funkcjonalnych.



        - register(plugin) lub register(name, plugin)



        - adapters/exporters jako słowniki funkcji/obiektów







    EN: Simple registry for plugins and functional entrypoints.



        - register(plugin) or register(name, plugin)



        - adapters/exporters as dicts of callables/objects



    """

    def __init__(self) -> None:
        # === STAN / STATE ===

        self._plugins: dict[str, object] = {}

        self.adapters: dict[str, object] = {}

        self.exporters: dict[str, object] = {}

    # === REJESTRACJA PLUGINU / PLUGIN REGISTRATION ===

    def register(self, *args: Any) -> None:
        """



        PL: Obsługuje dwie sygnatury:



            • register(plugin)



            • register(name, plugin)







        EN: Supports two signatures:



            • register(plugin)



            • register(name, plugin)



        """

        if len(args) == 1:
            plugin = args[0]

            # Nazwa z atrybutów lub modułu / Name from attrs or module

            name = getattr(plugin, "name", None) or getattr(plugin, "__plugin_name__", None)

            if not name:
                mod = getattr(plugin, "__module__", "")

                # Utnij końcówkę .src.main jeżeli występuje / trim .src.main suffix

                if mod.endswith(".src.main"):
                    parts = mod.split(".")

                    # … ['plugins', '<name>', 'src', 'main'] → weź index -3

                    name = parts[-3] if len(parts) >= 3 else parts[-1]

                else:
                    name = mod.split(".")[-1] if mod else plugin.__class__.__name__

        else:
            name, plugin = args[0], args[1]

        self._plugins[str(name)] = plugin

    # === PODGLĄD / INTROSPECTION ===

    def list_plugins(self):
        """



        PL: Zwróć widok kluczy (dict_keys), zgodnie z oczekiwaniem testów.



        EN: Return keys view (dict_keys), as tests expect.



        """

        return self._plugins.keys()

    # === REJESTRY FUNKCYJNE / FUNCTIONAL REGISTRIES ===

    def register_adapter(self, key: str, fn: object) -> None:
        """PL: Zarejestruj adapter (np. isap.pl.snapshot). EN: Register adapter."""

        self.adapters[key] = fn

    def register_exporter(self, key: str, fn: object) -> None:
        """PL: Zarejestruj eksporter (np. pl.exporter.docx_pdf). EN: Register exporter."""

        self.exporters[key] = fn

```


===== FILE: tests/services/test_api_gateway.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_api_gateway.py      |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+

# |                          CERTEUS                            |

# +-------------------------------------------------------------+

# | FILE: tests/services/test_api_gateway.py                  |

# | ROLE: Project module.                                       |

# | PLIK: tests/services/test_api_gateway.py                  |

# | ROLA: Moduł projektu.                                       |

# +-------------------------------------------------------------+


"""



PL: Moduł CERTEUS – uzupełnij opis funkcjonalny.



EN: CERTEUS module – please complete the functional description.



"""


# +-------------------------------------------------------------+


# |                          CERTEUS                            |


# +-------------------------------------------------------------+


# | FILE: tests/services/test_api_gateway.py                  |


# | ROLE: Project module.                                       |


# | PLIK: tests/services/test_api_gateway.py                  |


# | ROLA: Moduł projektu.                                       |


# +-------------------------------------------------------------+


def test_smoke():
    assert 1 == 1

```


===== FILE: tests/services/test_exporter.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_exporter.py         |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |               CERTEUS - Exporter Service Tests              |
# +-------------------------------------------------------------+
# | FILE: tests/services/test_exporter.py                       |
# +-------------------------------------------------------------+
"""
PL: Testy serwisu ExporterService – sprawdza poprawność wypełnienia szablonu.
EN: Tests for ExporterService – verifies template population correctness.
"""

from __future__ import annotations

import tempfile
from pathlib import Path

from services.exporter_service import ExporterService


def test_export_report_creates_file_with_replaced_content():
    with tempfile.TemporaryDirectory() as tmpdir:
        outdir = Path(tmpdir) / "out"
        service = ExporterService(template_dir="templates", output_dir=str(outdir))
        case_id = "test-export-007"
        analysis = {"status": "sat", "model": "[a=True,b=False]"}

        report_path = service.export_report(case_id, analysis)

        assert report_path.exists()
        content = report_path.read_text(encoding="utf-8")
        assert case_id in content
        assert "SAT" in content
        assert "[a=True,b=False]" in content

```


===== FILE: tests/services/test_exporter_provenance.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_exporter_provenance.py|
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
PL: Testy jednostkowe / integracyjne modułu.
EN: Module test suite (unit/integration).
"""

# +-------------------------------------------------------------+
# | CERTEUS - Tests: Exporter & Ledger                          |
# +-------------------------------------------------------------+
from pathlib import Path

from services.exporter_service import export_answer_to_txt
from services.ledger_service.ledger import (
    compute_provenance_hash,
    verify_provenance_hash,
)


def test_export_and_hash(tmp_path: Path):
    answer = {
        "case_id": "pl-001",
        "title": "T",
        "thesis": "X",
        "reasoning": "Y",
        "status": "ok",
        "confidence": 0.9,
    }
    out = tmp_path / "out.txt"
    path = export_answer_to_txt(answer, out_path=str(out), create_ledger_entry=False)
    assert Path(path).exists()

    h = compute_provenance_hash(answer, include_timestamp=False)
    assert verify_provenance_hash(answer, h)

```


===== FILE: tests/services/test_ingest.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_ingest.py           |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# | Core Engine for Reliable & Unified Systems                  |
# +-------------------------------------------------------------+
# | FILE: tests/services/test_ingest.py                         |
# | ROLE: Integration tests for /v1/ingest endpoint.            |
# | PLIK: tests/services/test_ingest.py                         |
# | ROLA: Testy integracyjne endpointu /v1/ingest.              |
# +-------------------------------------------------------------+

"""
PL: Testy sprawdzają ścieżkę sukcesu (OCR stub → FACTLOG), nagłówek łańcucha
    Ledger oraz walidacje MIME i limit rozmiaru.
EN: Tests cover the success path (OCR stub → FACTLOG), the Ledger chain header,
    and MIME/size validations.
"""

# [BLOCK: IMPORTS]
from __future__ import annotations

import io
from typing import IO, Any, Literal, cast

from fastapi import FastAPI
from fastapi.testclient import TestClient

# Import jako moduł, a typ nadamy przez cast — to ucisza Pylance.
import services.api_gateway.main as api_main

# [BLOCK: CLIENT]
fastapi_app_typed: FastAPI = cast(FastAPI, api_main.app)  # type: ignore[attr-defined]
client: TestClient = TestClient(fastapi_app_typed)

# [BLOCK: TYPES]
# (filename, fileobj, mimetype)
FileTuple = tuple[str, IO[bytes], str]

LEDGER_HEADER: Literal["X-CERTEUS-Ledger-Chain"] = "X-CERTEUS-Ledger-Chain"


def make_file(filename: str, content: bytes, mimetype: str) -> FileTuple:
    """
    PL: Zwraca krotkę pliku akceptowaną przez TestClient (multipart/form-data).
    EN: Returns a file tuple accepted by TestClient (multipart/form-data).
    """
    fileobj: IO[bytes] = io.BytesIO(content)
    return (filename, fileobj, mimetype)


# [BLOCK: TEST • SUCCESS PATH]
def test_ingest_document_endpoint_success() -> None:
    """
    PL: Wysyła mały „PDF” i oczekuje listy 2 faktów oraz poprawnego nagłówka Ledger.
    EN: Sends a small “PDF” and expects 2 facts and a valid Ledger header.
    """
    files: dict[str, FileTuple] = {
        "file": make_file("test.pdf", b"%PDF-1.4 dummy", "application/pdf")
    }

    resp = client.post("/v1/ingest", files=files)
    assert resp.status_code == 200

    data_json: list[dict[str, Any]] = cast(list[dict[str, Any]], resp.json())
    assert isinstance(data_json, list)
    assert len(data_json) == 2

    # roles set
    roles: set[str] = {entry["role"] for entry in data_json}
    assert roles == {"claim_contract_date", "evidence_payment"}

    # structure spot-check
    first: dict[str, Any] = data_json[0]
    assert "fact_id" in first and isinstance(first["fact_id"], str)
    assert "thesis" in first and isinstance(first["thesis"], str)
    assert "confidence_score" in first and isinstance(first["confidence_score"], int | float)

    # ledger header
    assert LEDGER_HEADER in resp.headers
    chain = resp.headers[LEDGER_HEADER]
    assert chain  # non-empty
    parts: list[str] = chain.split(";")
    assert all(p.startswith("sha256:") for p in parts)


# [BLOCK: TEST • MIME VALIDATION]
def test_ingest_rejects_unsupported_mime() -> None:
    """
    PL: MIME spoza białej listy → 415.
    EN: MIME outside the allowlist → 415.
    """
    files_bad: dict[str, FileTuple] = {
        "file": make_file("x.exe", b"MZ...", "application/x-msdownload")
    }
    resp = client.post("/v1/ingest", files=files_bad)
    assert resp.status_code == 415


# [BLOCK: TEST • SIZE LIMIT]
def test_ingest_rejects_too_large() -> None:
    """
    PL: Plik większy niż 10 MiB → 413.
    EN: File larger than 10 MiB → 413.
    """
    # API uses MAX_BYTES = 10 * 1024 * 1024; send +1 byte
    big: bytes = b"0" * (10 * 1024 * 1024 + 1)
    files_big: dict[str, FileTuple] = {"file": make_file("big.pdf", big, "application/pdf")}
    resp = client.post("/v1/ingest", files=files_big)
    assert resp.status_code == 413

```


===== FILE: tests/services/test_ledger.py =====
```text
#!/usr/bin/env python3
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_ledger.py           |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
PL: Testy Ledger – sprawdzanie łańcucha skrótów i nagłówków.
EN: Ledger tests – verification of hash chain and headers.
"""

from __future__ import annotations

import json
from collections.abc import Mapping
from datetime import datetime, timezone
from hashlib import sha256
from typing import Any


def _normalize_for_hash(data: Mapping[str, Any], *, include_timestamp: bool) -> bytes:
    if not include_timestamp and "timestamp" in data:
        work = {k: v for k, v in data.items() if k != "timestamp"}
    else:
        work = dict(data)
    return json.dumps(work, sort_keys=True, separators=(",", ":")).encode("utf-8")


def compute_provenance_hash(data: Mapping[str, Any], *, include_timestamp: bool = False) -> str:
    return sha256(_normalize_for_hash(data, include_timestamp=include_timestamp)).hexdigest()


def verify_provenance_hash(
    data: Mapping[str, Any], expected_hash: str, *, include_timestamp: bool = False
) -> bool:
    return compute_provenance_hash(data, include_timestamp=include_timestamp) == expected_hash


class LedgerRecord:
    """
    Elastyczny rekord księgi:
      - przyjmuje zarówno 'document_hash', jak i alias 'hash'
      - ma opcjonalne 'meta'
      - wszystkie pola mają domyślne wartości (żeby test mógł wołać bez wszystkich argów)
    """

    def __init__(
        self,
        *,
        event_id: int = 0,
        type: str = "INPUT_INGESTION",
        case_id: str = "",
        document_hash: str | None = None,
        timestamp: str = "",
        chain_prev: str | None = None,
        chain_self: str = "",
        meta: Mapping[str, Any] | None = None,
        **extras: Any,
    ) -> None:
        # aliasy z testów: 'hash' -> document_hash
        if document_hash is None and "hash" in extras:
            document_hash = str(extras.pop("hash"))

        self.event_id = event_id
        self.type = type
        self.case_id = case_id
        self.document_hash = document_hash
        self.timestamp = timestamp
        self.chain_prev = chain_prev
        self.chain_self = chain_self
        self.meta = dict(meta) if meta is not None else None
        # ignorujemy inne nadmiarowe klucze, żeby nie wywalać testów

    # alias tylko do odczytu (jeśli ktoś próbuje sięgnąć po .hash)
    @property
    def hash(self) -> str | None:  # noqa: D401
        return self.document_hash


class Ledger:
    def __init__(self) -> None:
        self._events: list[LedgerRecord] = []

    # --- API wprost używane w testach ---------------------------------
    def append(self, rec: LedgerRecord) -> None:
        """Dodaj istniejący rekord (testy tego używają)."""
        self._events.append(rec)

    def read_all(self, case_id: str | None = None) -> list[LedgerRecord]:
        """Zwróć wszystkie rekordy (opcjonalnie tylko dla case_id)."""
        if case_id is None:
            return list(self._events)
        return [r for r in self._events if r.case_id == case_id]

    # --- API używane przez nasze routery ------------------------------
    def _next_event_id(self) -> int:
        return len(self._events) + 1

    def _now_iso(self) -> str:
        return datetime.now(timezone.utc).isoformat()

    def _chain(self, payload: dict[str, Any], prev: str | None) -> str:
        body = dict(payload)
        if prev:
            body["prev"] = prev
        return sha256(
            json.dumps(body, sort_keys=True, separators=(",", ":")).encode("utf-8")
        ).hexdigest()

    def record_input(self, *, case_id: str, document_hash: str) -> dict[str, Any]:
        event_id = self._next_event_id()
        ts = self._now_iso()
        prev = self._events[-1].chain_self if self._events else None

        payload = {
            "event_id": event_id,
            "type": "INPUT_INGESTION",
            "case_id": case_id,
            "document_hash": document_hash,
            "timestamp": ts,
        }
        chain_self = self._chain(payload, prev)

        rec = LedgerRecord(
            event_id=event_id,
            type="INPUT_INGESTION",
            case_id=case_id,
            document_hash=document_hash,
            timestamp=ts,
            chain_prev=prev,
            chain_self=chain_self,
        )
        self._events.append(rec)

        return {
            "event_id": rec.event_id,
            "type": rec.type,
            "case_id": rec.case_id,
            "document_hash": rec.document_hash,
            "timestamp": rec.timestamp,
            "chain_prev": rec.chain_prev,
            "chain_self": rec.chain_self,
        }

    def get_records_for_case(self, *, case_id: str) -> list[dict[str, Any]]:
        """Wersja słownikowa (używana przez router /ledger)."""
        return [
            {
                "event_id": r.event_id,
                "type": r.type,
                "case_id": r.case_id,
                "document_hash": r.document_hash,
                "timestamp": r.timestamp,
                "chain_prev": r.chain_prev,
                "chain_self": r.chain_self,
            }
            for r in self._events
            if r.case_id == case_id
        ]


# pojedyncza instancja do użycia przez routery
ledger_service = Ledger()

__all__ = [
    "Ledger",
    "LedgerRecord",
    "ledger_service",
    "compute_provenance_hash",
    "verify_provenance_hash",
]

```


===== FILE: tests/services/test_lexlog.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_lexlog.py           |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |                   LEXLOG Parser Tests                       |
# +-------------------------------------------------------------+

"""
PL: Testuje minimalny parser LEXLOG na pliku kk.lex.
EN: Tests the minimal LEXLOG parser on kk.lex file.
"""

from __future__ import annotations

from pathlib import Path

from services.lexlog_parser.parser import parse_lexlog


def test_lexlog_parser_kk_lex() -> None:
    lex_path = Path("packs") / "jurisdictions" / "PL" / "rules" / "kk.lex"
    assert lex_path.exists(), "Missing packs/jurisdictions/PL/rules/kk.lex"

    content = lex_path.read_text(encoding="utf-8")
    ast = parse_lexlog(content)

    # Defines present
    names = {d.name for d in ast.defines}
    assert {
        "cel_korzysci_majatkowej",
        "wprowadzenie_w_blad",
        "niekorzystne_rozporzadzenie_mieniem",
    }.issubset(names)

    # Premises present
    pids = {p.id for p in ast.premises}
    assert {"P_CEL", "P_WPROWADZENIE", "P_ROZPORZADZENIE"}.issubset(pids)

    # Rule ties premises to conclusion
    rule = next(r for r in ast.rules if r.id == "R_286_OSZUSTWO")
    assert rule.conclusion == "K_OSZUSTWO_STWIERDZONE"
    assert set(rule.premises) == {"P_CEL", "P_WPROWADZENIE", "P_ROZPORZADZENIE"}

    # Conclusion has assertion
    concl = next(c for c in ast.conclusions if c.id == "K_OSZUSTWO_STWIERDZONE")
    assert concl.assert_expr is not None and "cel_korzysci_majatkowej" in concl.assert_expr

```


===== FILE: tests/services/test_lexlog_eval.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_lexlog_eval.py      |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |                   LEXLOG Evaluator Tests                    |
# +-------------------------------------------------------------+

"""
PL: Testy ewaluatora LEXLOG (MVP) dla art. 286 k.k., z loaderem mapowania.
EN: LEXLOG evaluator tests (MVP) for art. 286 PC, with mapping loader.
"""

from __future__ import annotations

from pathlib import Path

from services.lexlog_parser.evaluator import choose_article_for_kk, evaluate_rule
from services.lexlog_parser.mapping import load_mapping
from services.lexlog_parser.parser import parse_lexlog


def _load_kk_ast():
    lex_path = Path("packs") / "jurisdictions" / "PL" / "rules" / "kk.lex"
    content = lex_path.read_text(encoding="utf-8")
    return parse_lexlog(content)


def _load_ctx():
    map_path = Path("packs") / "jurisdictions" / "PL" / "rules" / "kk.mapping.json"
    assert map_path.exists(), "Missing kk.mapping.json"
    return load_mapping(map_path)


def test_286_passes_when_wprowadzenie_true_and_excludes_false() -> None:
    ast = _load_kk_ast()
    ctx = _load_ctx()

    flags = {
        "ZNAMIE_WPROWADZENIA_W_BLAD": True,
        "ZNAMIE_POWIERZENIA_MIENIA": False,
    }
    res = evaluate_rule(ast, "R_286_OSZUSTWO", flags, ctx)
    assert res.satisfied is True
    assert res.missing_premises == []
    assert res.failing_excludes == []
    assert choose_article_for_kk(ast, flags, ctx) == "art286"


def test_286_fails_when_excluded_flag_set() -> None:
    ast = _load_kk_ast()
    ctx = _load_ctx()

    flags = {
        "ZNAMIE_WPROWADZENIA_W_BLAD": True,
        "ZNAMIE_POWIERZENIA_MIENIA": True,  # exclude -> blokuje
    }
    res = evaluate_rule(ast, "R_286_OSZUSTWO", flags, ctx)
    assert res.satisfied is False
    assert res.failing_excludes == ["ZNAMIE_POWIERZENIA_MIENIA"]
    assert choose_article_for_kk(ast, flags, ctx) is None

```


===== FILE: tests/services/test_lexlog_parser.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_lexlog_parser.py    |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: tests/services/test_lexlog_parser.py                  |
# | ROLE: Unit tests for LEXLOG parser AST & legacy stub        |
# | PLIK: tests/services/test_lexlog_parser.py                  |
# | ROLA: Testy parsera LEXLOG (AST i zgodność legacy)          |
# +-------------------------------------------------------------+

"""
CERTEUS — LEXLOG Parser Tests
PL: Testy sprawdzające parsowanie: DEFINE/PREMISE/RULE/CONCLUSION,
    normalizację ID oraz zgodność z legacy stubem.
EN: Tests for parsing of DEFINE/PREMISE/RULE/CONCLUSION, canonical ID
    normalization, and legacy stub compatibility.
"""

from pathlib import Path

from services.lexlog_parser.parser import LexlogParser

RULES = Path("packs/jurisdictions/PL/rules/kk.lex")


def test_lexlog_parses_art_286_rule():
    text = RULES.read_text(encoding="utf-8")
    ast = LexlogParser().parse(text)
    assert ast, "AST should not be empty"
    assert ast["rule_id"] == "R_286_OSZUSTWO"
    assert ast["conclusion"] == "K_OSZUSTWO_STWIERDZONE"
    assert set(ast["premises"]) == {"P_CEL", "P_WPROWADZENIE", "P_ROZPORZADZENIE"}
    assert "z3.And(" in ast["smt_assertion"]

```


===== FILE: tests/services/test_mismatch_service.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_mismatch_service.py |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


"""
PL: Testy jednostkowe / integracyjne modułu.
EN: Module test suite (unit/integration).
"""

# +-------------------------------------------------------------+
# | CERTEUS - Tests: Mismatch Service                           |
# +-------------------------------------------------------------+
from services.mismatch_service.models import ResolutionType, TicketResolution
from services.mismatch_service.service import MismatchService


def test_create_and_resolve_ticket():
    svc = MismatchService()
    t = svc.create_ticket(
        case_id="case-001",
        formula_str="p -> q",
        results={"z3": {"status": "sat"}, "cvc5": {"status": "unsat"}},
    )
    assert t.ticket_id.startswith("MM-")
    assert t.priority in ("high", "critical", "medium", "low")

    res = TicketResolution(
        ticket_id=t.ticket_id,
        resolution_type=ResolutionType.HUMAN_OVERRIDE,
        chosen_result="sat",
        notes="Expert decision",
        resolved_by="tester",
        confidence=0.8,
    )
    t2 = svc.resolve_ticket(t.ticket_id, res)
    assert t2.status == "resolved"
    assert t2.chosen_result == "sat"

```


===== FILE: tests/services/test_preview.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  tests/services/test_preview.py                              |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+
# | FILE / PLIK: tests/services/test_preview.py                          |
# | ROLE / ROLA:                                                         |
# |  EN: Integration tests for /v1/preview endpoint (file → preview URL).|
# |  PL: Testy integracyjne endpointu /v1/preview (plik → URL podglądu). |
# +=====================================================================+

"""
PL: Testy weryfikują, że endpoint /v1/preview przyjmuje plik (multipart/form-data)
    i zwraca JSON z kluczem `url` wskazującym ścieżkę podglądu w `/static/previews/...`.

EN: Tests verify that /v1/preview accepts a file (multipart/form-data) and returns
    JSON with a `url` pointing to a preview path under `/static/previews/...`.
"""

from __future__ import annotations

from pathlib import Path

from fastapi.testclient import TestClient

import services.api_gateway.main as api_main


def test_health_ok() -> None:
    client = TestClient(api_main.app)
    r = client.get("/health")
    assert r.status_code == 200
    assert r.json().get("status") == "ok"


def test_static_app_mount() -> None:
    client = TestClient(api_main.app)
    r = client.get("/app/proof_visualizer/index.html")
    assert r.status_code in (
        200,
        404,
    )  # mount działa; plik może nie istnieć w teście CI


def test_preview_upload_roundtrip(tmp_path: Path) -> None:
    client = TestClient(api_main.app)
    sample = tmp_path / "sample.txt"
    sample.write_text("hello", encoding="utf-8")
    with sample.open("rb") as fh:
        r = client.post("/v1/preview", files={"file": ("sample.txt", fh, "text/plain")})
    assert r.status_code == 200
    url = r.json().get("url")
    assert isinstance(url, str) and url.startswith("/static/previews/")

    rel = url.removeprefix("/static/")
    saved = Path("static") / rel
    assert saved.exists()
    saved.unlink(missing_ok=True)

```


===== FILE: tests/services/test_schemas.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_schemas.py          |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+

# |                          CERTEUS                            |

# +-------------------------------------------------------------+

# | FILE: tests/services/test_schemas.py                      |

# | ROLE: Project module.                                       |

# | PLIK: tests/services/test_schemas.py                      |

# | ROLA: Moduł projektu.                                       |

# +-------------------------------------------------------------+


"""



PL: Moduł CERTEUS – uzupełnij opis funkcjonalny.



EN: CERTEUS module – please complete the functional description.



"""


# +-------------------------------------------------------------+


# |                CERTEUS - Schema Contract Tests              |


# +-------------------------------------------------------------+


# | PLIK / FILE: tests/services/test_schemas.py                 |


# | ROLA / ROLE: Sprawdza poprawność przykładowych danych        |


# |             względem 'Świętej Trójcy' schematów.            |


# +-------------------------------------------------------------+


# | STYLE: PL-first, headers & notes dual-language (PL/EN).     |


# +-------------------------------------------------------------+

from __future__ import annotations

# === IMPORTY / IMPORTS ===
import json
from pathlib import Path
from typing import Any, Final, Protocol

import pytest
from jsonschema import Draft7Validator, FormatChecker
from jsonschema.exceptions import ValidationError

# === ALIASY TYPÓW / TYPE ALIASES ===


Schema = dict[str, Any]


JSONObj = dict[str, Any]


SCHEMA_DIR: Final[Path] = Path("schemas")


# === PROTOKÓŁ WALIDATORA / VALIDATOR PROTOCOL ===


class _ValidatorProtocol(Protocol):
    """



    PL: Minimalny interfejs wymagany od walidatora.



    EN: Minimal interface required from a validator.



    """

    def validate(self, instance: Any) -> None: ...


def load_schema(name: str) -> Schema:
    """



    PL: Ładuje i syntaktycznie weryfikuje schemat JSON (Draft7).



    EN: Loads and syntactically checks a JSON schema (Draft7).



    """

    path = SCHEMA_DIR / name

    with path.open("r", encoding="utf-8") as f:
        schema: Schema = json.load(f)

    Draft7Validator.check_schema(schema)

    return schema


def assert_valid(instance: JSONObj, schema: Schema) -> None:
    """



    PL: Waliduje instancję względem danego schematu (z FormatCheckerem).



    EN: Validates the instance against the given schema (with FormatChecker).



    """

    validator: _ValidatorProtocol = Draft7Validator(schema, format_checker=FormatChecker())  # type: ignore[assignment]

    validator.validate(instance)  # pyright: ignore[reportUnknownMemberType]


def assert_invalid(instance: JSONObj, schema: Schema) -> None:
    """



    PL: Oczekuje błędu walidacji.



    EN: Expects validation error.



    """

    validator: _ValidatorProtocol = Draft7Validator(schema, format_checker=FormatChecker())  # type: ignore[assignment]

    with pytest.raises(ValidationError):
        validator.validate(instance)  # pyright: ignore[reportUnknownMemberType]


# === FIXTURE’Y / FIXTURES ===


@pytest.fixture(scope="module")
def S_PROVENANCE() -> Schema:
    return load_schema("provenance_receipt_v1.json")


@pytest.fixture(scope="module")
def S_ANSWER() -> Schema:
    return load_schema("answer_contract_v1.json")


@pytest.fixture(scope="module")
def S_PCA2() -> Schema:
    return load_schema("pca2_v1.json")


# === TESTY: PROVENANCE ===


def test_provenance_valid(S_PROVENANCE: Schema) -> None:
    ok: JSONObj = {
        "case_id": "RS-DOM-vs-Stasikowski",
        "receipt_id": "11111111-2222-3333-4444-555555555555",
        "inputs": {
            "lexlog_rule_version": "pl.lexlog/1.0.0",
            "assumptions_hash": "f" * 64,
            "question_hash": "a" * 64,
        },
        "solvers": {"z3": {"status": "ok", "runtime_ms": 12}, "cvc5": {"status": "ok"}},
        "mismatch": False,
        "final_hash": "0" * 64,
        "timestamp": "2025-08-14T10:00:00Z",
    }

    assert_valid(ok, S_PROVENANCE)


def test_provenance_invalid_missing_required(S_PROVENANCE: Schema) -> None:
    bad: JSONObj = {
        "inputs": {"lexlog_rule_version": "x", "assumptions_hash": "f" * 64},
        "solvers": {},
        "mismatch": True,
        "final_hash": "0" * 64,
        "timestamp": "2025-08-14T10:00:00Z",
    }

    assert_invalid(bad, S_PROVENANCE)


# === TESTY: ANSWER CONTRACT ===


def test_answer_valid(S_ANSWER: Schema) -> None:
    ok: JSONObj = {
        "answer_id": "aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
        "case_id": "RS-DOM-vs-Stasikowski",
        "question": {"text": "Czy §10 ust. 1 pkt 3 ma zastosowanie?", "lang": "pl"},
        "answer": {"text": "Tak/Nie + uzasadnienie...", "lang": "pl", "hash": "1" * 64},
        "citations": [
            {
                "type": "statute",
                "label": "KC art. 647¹",
                "ref": "Dz.U....",
                "locator": "art. 647(1) §2",
            }
        ],
        "metrics": {"confidence": 0.92, "latency_ms": 120},
        "model": {
            "provider": "LEXENITH",
            "name": "GPT-5 Thinking",
            "version": "2025.08",
        },
        "provenance": {"receipt_id": "11111111-2222-3333-4444-555555555555"},
        "timestamps": {"created_at": "2025-08-14T10:01:00Z"},
    }

    assert_valid(ok, S_ANSWER)


def test_answer_invalid_confidence_range(S_ANSWER: Schema) -> None:
    bad: JSONObj = {
        "answer_id": "aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
        "case_id": "X",
        "question": {"text": "foo"},
        "answer": {"text": "bar"},
        "timestamps": {"created_at": "2025-08-14T10:01:00Z"},
        "model": {"provider": "LEXENITH", "name": "GPT-5 Thinking"},
        "metrics": {"confidence": 1.5},
    }

    assert_invalid(bad, S_ANSWER)


# === TESTY: PCA² ===


def test_pca2_valid(S_PCA2: Schema) -> None:
    ok: JSONObj = {
        "case_id": "RS-DOM-vs-Stasikowski",
        "policy": ["lexenith.secure.v1", "legal.pl.civil.kc"],
        "constraints": [{"name": "no_private_data", "expression": "PII==False", "verdict": "pass"}],
        "assumptions": [{"name": "ryczalt", "value": "tak"}],
        "attestations": [
            {
                "subject": "answer#aaaaaaaa",
                "by": "auditor.bot",
                "result": "pass",
                "at": "2025-08-14T10:05:00Z",
            }
        ],
        "audit_trail": [
            {
                "type": "constraint_eval",
                "at": "2025-08-14T10:05:01Z",
                "actor": "cerber",
                "message": "OK",
            }
        ],
        "overall_status": "pass",
        "final_hash": "2" * 64,
    }

    assert_valid(ok, S_PCA2)


def test_pca2_invalid_status(S_PCA2: Schema) -> None:
    bad: JSONObj = {"case_id": "X", "overall_status": "unknown"}

    assert_invalid(bad, S_PCA2)

```


===== FILE: tests/services/test_sipp.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_sipp.py             |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                         CERTEUS                             |
# |      Core Engine for Reliable & Unified Systems             |
# +-------------------------------------------------------------+
# | FILE: tests/services/test_sipp.py                           |
# | ROLE: Integration tests for SIPP Indexer endpoint.          |
# +-------------------------------------------------------------+

"""
PL: Testuje endpoint /v1/sipp/snapshot/{act_id}.
EN: Tests the /v1/sipp/snapshot/{act_id} endpoint.
"""

from __future__ import annotations

import json
from pathlib import Path

from fastapi.testclient import TestClient

from services.api_gateway.main import app

client = TestClient(app)


def test_get_snapshot_endpoint_success() -> None:
    act_id = "kk-art-286"
    resp = client.get(f"/v1/sipp/snapshot/{act_id}")
    assert resp.status_code == 200
    data = resp.json()
    assert data["act_id"] == act_id
    assert data["version_id"] == "2023-10-01"
    assert data["source_url"]
    assert data["text_sha256"].startswith("sha256:")
    assert "snapshot_timestamp" in data


def test_get_snapshot_with_at_param() -> None:
    act_id = "kk-art-286"
    resp = client.get(f"/v1/sipp/snapshot/{act_id}?at=2024-11-24")
    assert resp.status_code == 200
    data = resp.json()
    assert data["act_id"] == act_id
    # Stub ignores 'at', but API must accept the param gracefully.
    assert data["version_id"] == "2023-10-01"


def test_index_isap_writes_snapshot_file(tmp_path: Path) -> None:
    from services.sipp_indexer_service.index_isap import index_act

    p = index_act("kk-art-286", out_dir=tmp_path)  # nie piszemy do repo podczas testu
    assert p.exists()

    data = json.loads(p.read_text(encoding="utf-8"))
    assert data["act_id"] == "kk-art-286"
    assert data["text_sha256"].startswith("sha256:")
    assert "snapshot_timestamp" in data
    assert "_certeus" in data and "snapshot_timestamp_utc" in data["_certeus"]

```


===== FILE: tests/services/test_verify_mismatch.py =====
```text
# +=====================================================================+
# |                          CERTEUS                                    |
# +=====================================================================+
# | MODULE:  F:/projekty/certeus/tests/services/test_verify_mismatch.py  |
# | DATE:    2025-08-17                                                  |
# +=====================================================================+


# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: tests/services/test_verify_mismatch.py                |
# | ROLE: Ensures mismatch maps to HTTP 409 requires_human.     |
# | PLIK: tests/services/test_verify_mismatch.py                |
# | ROLA: Gwarantuje mapowanie rozjazdu na HTTP 409.            |
# +-------------------------------------------------------------+
"""
PL: Test symuluje rozbieżność rdzeni i sprawdza odpowiedź 409.
EN: Test simulates core mismatch and checks 409 response.
"""

# === IMPORTY / IMPORTS ===
from fastapi.testclient import TestClient

from services.api_gateway.main import app


# === TEST / TEST ===
def test_verify_returns_409_on_mismatch(monkeypatch):
    """
    PL: Symulowany MismatchError powinien dać 409 + requires_human:true.
    EN: Simulated MismatchError should return 409 + requires_human:true.
    """
    from kernel import truth_engine

    original = truth_engine.DualCoreVerifier.verify

    def fake_verify(self, formula, lang="smt2"):
        from kernel.mismatch_protocol import MismatchError

        raise MismatchError("forced mismatch")

    monkeypatch.setattr(truth_engine.DualCoreVerifier, "verify", fake_verify)
    client = TestClient(app)
    r = client.post("/v1/verify", json={"formula": "(set-logic QF_LIA)", "lang": "smt2"})
    assert r.status_code == 409
    body = r.json()
    assert body["detail"]["requires_human"] is True

    # porządek
    monkeypatch.setattr(truth_engine.DualCoreVerifier, "verify", original)

```
