# +-------------------------------------------------------------+
# |                          CERTEUS                            |
# +-------------------------------------------------------------+
# | FILE: .github/workflows/ci-gates.yml                      |
# | ROLE: Project YAML manifest.                                |
# | PLIK: .github/workflows/ci-gates.yml                      |
# | ROLA: Manifest YAML projektu.                               |
# +-------------------------------------------------------------+

name: ci-gates

on:
  push:
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  ci-gates:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install tools
        run: |
          python -m pip install -U pip wheel setuptools
          # Install project runtime deps
          python -m pip install -e .
          # Install test/dev helpers explicitly used by the suite
          python -m pip install ruff pytest pytest-asyncio httpx z3-solver hypothesis openapi-spec-validator PyYAML

      - name: Load CI defaults (non-secrets)
        run: |
          python scripts/ci/load_env_defaults.py
      - name: Start mock OTLP receiver (background)
        run: |
          nohup python scripts/otel/mock_otlp.py >/dev/null 2>&1 &
      - name: Decorator split scan (report-only)
        run: |
          python scripts/fix_decorator_split.py --check || true
      - name: Premium Style Gate (sec.21)
        run: |
          python scripts/check_premium_style.py
      - name: Mark Style OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/style_ok.txt
      - name: Ruff Lint (no fix)
        run: |
          python -m ruff check .
      - name: Mark Lint OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/lint_ok.txt
      - name: Bandit security scan (report-only)
        continue-on-error: true
        run: |
          python -m pip install bandit
          # Report-only in ci-gates; enforcement lives in dedicated workflow
          bandit -q -r . -x .venv,venv,clients/web,clients/cli,dist,build || true
      - name: Gitleaks (secrets scan)
        continue-on-error: true
        uses: zricethezav/gitleaks-action@v2.3.7
        with:
          # Report-only for ci-gates; blocking in security-scan.yml
          args: detect --verbose --redact --no-banner --exit-code 1
      - name: Tests
        env:
          PYTEST_ADDOPTS: "-k 'not test_generate_proofs_cli_smoke_test'"
          OTEL_ENABLED: "1"
          OTEL_EXPORTER_OTLP_ENDPOINT: "http://127.0.0.1:4318"
          FINE_GRAINED_ROLES: "0"
        run: |
          python -m pytest -q
      - name: Mark Tests OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/tests_ok.txt
      - name: Perf Smoke (W11)
        run: |
          python scripts/perf/quick_bench.py --iters 5 --p95-max-ms 250 --out out/perf_bench.json
      - name: Perf Regression Gate (<= +5% vs prev)
        continue-on-error: true
        run: |
          set -e
          # Try to fetch previous perf benchmark from ci-status branch
          git fetch origin ci-status || true
          if git rev-parse --verify origin/ci-status >/dev/null 2>&1; then
            git show origin/ci-status:ci/perf_bench.json > out/prev_perf_bench.json 2>/dev/null || true
          fi
          python - << 'PY'
          import json, sys, pathlib
          repo = pathlib.Path('.').resolve()
          cur_p = repo/'out'/'perf_bench.json'
          prev_p = repo/'out'/'prev_perf_bench.json'
          if not cur_p.exists():
            print('No current perf report found; skipping regression gate')
            sys.exit(0)
          cur = json.loads(cur_p.read_text())
          worst = float(cur.get('worst_p95_ms', 0.0))
          thr_pct = 0.05
          if prev_p.exists():
            prev = json.loads(prev_p.read_text())
            base = float(prev.get('worst_p95_ms', 0.0)) or 1.0
            allowed = base * (1.0 + thr_pct)
            if worst > allowed:
              print(f'Regression: worst p95 {worst:.2f} ms > allowed {allowed:.2f} ms (base {base:.2f} +20%)')
              sys.exit(1)
          print('Perf regression gate: OK')
          PY
      - name: Mark Perf OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/perf_ok.txt

      - name: Upload perf report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-bench-${{ github.sha }}
          path: out/perf_bench.json

      - name: SLO Smoke (in-proc)
        env:
          SLO_MAX_P95_MS: "200"
          SLO_MAX_ERROR_RATE: "0.01"
        run: |
          python scripts/slo_gate/measure_api.py
          python scripts/slo_gate/check_slo.py
      - name: Mark SLO OK
        if: success()
        run: |
          mkdir -p out && echo ok > out/slo_ok.txt

      - name: Setup OPA CLI
        uses: open-policy-agent/setup-opa@v2
        with:
          version: latest
      - name: OPA policy tests (roles)
        continue-on-error: true
        run: |
          opa test policies/security -v
      - name: Governance consistency (smoke)
        run: |
          python scripts/validate_governance_consistency.py && echo ok > out/gov_ok.txt

      - name: Redaction Gate (informational)
        env:
          STRICT_REDACTION: "0"
        run: |
          echo '{"subject":{"name":"Jan KOWALSKI"}, "content":"Brak wrażliwych danych."}' | python scripts/gates/redaction_gate.py

      - name: Metrics smoke (/metrics)
        run: |
          python scripts/smokes/metrics_smoke.py
      - name: Mark metrics smoke OK
        if: success()
        run: |
          echo ok > out/metrics_ok.txt

      - name: OpenAPI smoke (/openapi.json)
        run: |
          python scripts/smokes/openapi_smoke.py
      - name: QTMP smoke (in-proc)
        run: |
          python scripts/smokes/qtm_smoke.py
      - name: QTMP smoke (sequence + expectation)
        run: |
          python scripts/smokes/qtm_seq_expect_smoke.py
      - name: LexQFT coverage smoke
        run: |
          python scripts/smokes/lexqft_smoke.py
      - name: Mark OpenAPI smoke OK
        if: success()
        run: |
          echo ok > out/openapi_ok.txt

      - name: OpenAPI smoke (/openapi.json)
        run: |
          python scripts/smokes/openapi_smoke.py

      - name: Comment PR with perf bench
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let body = '';
            try {
              const data = JSON.parse(fs.readFileSync('out/perf_bench.json', 'utf8'));
              const lines = [];
              lines.push(`### Perf Bench (p95)`);
              lines.push(`Worst p95: ${data.worst_p95_ms.toFixed(2)} ms (threshold ${data.threshold_ms} ms)`);
              lines.push('');
              lines.push('Endpoints:');
              for (const e of data.endpoints) {
                lines.push(`- ${e.method} ${e.path}: p95=${e.p95_ms.toFixed(2)} ms (min=${e.min_ms.toFixed(2)}, max=${e.max_ms.toFixed(2)})`);
              }
              body = lines.join('\n');
            } catch (e) {
              body = 'Perf bench: report not available';
            }
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body,
            });

      - name: Build PR comment summary
        if: github.event_name == 'pull_request' && always()
        run: |
          set -e
          # Try to fetch previous perf benchmark from ci-status branch
          git fetch origin ci-status || true
          if git rev-parse --verify origin/ci-status >/dev/null 2>&1; then
            git show origin/ci-status:ci/perf_bench.json > out/prev_perf_bench.json 2>/dev/null || true
          fi
          python - << 'PY'
          import json, os, pathlib, urllib.request
          repo = pathlib.Path('.').resolve()
          lines = []
          # Perf current
          perf = None
          try:
              perf = json.loads((repo/'out'/'perf_bench.json').read_text())
          except Exception:
              pass
          # Perf previous
          prev = None
          try:
              prev = json.loads((repo/'out'/'prev_perf_bench.json').read_text())
          except Exception:
              pass
          lines.append('### Perf Bench (p95)')
          if perf:
              worst = float(perf.get('worst_p95_ms', 0.0))
              thr = perf.get('threshold_ms')
              lines.append(f"Worst p95: {worst:.2f} ms (threshold {thr} ms)")
              if prev and 'worst_p95_ms' in prev:
                  delta = worst - float(prev.get('worst_p95_ms', 0.0))
                  sign = '+' if delta >= 0 else ''
                  lines.append(f"Δ vs prev: {sign}{delta:.2f} ms")
              lines.append('')
              lines.append('Endpoints:')
              for e in perf.get('endpoints', []):
                  lines.append(f"- {e['method']} {e['path']}: p95={e['p95_ms']:.2f} ms (min={e['min_ms']:.2f}, max={e['max_ms']:.2f})")
              lines.append('')
          else:
              lines.append('Perf bench: report not available')
          # SLO summary with pass/fail (defaults align with step env)
          try:
              slo = json.loads((repo/'out'/'slo.json').read_text())
              p95 = float(slo.get('p95_ms', 0.0))
              er = float(slo.get('error_rate', 0.0))
              max_p95 = float(os.getenv('SLO_MAX_P95_MS', '300'))
              max_er = float(os.getenv('SLO_MAX_ERROR_RATE', '0.01'))
              ok = (p95 <= max_p95) and (er <= max_er)
              lines.append(f"SLO: p95={p95:.2f} ms (<= {max_p95}), error_rate={er:.4f} (<= {max_er}) => {'✅' if ok else '❌'}")
          except Exception:
              lines.append('SLO: report not available')
          # Smokes
          metrics_ok = (repo/'out'/'metrics_ok.txt').exists()
          openapi_ok = (repo/'out'/'openapi_ok.txt').exists()
          gov_ok = (repo/'out'/'gov_ok.txt').exists()
          lines.append(f"Smokes: metrics={'✅' if metrics_ok else '❌'} openapi={'✅' if openapi_ok else '❌'} governance={'✅' if gov_ok else '❌'}")
          # Local gate ticks
          def _tick(p: str) -> str:
              return '✅' if (repo/'out'/p).exists() else '❌'
          lines.append(f"Gates: style={_tick('style_ok.txt')} lint={_tick('lint_ok.txt')} tests={_tick('tests_ok.txt')} perf={_tick('perf_ok.txt')} slo={_tick('slo_ok.txt')}")
          # Proof Gate related workflow ticks via GitHub API
          try:
              token = os.getenv('GITHUB_TOKEN')
              repo_slug = os.getenv('GITHUB_REPOSITORY')
              sha = os.getenv('GITHUB_SHA')
              req = urllib.request.Request(
                  f"https://api.github.com/repos/{repo_slug}/actions/runs?head_sha={sha}",
                  headers={"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"},
              )
              with urllib.request.urlopen(req, timeout=10) as resp:  # nosec B310
                  data = json.loads(resp.read().decode('utf-8'))
                  runs = data.get('workflow_runs') or []
                  want = {
                      'Proof Gate': 'pg',
                      'Gauge-Gate': 'gg',
                      'Path-Coverage-Gate': 'pc',
                      'Boundary-Rebuild-Gate': 'br',
                      'asset-guard': 'ag',
                  }
                  status_map = {v: '❌' for v in want.values()}
                  for r in runs:
                      name = r.get('name')
                      concl = r.get('conclusion')
                      key = want.get(name)
                      if key and concl:
                          status_map[key] = '✅' if concl == 'success' else '❌'
                  lines.append("Workflows: PG={} Gauge={} PathCov={} Boundary={} Assets={}".format(
                      status_map['pg'], status_map['gg'], status_map['pc'], status_map['br'], status_map['ag']
                  ))
          except Exception:
              lines.append('Workflows: (status not available)')
          (repo/'out'/'ci_pr_comment.md').write_text('\n'.join(lines), encoding='utf-8')
          PY

      - name: Comment PR with SLO/Smokes/Perf
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let body = 'CI Summary';
            try { body = fs.readFileSync('out/ci_pr_comment.md','utf8'); } catch(e) {}
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body,
            });

      - name: Publish CI status to branch
        if: always()
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

          attempt=0
          max_attempts=5
          until [ $attempt -ge $max_attempts ]; do
            attempt=$((attempt+1))
            echo "[ci-status] Attempt $attempt/$max_attempts"
            # Always refetch latest remote tip
            git fetch origin ci-status || true
            if git rev-parse --verify origin/ci-status >/dev/null 2>&1; then
              git checkout -B ci-status origin/ci-status
            else
              git checkout --orphan ci-status
              git rm -rf . || true
            fi
            mkdir -p ci
            ts="$(date -u +%FT%TZ)"
            printf '%s\n' '{' \
              "  \"repo\": \"${{ github.repository }}\"," \
              "  \"workflow\": \"${{ github.workflow }}\"," \
              "  \"run_id\": \"${{ github.run_id }}\"," \
              "  \"run_number\": \"${{ github.run_number }}\"," \
              "  \"event\": \"${{ github.event_name }}\"," \
              "  \"branch\": \"${{ github.ref_name }}\"," \
              "  \"sha\": \"${{ github.sha }}\"," \
              "  \"status\": \"${{ job.status }}\"," \
              "  \"updated_at\": \"${ts}\"" \
            '}' > ci/status.json
            # Persist latest perf bench for trend comparisons
            if [ -f out/perf_bench.json ]; then
              cp -f out/perf_bench.json ci/perf_bench.json || true
            fi
            git add ci/status.json
            git add ci/perf_bench.json || true
            git commit -m "ci: update status to ${{ job.status }} for ${{ github.sha }}" || echo "No changes"
            if git push origin ci-status; then
              echo "[ci-status] Push OK"
              break
            else
              echo "[ci-status] Push failed (likely race). Retrying..."
              sleep 2
            fi
          done
          if [ $attempt -ge $max_attempts ]; then
            echo "[ci-status] Failed to push after $max_attempts attempts"
            exit 1
          fi



