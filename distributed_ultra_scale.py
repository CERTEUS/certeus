#!/usr/bin/env python3
"""
üåê DISTRIBUTED ULTRA-SCALE SYSTEM
Blockchain-level scalability: distributed nodes, sharding, consensus
Target: Millions of operations/s across multiple nodes
"""

import asyncio
from dataclasses import dataclass
import hashlib
import random
import time
from typing import Any
import uuid

# aiohttp dependency removed - not used in current implementation


@dataclass
class NodeInfo:
    """Distributed node information"""

    node_id: str
    host: str
    port: int
    shard_range: tuple[int, int]  # (start, end) shard range
    capacity: int = 10000  # Operations per second capacity
    is_leader: bool = False
    last_heartbeat: float = 0.0


@dataclass
class ShardedOperation:
    """Sharded operation for distributed processing"""

    operation_id: str
    shard_key: str
    shard_id: int
    node_id: str
    data: dict[str, Any]
    timestamp: float
    signature: str = ""


class ConsensusManager:
    """Distributed consensus manager for ultra-scale coordination"""

    def __init__(self, node_id: str):
        self.node_id = node_id
        self.nodes: dict[str, NodeInfo] = {}
        self.leader_id: str | None = None
        self.consensus_round = 0
        self.pending_operations: list[ShardedOperation] = []

        print(f"üåê Consensus manager initialized for node {node_id}")

    def add_node(self, node: NodeInfo):
        """Add node to distributed cluster"""
        self.nodes[node.node_id] = node
        print(f"‚úÖ Added node {node.node_id} with shards {node.shard_range}")

    def elect_leader(self) -> str:
        """Simple leader election based on node capacity"""
        if not self.nodes:
            return self.node_id

        # Choose node with highest capacity as leader
        leader = max(self.nodes.values(), key=lambda n: n.capacity)
        self.leader_id = leader.node_id

        # Update leader status
        for node in self.nodes.values():
            node.is_leader = node.node_id == self.leader_id

        print(f"üèÜ Leader elected: {self.leader_id}")
        return self.leader_id

    def reach_consensus(self, operation: ShardedOperation) -> bool:
        """Reach consensus on distributed operation"""
        # Simplified consensus: majority agreement
        votes = 0
        total_nodes = len(self.nodes) + 1  # +1 for current node

        # Vote based on operation validity
        if self._validate_operation(operation):
            votes += 1

        # Simulate other nodes voting (in real system, this would be network calls)
        for node in self.nodes.values():
            if self._simulate_node_vote(node, operation):
                votes += 1

        consensus_reached = votes > total_nodes // 2

        if consensus_reached:
            self.pending_operations.append(operation)
            self.consensus_round += 1

        return consensus_reached

    def _validate_operation(self, operation: ShardedOperation) -> bool:
        """Validate operation for consensus"""
        # Check operation structure
        if not operation.operation_id or not operation.data:
            return False

        # Check shard assignment
        target_shard = self._calculate_shard(operation.shard_key)
        if target_shard != operation.shard_id:
            return False

        # Check timestamp (not too old/future)
        current_time = time.time()
        if abs(current_time - operation.timestamp) > 60:  # 1 minute tolerance
            return False

        return True

    def _simulate_node_vote(self, node: NodeInfo, operation: ShardedOperation) -> bool:
        """Simulate node voting (in real system: network call)"""
        # Simulate network latency and availability
        if time.time() - node.last_heartbeat > 30:
            return False  # Node unavailable

        # Simulate vote based on node characteristics
        vote_probability = 0.95  # 95% agreement rate
        return random.random() < vote_probability

    def _calculate_shard(self, shard_key: str) -> int:
        """Calculate shard ID from key"""
        hash_value = int(hashlib.sha256(shard_key.encode()).hexdigest()[:16], 16)
        return hash_value % 1000  # 1000 shards


class DistributedShardManager:
    """Manages data sharding across distributed nodes"""

    def __init__(self, total_shards: int = 1000):
        self.total_shards = total_shards
        self.shard_map: dict[int, str] = {}  # shard_id -> node_id
        self.node_shards: dict[str, list[int]] = {}  # node_id -> [shard_ids]
        self.replication_factor = 3

        print(f"üåê Shard manager: {total_shards} shards")

    def assign_shards(self, nodes: dict[str, NodeInfo]):
        """Assign shards to nodes with load balancing"""
        node_list = list(nodes.keys())
        if not node_list:
            return

        # Clear existing assignments
        self.shard_map.clear()
        self.node_shards.clear()

        # Initialize node shard lists
        for node_id in node_list:
            self.node_shards[node_id] = []

        # Distribute shards evenly
        shards_per_node = self.total_shards // len(node_list)
        extra_shards = self.total_shards % len(node_list)

        current_shard = 0
        for i, node_id in enumerate(node_list):
            # Base shards + extra if needed
            num_shards = shards_per_node + (1 if i < extra_shards else 0)

            for _ in range(num_shards):
                self.shard_map[current_shard] = node_id
                self.node_shards[node_id].append(current_shard)
                current_shard += 1

        # Update node shard ranges
        for node_id, node in nodes.items():
            if node_id in self.node_shards and self.node_shards[node_id]:
                shards = self.node_shards[node_id]
                node.shard_range = (min(shards), max(shards))

        print(f"‚úÖ Shards assigned: {len(node_list)} nodes, avg {shards_per_node} shards/node")

    def get_node_for_shard(self, shard_id: int) -> str | None:
        """Get responsible node for shard"""
        return self.shard_map.get(shard_id)

    def get_nodes_for_key(self, key: str) -> list[str]:
        """Get nodes responsible for key (with replication)"""
        shard_id = self._calculate_shard_id(key)
        primary_node = self.shard_map.get(shard_id)

        if not primary_node:
            return []

        # Add replica nodes
        nodes = [primary_node]
        all_nodes = list(self.shard_map.values())

        for _ in range(min(self.replication_factor - 1, len(all_nodes) - 1)):
            replica_shard = (shard_id + len(nodes)) % self.total_shards
            replica_node = self.shard_map.get(replica_shard)
            if replica_node and replica_node not in nodes:
                nodes.append(replica_node)

        return nodes

    def _calculate_shard_id(self, key: str) -> int:
        """Calculate shard ID for key"""
        hash_value = int(hashlib.sha256(key.encode()).hexdigest()[:16], 16)
        return hash_value % self.total_shards


class DistributedNode:
    """Single node in distributed ultra-scale system"""

    def __init__(self, node_id: str, host: str = "localhost", port: int = 8000):
        self.node_info = NodeInfo(
            node_id=node_id,
            host=host,
            port=port,
            shard_range=(0, 0),
            capacity=random.randint(5000, 15000),  # Random capacity
            last_heartbeat=time.time(),
        )

        self.consensus_manager = ConsensusManager(node_id)
        self.local_operations: list[ShardedOperation] = []
        self.performance_metrics = {
            'operations_processed': 0,
            'operations_per_second': 0.0,
            'consensus_success_rate': 0.0,
            'network_latency_ms': 0.0,
        }

        # Background tasks
        self.running = False
        self.heartbeat_task: asyncio.Task | None = None
        self.process_task: asyncio.Task | None = None

        print(f"üåê Node {node_id} initialized (capacity: {self.node_info.capacity} ops/s)")

    async def start(self):
        """Start distributed node"""
        self.running = True

        # Start background tasks
        self.heartbeat_task = asyncio.create_task(self._heartbeat_loop())
        self.process_task = asyncio.create_task(self._process_operations_loop())

        print(f"üöÄ Node {self.node_info.node_id} started")

    async def stop(self):
        """Stop distributed node"""
        self.running = False

        if self.heartbeat_task:
            self.heartbeat_task.cancel()
        if self.process_task:
            self.process_task.cancel()

        print(f"üõë Node {self.node_info.node_id} stopped")

    async def submit_operation(self, operation: ShardedOperation) -> bool:
        """Submit operation for distributed processing"""
        # Check if this node should handle the operation
        if not self._should_handle_operation(operation):
            return False

        # Reach consensus
        consensus = self.consensus_manager.reach_consensus(operation)

        if consensus:
            self.local_operations.append(operation)
            self.performance_metrics['operations_processed'] += 1
            return True

        return False

    async def _heartbeat_loop(self):
        """Send heartbeat to maintain cluster membership"""
        while self.running:
            self.node_info.last_heartbeat = time.time()

            # Update performance metrics
            await self._update_performance_metrics()

            await asyncio.sleep(5)  # 5-second heartbeat

    async def _process_operations_loop(self):
        """Process local operations at high speed"""
        batch_size = 1000

        while self.running:
            if len(self.local_operations) >= batch_size:
                # Process batch
                batch = self.local_operations[:batch_size]
                self.local_operations = self.local_operations[batch_size:]

                start_time = time.perf_counter()
                await self._process_operation_batch(batch)
                elapsed = time.perf_counter() - start_time

                # Update throughput metrics
                if elapsed > 0:
                    self.performance_metrics['operations_per_second'] = len(batch) / elapsed

            await asyncio.sleep(0.01)  # 10ms processing interval

    async def _process_operation_batch(self, operations: list[ShardedOperation]):
        """Process batch of operations ultra-fast"""
        # Simulate ultra-fast batch processing
        for operation in operations:
            # Validate and process operation
            if self._validate_operation_locally(operation):
                # Simulate processing time
                await asyncio.sleep(0.0001)  # 0.1ms per operation

    def _should_handle_operation(self, operation: ShardedOperation) -> bool:
        """Check if this node should handle the operation"""
        shard_start, shard_end = self.node_info.shard_range
        return shard_start <= operation.shard_id <= shard_end

    def _validate_operation_locally(self, operation: ShardedOperation) -> bool:
        """Local operation validation"""
        return bool(operation.operation_id and operation.data)

    async def _update_performance_metrics(self):
        """Update node performance metrics"""
        # Simulate network latency measurement
        self.performance_metrics['network_latency_ms'] = random.uniform(1.0, 10.0)

        # Calculate consensus success rate
        total_consensus = len(self.consensus_manager.pending_operations)
        if total_consensus > 0:
            success_rate = min(1.0, total_consensus / (total_consensus + 1))
            self.performance_metrics['consensus_success_rate'] = success_rate


class DistributedUltraScaleSystem:
    """
    üåê Distributed ultra-scale system
    Target: Millions of operations/s across multiple nodes
    """

    def __init__(self, num_nodes: int = 10):
        self.num_nodes = num_nodes
        self.nodes: dict[str, DistributedNode] = {}
        self.shard_manager = DistributedShardManager()

        # System metrics
        self.total_operations = 0
        self.start_time = 0.0
        self.peak_throughput = 0.0

        print(f"üåê Distributed ultra-scale system: {num_nodes} nodes")

    async def initialize_cluster(self):
        """Initialize distributed cluster"""
        print("üöÄ Initializing distributed cluster...")

        # Create nodes
        for i in range(self.num_nodes):
            node_id = f"node-{i:03d}"
            port = 8000 + i

            node = DistributedNode(node_id, "localhost", port)
            self.nodes[node_id] = node

        # Set up cross-node consensus
        for node in self.nodes.values():
            for other_node in self.nodes.values():
                if other_node.node_info.node_id != node.node_info.node_id:
                    node.consensus_manager.add_node(other_node.node_info)

        # Assign shards
        node_infos = {nid: node.node_info for nid, node in self.nodes.items()}
        self.shard_manager.assign_shards(node_infos)

        # Elect leaders
        for node in self.nodes.values():
            node.consensus_manager.elect_leader()

        print(f"‚úÖ Cluster initialized: {len(self.nodes)} nodes")

    async def start_cluster(self):
        """Start all nodes in cluster"""
        print("üöÄ Starting distributed cluster...")

        self.start_time = time.perf_counter()

        # Start all nodes
        start_tasks = [node.start() for node in self.nodes.values()]
        await asyncio.gather(*start_tasks)

        print("‚úÖ Cluster started")

    async def stop_cluster(self):
        """Stop cluster and collect metrics"""
        print("üõë Stopping distributed cluster...")

        # Stop all nodes
        stop_tasks = [node.stop() for node in self.nodes.values()]
        await asyncio.gather(*stop_tasks)

        # Calculate final metrics
        elapsed = time.perf_counter() - self.start_time
        avg_throughput = self.total_operations / elapsed if elapsed > 0 else 0

        print("üìä Cluster Statistics:")
        print(f"   Total Operations: {self.total_operations:,}")
        print(f"   Elapsed Time: {elapsed:.3f}s")
        print(f"   Average Throughput: {avg_throughput:.0f} ops/s")
        print(f"   Peak Throughput: {self.peak_throughput:.0f} ops/s")

        return {
            'total_operations': self.total_operations,
            'elapsed_time': elapsed,
            'avg_throughput': avg_throughput,
            'peak_throughput': self.peak_throughput,
            'num_nodes': len(self.nodes),
        }

    async def submit_distributed_operation(self, data: dict[str, Any]) -> bool:
        """Submit operation to distributed system"""
        # Create sharded operation
        operation_id = str(uuid.uuid4())
        shard_key = data.get('key', operation_id)
        shard_id = self.shard_manager._calculate_shard_id(shard_key)

        # Find responsible nodes
        responsible_nodes = self.shard_manager.get_nodes_for_key(shard_key)

        if not responsible_nodes:
            return False

        # Create operation
        operation = ShardedOperation(
            operation_id=operation_id,
            shard_key=shard_key,
            shard_id=shard_id,
            node_id=responsible_nodes[0],  # Primary node
            data=data,
            timestamp=time.time(),
        )

        # Submit to primary node
        primary_node = self.nodes.get(responsible_nodes[0])
        if primary_node:
            success = await primary_node.submit_operation(operation)
            if success:
                self.total_operations += 1

                # Update peak throughput
                current_throughput = sum(
                    node.performance_metrics['operations_per_second'] for node in self.nodes.values()
                )
                self.peak_throughput = max(self.peak_throughput, current_throughput)

                return True

        return False

    def get_cluster_metrics(self) -> dict[str, Any]:
        """Get comprehensive cluster metrics"""
        node_metrics = {}
        total_capacity = 0
        total_ops_per_sec = 0

        for node_id, node in self.nodes.items():
            metrics = node.performance_metrics.copy()
            metrics['capacity'] = node.node_info.capacity
            metrics['shard_range'] = node.node_info.shard_range
            metrics['is_leader'] = node.node_info.is_leader

            node_metrics[node_id] = metrics
            total_capacity += node.node_info.capacity
            total_ops_per_sec += metrics['operations_per_second']

        return {
            'total_operations': self.total_operations,
            'peak_throughput': self.peak_throughput,
            'current_throughput': total_ops_per_sec,
            'total_capacity': total_capacity,
            'utilization': total_ops_per_sec / total_capacity if total_capacity > 0 else 0,
            'num_nodes': len(self.nodes),
            'num_shards': self.shard_manager.total_shards,
            'node_metrics': node_metrics,
        }


async def create_distributed_ultra_scale() -> DistributedUltraScaleSystem:
    """Create distributed ultra-scale system"""
    system = DistributedUltraScaleSystem(num_nodes=8)
    await system.initialize_cluster()
    return system


# Global distributed system instance
_distributed_system: DistributedUltraScaleSystem | None = None


async def get_distributed_system() -> DistributedUltraScaleSystem:
    """Get global distributed ultra-scale system"""
    global _distributed_system

    if _distributed_system is None:
        _distributed_system = await create_distributed_ultra_scale()

    return _distributed_system


if __name__ == "__main__":

    async def test_distributed_ultra_scale():
        """Test distributed ultra-scale system"""
        print("üåêüåêüåê DISTRIBUTED ULTRA-SCALE TEST üåêüåêüåê")

        system = await get_distributed_system()
        await system.start_cluster()

        # Generate test workload
        test_operations = [
            {'key': f'test-key-{i}', 'data': f'ultra_scale_data_{i}', 'operation': 'CREATE'} for i in range(10000)
        ]

        start_time = time.perf_counter()

        # Submit operations concurrently
        submit_tasks = [system.submit_distributed_operation(op) for op in test_operations]

        results = await asyncio.gather(*submit_tasks)
        successful_ops = sum(1 for r in results if r)

        elapsed = time.perf_counter() - start_time
        throughput = successful_ops / elapsed

        print(f"‚úÖ Processed {successful_ops:,}/{len(test_operations):,} operations in {elapsed:.3f}s")
        print(f"üåê Distributed Throughput: {throughput:.0f} ops/s")
        print(f"üìä Cluster Metrics: {system.get_cluster_metrics()}")

        await system.stop_cluster()

    asyncio.run(test_distributed_ultra_scale())
